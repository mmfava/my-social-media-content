---
author:
  - Jonas Schuett
citation: " Schuett, J. (2024). Frontier AI developers need an internal audit function. Risk Analysis,  1–21. https://doi.org/10.1111/risa.17665"
doi: 10.1111/risa.17665
url: https://onlinelibrary.wiley.com/doi/10.1111/risa.17665
created: 2025-01-16
title: Frontier AI developers need an internal audit function
updated: 2025-01-16T13:24
published: 2024-10-21
---
First published: 21 October 2024

Citations: [2](https://onlinelibrary.wiley.com/doi/10.1111/#citedby-section)

Sections

- [Abstract](https://onlinelibrary.wiley.com/doi/10.1111/#d387186604)
- [1 INTRODUCTION](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0010-title)
- [2 SETTING THE STAGE](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0020-title)
- [3 ADVANCING THE ARGUMENT](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0220-title)
- [4 CONCLUSION](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0350-title)
- [ACKNOWLEDGMENTS](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0390-title)
- [CONFLICT OF INTEREST STATEMENT](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0400-title)
- [REFERENCES](https://onlinelibrary.wiley.com/doi/10.1111/#references-section-1)
- [Citing Literature](https://onlinelibrary.wiley.com/doi/10.1111/#citedby-section)

## Abstract

This article argues that frontier artificial intelligence (AI) developers need an internal audit function. First, it describes the role of internal audit in corporate governance: internal audit evaluates the adequacy and effectiveness of a company's risk management, control, and governance processes. It is organizationally independent from senior management and reports directly to the board of directors, typically its audit committee. In the Institute of Internal Auditors' Three Lines Model, internal audit serves as the third line and is responsible for providing assurance to the board, whereas the combined assurance framework highlights the need to coordinate the activities of internal and external assurance providers. Next, the article provides an overview of key governance challenges in frontier AI development: Dangerous capabilities can arise unpredictably and undetected; it is difficult to prevent a deployed model from causing harm; frontier models can proliferate rapidly; it is inherently difficult to assess frontier AI risks; and frontier AI developers do not seem to follow best practices in risk governance. Finally, the article discusses how an internal audit function could address some of these challenges: Internal audit could identify ineffective risk management practices; it could ensure that the board of directors has a more accurate understanding of the current level of risk and the adequacy of the developer's risk management practices; and it could serve as a contact point for whistleblowers. But frontier AI developers should also be aware of key limitations: Internal audit adds friction; it can be captured by senior management; and the benefits depend on the ability of individuals to identify ineffective practices. In light of rapid progress in AI research and development, frontier AI developers need to strengthen their risk governance. Instead of reinventing the wheel, they should follow existing best practices. Although this might not be sufficient, they should not skip this obvious first step.

## 1 INTRODUCTION

The last few years have shown a remarkable trend: Using more data and more compute to train bigger artificial intelligence (AI) models leads to predictable improvements in their performance.[1](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0001_note_0 "Link to note") This phenomenon is commonly referred to as “scaling laws” (Bahri et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0026); Hestness et al., [2017](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0137); Hoffmann et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0140); Kaplan et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0164)) and the claim that this trend will continue as the “scaling hypothesis” (Gwern, [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0126)). Although it is unclear if the scaling hypothesis is true (Lohn & Musser, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0188); Narayanan & Kapoor, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0219); Sevilla et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0289); Villalobos et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0314)), the underlying trend has already led to the development of highly capable and increasingly general AI systems, such as GPT-4o (OpenAI, [2024c](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0242)), Claude 3.5 Sonnet (Anthropic, [2024b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0015), [2024c](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0016)), and Gemini 1.5 (Google DeepMind, [2024b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0122)).

But this trend also has concerning implications. As models are scaled up, new capabilities can emerge unintentionally and unpredictably (Ganguli, Hernandez et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0114); Wei et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0318)),[2](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0002_note_1 "Link to note") some of which might be dangerous (Phuong et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253); Shevlane et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292)). For example, models might become able to deceive, persuade, and manipulate people (Carlsmith, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0056); El-Sayed et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0096); Hackenburg & Margetts, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0127); Hagendorff, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0130); Hubinger et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0143); Park et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0248)), find and exploit cyber vulnerabilities (Fang et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0106); Lohn & Jackson, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0187); Mirsky et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0206); Zhang et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0335)), or provide instructions for the acquisition of biological weapons (Gopal et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0336); Mouton et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0216); OpenAI, [2024a](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0240); Sandbrink, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0273); Soice et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0297)). Terrorists, cybercriminals, or other malicious actors could use such models to cause large-scale harm (Anderljung & Hazell, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0005); Brundage et al., [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0046); Marchal et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0193)). There are also increasing concerns that autonomous agents (Chan et al., [2023, 2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0063%20#risa17665-bib-0064); Cohen et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0074); Gabriel et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0112); Shavit et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0290)) might at some point be able to create copies of themselves, acquire resources, and seek power (Kinniment et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0169); Krakovna & Kramar, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0177); Turner & Tadepalli, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0312); Turner et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0311)). Some people even worry that certain capabilities could result in a global catastrophe (Center for AI Safety, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0060); Hendrycks et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0135); Ngo et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0223)).[3](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0003_note_2 "Link to note") Against this background, managing the risks of frontier AI models is an important and urgent challenge.

To rise to this challenge, companies like OpenAI, Google DeepMind, and Anthropic need to strengthen their risk management practices (Bengio et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0032)). In some cases, they need to develop new solutions, such as alignment techniques (Anwar et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0017); Ji et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0158)), model evaluations for dangerous capabilities (Phuong et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253); Shevlane et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292)), or AI safety frameworks (Anthropic, [2023a](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0009); Google DeepMind, [2024a](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0121); OpenAI, [2023f](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0238)). In other cases, it would be more appropriate to learn from other industries. Instead of reinventing the wheel, frontier AI developers should apply EXI best practices in risk management to an AI context.[4](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0004_note_3 "Link to note") One such practice is internal audit.

This article argues that frontier AI developers need an internal audit function. Section [2](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0020) sets the stage by defining key terms and reviewing relevant literature. It also describes the role of internal audit in corporate governance based on the Three Lines Model and the combined assurance framework and gives an overview of key governance challenges in frontier AI development. Section [3](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0220) advances the main argument by discussing how internal audit can address some of these challenges, while acknowledging key limitations. Section [4](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0350) concludes with a summary of the article's main contributions and suggestions for further research.

## 2 SETTING THE STAGE

### 2.1 Terminology

Let us first define some of the key terms used in this article, namely, “internal audit,” “AI,” and “frontier AI,” as well as related terms like “general-purpose AI (GPAI),” “foundation model,” and “artificial general intelligence (AGI).”

#### 2.1.1 Internal audit

The term “internal audit” might cause confusion because it is used in two different ways. In corporate governance, internal audit is a technical term. It refers to a specific organizational function (e.g., a team) that evaluates the adequacy and effectiveness of a company's risk management, control, and governance processes (IIA, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0153); Nagy & Cenker, [2002](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0217)). This function is organizationally independent from senior management and reports directly to the board of directors, typically its audit committee. In doing so, internal audit provides independent and objective assurance about the organization's risk management practices. Internal audit professionals are typically Certified Internal Auditors, though a certificate is not required (IIA, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0152)). The most senior individual with responsibilities for internal audit services is typically called Chief Audit Executive (CAE) or Head of Internal Audit (IIA, [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0150)).

Outside corporate governance, the term is used more loosely. It typically refers to any audit that is done internally, as opposed to a third-party audit (Birhane et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0039); Raji et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0259)). The basic idea behind auditing is that an auditor (e.g., an audit firm) evaluates whether the object of the audit (e.g., an AI system or the governance of an organization) complies with predefined audit criteria (e.g., regulations or standards) (Birhane et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0039); Brundage et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0047); IEEE, [2008](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0145); Mökander et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0211)). For example, during a financial audit, a certified accounting firm like KPMG or EY evaluates whether a company has prepared its financial statement in accordance with a recognized accounting standard like IFRS or US GAAP. Similarly, during a model audit, an organization like METR[5](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0005_note_4 "Link to note") might evaluate a model against a list of realistic autonomous tasks (Kinniment et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0169)). An audit is external if the auditor is not part of the organization that is being audited (Anderljung, Smith et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0007); Falco et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0105); Raji & Buolamwini, [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0258); Raji et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0260)). It is internal if the auditor is part of that organization (Raji et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0259)).[6](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0006_note_5 "Link to note")

The two ways of using the term overlap, but there are also important differences. Internal audit as a specific function has a precisely defined scope (evaluating the organization's risk management practices), purpose (providing independent and objective assurance to the board), and position within the organization (it is organizationally independent from senior management and reports directly to the board). In contrast, internal audit as the opposite of external audit is less precisely defined. It could assess models, applications, or governance structures (Mökander et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0212)), which may or may not include the organization's risk management practices. For the purpose of this article, I use the term in the first sense: I mean the specific function that evaluates the effectiveness of risk management practices, not the opposite of external audit.

#### 2.1.2 Artificial intelligence

There is no generally accepted definition of the term “AI.” It was first mentioned by McCarthy et al. ([1955](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0197)) in the funding proposal for the famous Dartmouth workshop, which is widely considered to be the birthplace of the field (Nilsson, [2009](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0224)). Since then, a vast spectrum of definitions has emerged (e.g., Kurzweil, [1990](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0178); McCarthy, [2007](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0196); Minsky, [1969](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0205); Nilsson, [2009](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0224)). Categorizations of different definitions have been proposed by and Bhatnagar et al. ([2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0036)), Russell and Norvig ([2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0270)), Wang ([2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0316)). However, for the purposes of this article, a precise definition is not necessary. A social definition is sufficient, according to which AI is what people generally consider to be AI (Cihon et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0069)).[7](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0007_note_6 "Link to note") At the moment, most people seem to mean deep neural networks trained on large datasets with large amounts of compute using supervised, unsupervised, or reinforcement learning (RL) algorithms (Maslej et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0194)). This includes large language models like GPT-4o (OpenAI, [2024c](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0242)), Claude 3.5 Sonnet (Anthropic, [2024b, 2024c](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0015%20#risa17665-bib-0016)), and Gemini 1.5 (Google DeepMind, [2024b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0122)), image-generation models like DALL·E 3 (Betker et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0034); OpenAI, [2023a](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0233)), and Stable Diffusion 3 (Esser et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0101); Rombach et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0263)), and video-generation models like Sora (Brooks et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0045)). More recently, large multimodal models have become popular, that is, models that can process different types of inputs like text, image, audio, and video (Maslej et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0194)).

#### 2.1.3 Frontier AI

The term “frontier AI” has only recently gained traction. It has been used in several publications (e.g., Anderljung, Barnhart et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0006); Bucknall & Trager, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0050); Shevlane et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292)) and in the context of the UK AI Safety Summit in Bletchley Park (DSIT, [2023a, 2023b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0086%20#risa17665-bib-0087), [2023c, 2023d](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0088%20#risa17665-bib-0089)) and the AI Seoul Summit 2024 (DSIT, [2024a, 2024b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0090%20#risa17665-bib-0091)). It has also been adopted by several companies, including Google DeepMind ([2023a, 2024a](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0119%20#risa17665-bib-0121)), OpenAI ([2023c, 2023e](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0235%20#risa17665-bib-0237), [2023f](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0238)), and Anthropic ([2023a, 2023b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0009%20#risa17665-bib-0010)), as well as the Frontier Model Forum ([2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0110)). In this article, “frontier AI models” are defined as highly capable GPAI models that can perform a wide variety of tasks and match or exceed the capabilities present in the most advanced models (DSIT, [2024b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0091)).[8](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0008_note_7 "Link to note") Similar definitions have been suggested by Shevlane et al. ([2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292)),[9](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0009_note_8 "Link to note") Anderljung, Barnhart et al. ([2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0006)),[10](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0010_note_9 "Link to note") and Phuong et al. ([2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253)).[11](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0011_note_10 "Link to note") But note that the term has also been criticized (Helfrich, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0133)). By “frontier AI developer,” I mean any organization that develops frontier AI models.

#### 2.1.4 Related terms

The term frontier AI model overlaps with several other terms. For example, a frontier model is a special type of “GPAI” model. Although the term frontier AI is defined relative to existing capabilities, the term GPAI puts more emphasis on the generality of the model's capabilities (Gutierrez et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0125)). The term GPAI, in turn, is often used synonymously with “foundation model” (Jones, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0161)), defined as any model trained on broad data that can be adapted to a wide range of downstream tasks (Bommasani et al., [20212](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0041)). The main difference is that the term foundation model puts more emphasis on the model's role in the supply chain: It serves as a foundation for other models and applications. Another related term is “AGI,” which can be defined as an AI system that achieves or exceeds human performance across a wide range of cognitive tasks (Altman, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0003); Goertzel, [2014](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0116); Goertzel & Pennachin, [2007](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0117); Morris et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0215)). Although some scholars think that current models at the frontier should already be considered AGI (Bubeck et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0048)), others remain skeptical that AGI will ever be built (Fjelland, [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0107); Mitchell, [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0207), [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0337)).

### 2.2 Related work

To contextualize the article within the literature, let us now review previous work on internal audit in general, internal audits of information technologies (IT), internal audits of AI, and frontier AI governance more broadly.

#### 2.2.1 Literature on internal audit

Internal audit is a well-studied governance measure (Büchling et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0049); Cascarino, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0058); Coetzee et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0072); Kotb et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0176); Roussy & Perron, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0267)). There are numerous empirical studies on the value of internal audit (Eulerich & Eulerich, [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0102); Jiang et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0159); Lenz & Hahn, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0182)). For example, internal audit is associated with increased financial performance (Jiang et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0159)), a decline in perceived risk (Carcello et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0055)), strengthened internal controls (Lin et al., [2011](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0186); Oussii & Boulila Taktak, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0245)), and improved fraud prevention (Coram et al., [2008](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0075); Drogalas et al., [2017](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0084); Ma'ayan & Carmeli, [2016](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0195)). There is also literature on the drivers of internal audit effectiveness (Arena & Azzone, [2009](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0018); Coetzee & Lubbe, [2014](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0073); Erasmus & Coetzee, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0099); Soh & Martinov-Bennie, [2011](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0296)), the challenge of becoming or remaining independent from senior management (Guénin-Paracini et al., [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0124); Nordin, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0229); Roussy, [2013](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0265); Roussy & Rodrigue, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0268); Stewart & Subramaniam, [2010](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0304)), and the future of internal audit (Betti & Sarens, [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0035); Chambers & Odar, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0061); Christ et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0066)).

#### 2.2.2 Literature on internal IT audits

There is also extensive literature on internal IT audits, that is, internal audits that assess the effectiveness, efficiency, and security of IT systems and processes (Hermanson et al., [2000](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0136); Merhout & Havelka, [2008](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0200); Senft & Gallegos, [2008](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0288); Weidenmier & Ramamoorti, [2006](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0319)). There are numerous empirical studies that examine the value and effectiveness of internal IT audits as part of IT governance (De Haes & Van Grembergen, [2009](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0080)). For example, Steinbart et al. ([2012](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0303)) found that internal IT audits can improve information security, whereas Stafford et al. ([2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0302)) investigated the role of IT audits in identifying noncompliance with security policies in the workplace. Stoel et al. ([2012](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0305)) identified factors that contribute to IT audit quality, including the technical competence of auditors and the quality of client relationships, whereas Merhout and Havelka ([2008](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0200)) identified six critical factors for audit success and argued for a value-added partnership between IT management and auditors.

#### 2.2.3 Literature on internal audit and AI

Internal AI audits can be seen as a subset of internal IT audits (ISACA, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0155)). Although there is some literature on the intersection of internal audit and AI, most of it is about how internal auditors can use AI (Couceiro et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0076); Emett et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0097); Kahyaoglu & Aksoy, [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0163); Wassie & Lakatos, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0317)).[12](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0012_note_11 "Link to note") The most relevant piece is an article that applies the Three Lines Model, a risk governance framework where internal audit serves as the third line, to an AI context (Schuett, [2023c](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0282)). Here, I conduct a more in-depth analysis of the third line. Besides that, the Institute of Internal Auditors (IIA) has published a three-part series, in which they propose an AI auditing framework (IIA, [2017a, 2017b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0147%20#risa17665-bib-0148), [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0149)). In the first two parts, they specifically discuss the role of internal audit, though the relevant passages are rather short and somewhat outdated. There is only one study that mentions internal audit in the context of AGI developers (Schuett et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0283)), but only as part of a broader expert survey on best practices in AGI safety and governance. There does not seem to be an investigation of the benefits and limitations of internal audit at frontier AI developers.

#### 2.2.4 Literature on frontier AI governance

The study of frontier AI governance is a small but growing field. Ahead of the UK AI Safety Summit, the UK Department for Science, Innovation and Technology (DSIT) published a policy paper on emerging processes in frontier AI safety (DSIT, [2023c](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0088)). At the AI Seoul Summit 2024, 16 companies agreed to the Frontier AI Safety Commitments (DSIT, [2024b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0091)). Besides that, there is literature on frontier AI regulation (Anderljung, Barnhart et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0006); Schuett, Anderljung et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0285)), evaluating frontier models for dangerous capabilities (Phuong et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253); Shevlane et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292)), setting risk thresholds for frontier AI (Koessler et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0173)), responsible reporting for frontier AI development (Kolt et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0175)), responding to risks that are discovered after a frontier model has been deployed (O'Brien et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0246)), promoting external scrutiny of frontier models (Anderljung, Smith et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0007); Bucknall & Trager, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0050)), overseeing frontier AI through know-your-customer requirements for compute providers (Egan & Heim, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0094); Sastry et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0276)), and establishing an international governance regime for frontier AI (Gruetzemacher et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0123); Ho et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0138); Trager et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0310)). It is worth noting, however, that not all relevant literature uses the term frontier AI. Some of it is about GPAIS (e.g., Barrett et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0029)), foundation models (e.g., Kapoor et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0165); Partnership on AI, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0249); Seger, Dreksler et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0287); Vipra & Korinek, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0315)), generative AI (e.g., Hacker et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0128); Longpre et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0189)), or AGI (e.g., Koessler & Schuett, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0172); Schuett et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0283)).

### 2.3 The role of internal audit in corporate governance

Internal audit plays a key role in corporate governance. Below, I describe its role based on the Three Lines Model and the combined assurance framework. This provides the theoretical foundation for the main argument in Section [3](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0220).

#### 2.3.1 Three Lines Model

Risk management roles and responsibilities are increasingly split across multiple teams (e.g., legal, compliance, and cybersecurity). However, without proper coordination, work can be duplicated and gaps in risk coverage can occur (Bantleon et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0028)). The Three Lines Model, formerly known as Three Lines of Defense (Davies & Zhivitskaya, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0079); IIA, [2013](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0146)), is intended to address this problem (IIA, [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0150)). It is a popular risk governance framework that helps organizations to assign and coordinate risk management roles and responsibilities. It distinguishes between three roles, which it calls “lines.” The first line provides products and services to clients and is ultimately responsible for risk management. The second line assists the first line. It provides complementary expertise and support but also monitors and challenges the first line. The third line provides independent and objective assurance and advice to the governing body (i.e., the board of directors). It is not directly involved in core operations. External assurance providers such as third-party auditors and supervisory authorities provide additional assurance. They are even more independent than internal audit. The model is illustrated in Figure [1](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-fig-0001). Although it has been criticized (Arndorfer & Minto, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0020); Leech & Hanlon, [2016](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0180)), it remains “the most carefully articulated risk management system that has so far been developed” (Davies & Zhivitskaya, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0079)). Most listed companies have implemented the model; it is particularly popular in the financial industry (Bantleon et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0028); Huibers, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0144)).

![Details are in the caption following the image](https://onlinelibrary.wiley.com/cms/asset/468712de-4155-4064-a596-089ec5947878/risa17665-fig-0001-m.png)

Institute of Internal Auditors (IIA)’s Three Lines Model.

In the Three Lines Model, internal audit serves as the third line. It is separate from the first two lines and reports directly to the board of directors, typically its audit committee (Sarens et al., [2009](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0274)). The board plays a key role in corporate governance. It sets the company's strategic priorities, is responsible for risk oversight, and has significant influence over management (e.g., it can replace senior executives) (Zald, [1969](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0329)). However, as nonexecutive board members only work part-time, they rely on information provided to them by the executives (Davies & Zhivitskaya, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0079)), which might only tell the board what they think it wants to hear, not what it needs to hear. As a result, it can be difficult for the board to get an accurate view of the current level of risk and the effectiveness of the company's risk management practices. This situation can be phrased as a principal agent-problem: There is an information asymmetry (problem) between the board, which is legally responsible for risk oversight (principal), and the executives, who are responsible for risk-related activities (agents). The role of internal audit is to tackle this problem by providing the board with independent and objective information. It is often described as the board's “eyes and ears” (IIA, [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0150)). Although the Chief Risk Officer (CRO), the most senior executive responsible for risk management (Karanja & Rosso, [2017](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0166); Li et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0185)), reports on the current level of risk and outcomes of risk management activities, the CAE tells the board how much they can trust these reports (e.g., “their method for evaluating risks is flawed”). As internal audit is organizationally independent from senior management, it is less biased and more objective. With these two reporting lines, the board has a more complete picture.

#### 2.3.2 Combined assurance framework

But internal audit is not the only assurance provider. In many companies, the board also gets reports from other internal assurance providers (e.g., compliance or quality control) and external assurance providers (e.g., external auditors), which might have radically different perspectives on assurance (Roussy & Brivot, [2016](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0266)). If the different assurance activities are performed in isolation, the board can suffer from “assurance fatigue” and assurance gaps can occur (Sarens et al., [2012](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0275)). Put simply, more assurance is not always better. Against this background, the purpose of combined assurance is to “provide holistic assurance to the board on the effectiveness of risk management and internal control systems by coordinating assurance activities from various sources of assurance” (Decaux & Sarens, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0081)).[13](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0013_note_12 "Link to note")

Combined assurance has a number of advantages, including (1) improved knowledge transfer between internal parties, (2) more efficient use of resources by eliminating overlapping efforts, (3) focus on higher risk, and (4) improved communication with senior management and the board (IIA, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0151)). However, many companies seem to find it challenging to implement the framework (PwC, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0256)). A common obstacle is that the Three Lines Model is implemented poorly. Blurred lines might result in irrelevant, inefficient, or inadequate coordination of assurance efforts. Against this background, a clear allocation of assurance responsibilities and communication between the lines is crucial (Huibers, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0144); IIA, [2020, 2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0150%20#risa17665-bib-0151); PwC, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0256)).

As internal audit is typically the main internal assurance provider, it has a special responsibility for promoting combined assurance (Huibers, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0144)). This is also reflected in Standard 9.5 of the IIA's Global Internal Audit Standards, which states that “the chief audit executive must coordinate with internal and external providers of assurance services and consider relying upon their work. Coordination of services minimizes duplication of efforts, highlights gaps in coverage of key risks, and enhances the overall value added by providers” (IIA, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0153)). Figure [2](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-fig-0002) illustrates combined assurance within the Three Lines Model.

![Details are in the caption following the image](https://onlinelibrary.wiley.com/cms/asset/e86e0052-c99a-4a93-ac00-dc73a625afa2/risa17665-fig-0002-m.png)

Combined assurance within the Three Lines Model.

Before we can discuss whether frontier AI developers need an internal audit function, we first need to understand the specific governance challenges they face.

### 2.4 Key governance challenges in frontier AI development

The governance of frontier AI development raises many challenges—from ensuring broad participation in high-stakes governance decisions (Birhane, Isaac et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0037); Delgado et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0082); Seger, Ovadya et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0286)) to managing labor market impacts (Eloundou et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0095); Frank et al., [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0109)). However, this article focuses on governance challenges that may arise in the context of reducing societal risks from frontier AI models.[14](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0014_note_13 "Link to note") Below, I list five key challenges.[15](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0015_note_14 "Link to note") They are summarized in Table [1](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-tbl-0001 "Link to table").

TABLE 1. Key governance challenges in frontier artificial intelligence (AI) development.

| Key challenges | Description |
| --- | --- |
| The expected capabilities problem | Dangerous capabilities can arise unpredictably and undetected, both during development and after deployment |
| The deployment safety problem | It is difficult to prevent a deployed model from causing harm |
| The proliferation problem | Frontier models can proliferate rapidly |
| The risk assessment problem | It is inherently difficult to assess frontier AI risks |
| The risk governance problem | Frontier AI developers do not follow best practices in risk governance |

#### 2.4.1 The unexpected capabilities problem

Dangerous capabilities can arise unpredictably and undetected, both during development and after deployment. In general, model performance tends to improve smoothly with more data, more parameters, and more compute (Bahri et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0026); Hoffmann et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0140); Kaplan et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0164)). However, specific capabilities can emerge sharply, transitioning seemingly instantaneously from not present to present, and unpredictability, appearing at seemingly unforeseeable model scales (Ganguli, Hernandez et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0114); Wei et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0318)).[16](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0016_note_15 "Link to note") But as mentioned above, some of these capabilities might be dangerous (Phuong et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253); Shevlane et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292)). Although there is increasing interest in model evaluations for dangerous capabilities (Phuong et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253); Shevlane et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292)), the field is still nascent and it is not feasible to test for all relevant capabilities (Burden, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0051); Burnell et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0052)). As a result, some capabilities are discovered long after a model has been deployed. This phenomenon has been called “capability overhang” (Clark, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0070)). It is also possible to actively enhance certain dangerous capabilities. For example, malicious actors might fine-tune the model on a task-specific dataset (Goldstein et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0118)) or combine it with external tools (Cai et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0054); Davidson et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0078); Mialon et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0204); Schick et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0279)).

#### 2.4.2 The deployment safety problem

It is difficult to prevent a deployed model from causing harm. One reason is that reliably controlling the behavior of frontier models, also known as “alignment,” remains an unsolved technical problem (Amodei et al., [2016](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0004); Anwar et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0017); Gabriel, [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0111); Hendrycks et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0134); Ji et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0158); Kenton et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0167); Ngo et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0223)).[17](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0017_note_16 "Link to note") Another reason is that safety measures can be circumvented. For example, it is surprisingly easy to “jailbreak” a model (Anil et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0008); Yong et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0328)) or to bypass its safety filters (Rando et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0261)). It is also possible to remove safety measures like RL from human feedback by fine-tuning the model (Gade et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0113); Lermen et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0183); Zhan et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0330)). Finally, frontier models are dual use: They can be used for both beneficial and harmful purposes (Anderljung & Hazell, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0005); Urbina et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0313)). For example, language models can be used to write professional emails, but also phishing messages (Hazell, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0131)). As the harmfulness of frontier models is often highly contextual (Weidinger et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0321)), not merely a model property (Narayanan & Kapoor, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0219)), mitigating misuse risks is particularly challenging (Clifford, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0071)).

#### 2.4.3 The proliferation problem

Frontier models can proliferate rapidly. There are at least four ways in which this can happen. First, many developers make their models available via an application programming interface (Bucknall & Trager, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0050); Shevlane, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0291); Solaiman et al., [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0299); Solaiman, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0298)). This includes models like GPT-4 (OpenAI, [2023b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0234)), Gemini (Google DeepMind, [2023b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0120)), and Claude 3 (Anthropic, [2024b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0015)). Although they typically restrict who can access the model and how they can use it (Soleiman et al., 2019; O'Brien et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0246)), it remains difficult to reliably prevent cases of misuse (see above). Second, models might get reproduced. Popular models often get reproduced or improved upon within 1–2 years of their initial release (Zhao et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0331)). The developers of the reproduced models are typically independent researchers who do not have the resources to take extensive safety measures. Third, some developers open-source their models, that is, they make the model architecture and weights freely and publicly accessible.[18](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0018_note_17 "Link to note") Llama 3 (Meta, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0201)) or BLOOM (Scao et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0277)) are two popular examples. As open-source models provide significant societal benefits, many people in the AI community are generally pro open source (Shrestha et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0293)). However, at some point—though arguably not yet (Kapoor et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0165))—the risks may outweigh the benefits (Bateman et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0030); Seger, Dreksler et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0287)). But as mentioned above, it is difficult to know in advance if a model has certain dangerous capabilities, and once a model has been open-sourced, it cannot be taken back. Fourth, malicious actors might steal the model weights—potentially even before the model has been released. Although some developers take extensive measures to secure their models (Anthropic, [2023b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0010)), sufficient defenses are becoming increasingly costly (Nevo et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0222)).

#### 2.4.4 The risk assessment problem

It is inherently difficult to assess risks from frontier AI models. One reason is that many risks are unprecedented. Although a wide range of potential risks have already been identified (Bommasani et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0041); Hendrycks et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0135); Slattery et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0295); Weidinger et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0320)), some risks will most likely be missing, including so-called black swans (Aven, [2013](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0022); Kolt, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0174); Taleb, [2007](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0306), 2016). The unprecedented nature of many risks also makes it difficult to estimate their impact and likelihood. As historical data are limited, developers often have to rely on forecasts. Although there are robust ways to improve forecasting accuracy (Chang et al., [2016](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0065); Petropoulos et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0252); Tetlock, [2005](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0307); Tetlock & Gardner, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0308)), estimates will be more uncertain and less reliable. Estimates would be even less reliable if, at some point, models become able to detect when they are being tested and only follow instructions in a testing environment (Berglund et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0033); Carlsmith, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0056); Carranza et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0057)). Another reason why assessing frontier AI risk is difficult is that the field is still in its infancy. Despite encouraging developments in some domains, most notably model evaluations (Kinniment et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0169); Liang et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0184); Phuong et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253); Shevlane et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292); Srivastava et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0301); Weidinger et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0322)), best practices have not yet emerged (Schuett et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0283)). Compared to other safety-critical industries like nuclear and aviation, current methods in frontier AI risk assessment seem much less sophisticated (Koessler & Schuett, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0172)). Against this background, we should expect that some risk assessment practices will be flawed and not fit-for-purpose (Rae et al., [2014](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0257); Røyksund & Flage, [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0271)).

#### 2.4.5 The risk governance problem

Although some developers have implemented innovative governance structures—for example, the OpenAI nonprofit governs, the OpenAI LP and owns all returns above a certain threshold (OpenAI, [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0232)), whereas Anthropic's Long-Term Benefit Trust has the power to elect and remove some of Anthropic's board members (Anthropic, [2023c](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0011))—they mostly do not follow existing best practices in risk governance.[19](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0019_note_18 "Link to note") In particular, they do not seem to have established a board risk committee, appointed a CRO, set up an internal audit function, or implemented the Three Lines Model. It is unclear if they even have a central risk function. For example, OpenAI's preparedness team is only responsible for managing catastrophic risks from frontier AI (OpenAI, [2023c](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0235)), whereas Google DeepMind's Responsibility and Safety Council (RSC) is mostly concerned with upholding their AI ethics principles (Google DeepMind, [2023b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0120)). They do not seem to have a team that is responsible for holistic risk management, which is a key element of modern enterprise risk management (Bromiley et al., [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0044); McShane, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0199)). It remains to be seen if OpenAI's recent announcement to create a cross-functional Safety Advisory Group (SAG) will serve this function (OpenAI, [2023f](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0238)). Finally, frontier AI developers do not seem to have a function that is responsible for evaluating the effectiveness of their risk management practices. This is a major shortcoming in light of the risk assessment problem mentioned above.

## 3 ADVANCING THE ARGUMENT

In this section, I argue that frontier AI developers need an internal audit function. Although internal audit already plays a key role in the governance of IT (ISO & IEC, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0156)), the following discussion focuses on how internal audit can address some of the specific governance challenges related to the development of frontier AI models, while also acknowledging key limitations.

### 3.1 How internal audit can address some of the governance challenges in frontier AI development

There are at least three ways in which internal audit can address some of the abovementioned governance challenges.

#### 3.1.1 Identifying ineffective risk management practices

First, internal audit can help to identify ineffective or inadequate risk management practices. For example, it could assess the accuracy and reliability of the developer's model evaluations (Kinniment et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0169); Liang et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0184); Phuong et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253); Shevlane et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292); Srivastava et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0301); Weidinger et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0322)). There are at least four reasons why their evaluations might be inaccurate or unreliable: (1) Developers might simply not conduct evaluations of certain capabilities. To avoid blind spots, internal audit could scrutinize the developer's process for deciding what evaluations to run and engage with industry bodies like the FMF to learn more about what evaluations other developers run. (2) It is often unclear how much information evaluations in a lab environment provide about a model's behavior in the real world. To better understand how evaluation results generalize, internal audit could attempt to predict future evaluation results based on an initial set of evaluations (Chan, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0062)). This would help decision-makers to calibrate their trust in evaluation results. (3) Evaluations are often conducted by the same people who develop the model.[20](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0020_note_19 "Link to note") As researchers and engineers have strong incentives to quickly develop and deploy models, they might—consciously or not—downplay or ignore concerning evaluation results. To reduce this risk, internal audit could oversee discussions of evaluation results and challenge potentially biased interpretations. (4) Most evaluations only look at a model's behavior, without understanding the reasons for the behavior (Hubinger, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0141)).[21](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0021_note_20 "Link to note") This approach would be unreliable if, at some point, models become able to deceive the people who conduct the evaluations and only pretend to behave in a certain way (Anthropic, [2024a](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0014); Carlsmith, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0056); Hagendorff, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0130); Hubinger et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0143); Järviniemi & Hubinger, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0157); Pacchiardi et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0247); Park et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0248)), a concern that has been coined “sandbagging” (van der Weij et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0323)). Depending on their technical expertise, internal auditors could empirically test the extent to which a model demonstrates deceptive capabilities and tendencies, perhaps similar to Anthropic's Alignment Stress-Testing Team (Hubinger, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0142); Hubinger et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0143); MacDiarmid et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0191)).

Internal audit could also assess the adequacy of the developer's security measures (Anthropic, [2023b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0010)). For example, a recent RAND report suggests that frontier AI developers are currently unable to defend against cyberattacks from advanced threat actors (Nevo et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0222)). Internal audit could commission an external red team to identify vulnerabilities. It could also monitor compliance with relevant security standards such as the NIST Secure Software Development Framework (NIST, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0226)) and the Supply Chain Levels for Software Artifacts (OpenSSF, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0243)).

In addition to that, internal audit could verify whether the developer complies with its AI safety framework.[22](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0022_note_21 "Link to note") For example, Anthropic's Responsible Scaling Policy (RSP) (Anthropic, [2023a](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0009)), OpenAI's Preparedness Framework (OpenAI, [2023f](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0238)), and Google DeepMind's Frontier Safety Framework (Google DeepMind, [2024a](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0121)) all contain commitments to pause the development and deployment process if their safety measures are inadequate for the model's level of capabilities. But as a pause would be extremely costly for the developer, the people involved have strong incentives to conclude that the safety measures are adequate (Alaga & Schuett, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0002)). They might adopt a box-ticking mindset and only superficially comply with the commitments in their framework. To find out if this is the case, internal audit could attend meetings, conduct interviews, and review documents. Taken together, these examples illustrate that, without a deliberate attempt to identify ineffective or inadequate risk management practices, some shortcomings will likely remain unnoticed.

#### 3.1.2 Better informed board of directors

Second, internal audit can ensure that the board of directors has a more accurate view of the current level of risk and the effectiveness of the company's risk management practices. At many frontier AI developers, the board of directors plays a special role. The recent scandal around some of OpenAI's board members—who first fired CEO Sam Altman (OpenAI, [2023d](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0236)), and then had to resign after internal criticisms and a public outcry (OpenAI, [2023g, 2024b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0239%20#risa17665-bib-0241))—serves as a cautionary example. The board members reportedly felt that Altman withheld important information and even lied to them (Duhigg, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0092)). However, as many details of the case are not yet public, such reports should be taken with a grain of salt. It is difficult to tell what, if anything, an internal audit function would have changed in this particular case. But it shows that information asymmetries between the board and senior management can be extremely consequential. Shortly after the board scandal, OpenAI published their Preparedness Framework (OpenAI, [2023f](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0238)). According to this framework, the new board needs to be involved in certain decisions and can even overrule senior management. Anthropic's RSP contains similar provisions (Anthropic, [2023a](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0009)). For example, the board needs to approve updates of the RSP. As the boards of frontier developers are increasingly involved in high-stakes decisions, they need independent and objective information about those decisions. Setting up an internal audit function could help to address this challenge.

#### 3.1.3 Contact point for whistleblowers

Third, internal audit could serve as a contact for whistleblowers. Detecting misconduct is often difficult: It is hard to observe from the outside, whereas insiders might not report it because they face a conflict between personal values and loyalty (Dungan et al., [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0093); Jubb, [1999](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0162)), or because they fear retaliation (Bjørkelo, [2013](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0040)). For example, an engineer might become convinced that a model shows early signs of power-seeking behavior (Krakovna & Kramar, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0177); Ngo et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0223); Turner & Tadepalli, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0312); Turner et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0311)), but the research lead might want to release the model anyway and threatens to fire the engineer if they speak up. In such cases, whistleblower protection is vital. This might become even more important as commercial pressure increases because it might incentivize developers to cut corners on safety (Armstrong et al., [2016](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0019); Naudé & Dimitri, [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0221)). Internal audit could protect whistleblowers by providing a trusted contact point. It would be more trustworthy than other organizational units because it is organizational independent from management. But as it would still be part of the organization, confidentiality would be less of a problem. This can be particularly important if the information is highly sensitive and its dissemination could be harmful in itself (Bostrom, [2011](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0042); Urbina et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0313)). Internal audit could report the case to the board of directors who could engage with management to address the issue. In the example, this could avoid rapid proliferation of a model with potentially dangerous capabilities. Internal audit could also advise the whistleblower on other steps they could take to protect themselves or do something about the misconduct. Finally, it increases the chance that concerns during the development phase are also taken into consideration. Although this is not a typical role of internal audit, it has been discussed in the literature (Jubb, [1999](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0162)).

### 3.2 Limitations

However, internal audit also has limitations. The following limitations seem particularly relevant for frontier AI developers, even though they are not unique to AI.

#### 3.2.1 Internal audit adds friction

The internal audit team interacts with many different people, including C-suite executives and senior researchers. To them, internal audit might seem annoying, distracting, and bureaucratic. They might even be actively opposed if they fear that internal audit discovers flaws in their work,[23](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0023_note_22 "Link to note") especially if they can be held personally liable for their mistakes (Thekdi & Aven, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0309)). Internal audit might also (indirectly) delay decisions. For example, OpenAI spent 6 months on safety research, risk assessment, and iteration before releasing GPT-4 (OpenAI, [2023b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0234)). Although this level of scrutiny is commendable, it seems plausible that internal audit would have found flaws in some of their risk assessment methods. Depending on the severity of these flaws, internal audit would presumably have escalated the issue to the board, which might have started an investigation that might have resulted in an improvement of the methods and a repetition of the initial assessment. This could have delayed the release for additional weeks or months. To be clear, such a delay would not only be in the interest of society, it might also be in the developers’ own interest. Rushed releases can expose companies to significant financial and reputational risks. For example, Alphabet's share price dropped 9%—$100 billion in market value—after Google's chatbot Bard made some mistakes in a public demo (Coulter & Bensinger, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0077)). Similarly, Microsoft faced negative press coverage after users reported that Bing Chat sometimes gives bizarre answers and even threatens users (Perrigo, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0251); Roose, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0264)).

#### 3.2.2 Internal audit can be captured by senior management

In theory, the internal audit function should be organizationally independent from senior management. In practice, however, senior management often finds informal ways to exercise influence. Becoming and remaining independent is an ongoing challenge of every internal audit function (Guénin-Paracini et al., [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0124); Nordin, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0229); Roussy, [2013](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0265); Roussy & Rodrigue, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0268); Stewart & Subramaniam, [2010](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0304)). Against this background, external assurance providers are an important complement. In the context of frontier AI developers, this mainly includes external auditors (Anderljung, Smith et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0007); Birhane et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0039); Mökander & Floridi, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0210); Mökander et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0212); Raji & Buolamwini, [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0258); Raji et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0260)), independent red teams (Anthropic, [2023d](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0012); Ganguli, Lovitt et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0115); Perez et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0250)), and ethics boards (Schuett et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0283)).

#### 3.2.3 Benefits depend on individuals

Simply having an internal audit function is not sufficient to seize the abovementioned benefits. The value of internal audit mainly depends on the people involved and their ability and willingness to identify ineffective risk management practices. Unfortunately, there do not seem to be many candidates who have both AI and internal audit expertise. As internal audit has traditionally had a strong financial focus (Abbott et al., [2016](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0001)) and frontier AI is a more recent phenomenon, skill gaps and the absence of clearly developed assurance methodologies will be a challenge. Frontier AI developers could either hire internal audit professionals and train them in AI (“bring AI to internal audit”) or they could train AI governance experts in internal audit (“bring internal audit to AI”). Since both approaches have advantages and disadvantages, developers should arguably do a mix of both. The abilities of internal auditors can also be enhanced by AI. AI is already used to support internal audit in other companies (Couceiro et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0076); Emett et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0097); Kahyaoglu & Aksoy, [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0163)) and using AI to oversee AI is a common theme in the technical safety debate (Bowman et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0043); Burns et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0053); Christiano et al., [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0068); Irving et al., [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0154); Kenton et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0168); Leike et al., [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0181)).

### 3.3 Discussion

The abovementioned limitations do not provide an argument against internal audit per se. In particular, they are not enough to outweigh the benefits. Instead, decision-makers should be aware of the limitations and proactively address them. Below, I discuss how best practices from internal IT audits could be applied to AI and how internal AI audits can be aligned with other assurance activities. I also argue that we will eventually need frontier AI regulation.

#### 3.3.1 Internal AI audits should incorporate best practices from internal IT audits where appropriate

As internal AI audits can be seen as a subset of internal IT audits (ISACA, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0155)), it generally makes sense to apply existing best practices from internal IT audits to AI. However, due to the specific governance challenges in frontier AI development (see Section [2.4](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0160)), existing best practices from internal IT audit need to be modified and adapted to an AI context. Below, I summarize six critical factors that influence internal IT audit quality, as identified by Merhout and Havelka ([2008](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0200)), discuss how they can be applied to internal AI audits, and flag potential challenges (Table [2](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-tbl-0002 "Link to table")).

TABLE 2. Applying best practices from internal information technology (IT) audits to internal artificial intelligence (AI) audits.

| Best practices in internal IT audit (Merhout & Havelka, [2008](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0200)) | Applying best practices from internal IT audits to internal AI audits | Potential challenges of applying best practices from internal IT audits to internal AI audits |
| --- | --- | --- |
| Internal IT audit teams should use an audit methodology | Internal AI audit teams should use an AI-specific audit methodology informed by best practices in frontier AI safety and governance | - Internal audit methodologies for frontier AI do not yet exist - Due to the specific governance challenges of frontier AI, existing IT audit methodologies are insufficient for frontier AI - Creating audit methodologies for frontier AI is difficult because best practices in frontier AI safety and governance do not yet exist - Audit methodologies for frontier AI might quickly become outdated because the AI risk landscape and the state of the art in AI safety and governance are rapidly evolving |
| Internal IT audit teams should have enough time to conduct audits (e.g., for fieldwork) | Internal AI audit teams should have enough time to deeply engage with and scrutinize the developer's safety and governance practices (e.g., threat models, model evaluations, and alignment techniques) | - Frontier AI developers have strong economic incentives to release models quickly. Internal audit teams might therefore be pressured to rush audits |
| Internal IT audit teams should have support from senior management | Internal AI audit teams should have support from senior management, especially the executives who are responsible for AI research, product development, and risk management | - At least historically, frontier AI developers wanted to “move fast and break things.” They were generally opposed to formal processes and structures - Senior management might not see the need to support internal audit. This is because frontier AI developers are not legally required to set up an internal audit function. It is also unclear to what extent key stakeholders like business partners and investors expect developers to set up an internal audit function on a voluntary basis |
| Auditees should respond quickly to inquiries from the internal IT audit team. Communication should be honest and open. The relationship should be based on mutual trust | Teams working on frontier AI safety and governance (e.g., members of the alignment team) should respond quickly to inquiries from the internal AI audit team. Developers should promote collaboration and trust between different teams | - At some frontier AI developers, there seems to be a divide between product and safety teams, which makes it more challenging to promote collaboration and trust - Trust builds over time. But as developers currently do not have an internal audit function, we should expect some initial reservations |
| Internal IT audit teams should have support to adapt to organizational change (e.g., restructurings of business processes) | Internal AI audit teams should have support to adapt to organizational change (e.g., by hiring more auditors and giving them more time) | - It might be difficult to get the necessary support given the potential lack of buy-in from senior management and the divide between product and safety teams (see above) |
| The scope and objectives of IT audits should be clearly defined | The scope and objectives of the AI audit should be clearly defined | - The definition of the scope and objectives may need to be updated frequently because the AI risk landscape and the state of the art in AI safety and governance are rapidly evolving |

#### 3.3.2 Internal audit needs to be aligned with other assurance providers

Aligning different assurance providers is particularly important in the context of frontier AI. Due to the specific governance challenges of frontier AI development (see Section [2.4](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-sec-0160)), it is important that senior management and the board of directors have a good understanding of the level of risk and the adequacy of risk management practices, including key uncertainties and potential deficiencies. They need multiple sources of assurance. However, if these sources are not aligned, senior management and the board might form incorrect views about risks or risk management practices. For example, the board of directors might think that the company's safety framework is effective, without knowing that the company has not implemented parts of the framework.

However, aligning different assurance providers of frontier AI developers raises a distinct set of challenges: (1) As of today, there is not much to align. Existing assurance activities focus mainly on red teaming (Anthropic, [2023d](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0012); Ganguli, Lovitt et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0115); Perez et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0250)) and model evaluations (Anthropic, [2023e](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0013); Phuong et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253); Shevlane et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292)). There is hardly any assurance of developers’ internal governance. (2) Best practices in frontier AI assurance are still emerging. It is difficult to align assurance activities if they are constantly changing. (3) Safety practices and governance structures are also changing. It is difficult to develop best practices for assurance activities if their target is constantly moving. (4) Frontier AI developers do not seem to be aware of the need to align different assurance activities. The issue has not been raised by developers, policymakers, or scholars.

To overcome these and other challenges, frontier AI developers may take the following measures: (1) Developers should expand their assurance activities. In addition to setting up an internal audit function, they should commission governance audits that assess the adequacy of their safety frameworks and verify that they adhere to them. (2) Developers should encourage different assurance providers to speak the same language. Among other things, they should use the same terms, concepts, risk taxonomies, threat models, and thresholds. (3) Developers should encourage different assurance providers to submit integrated reports where appropriate (Kolt et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0175)). It should be avoided that each assurance provider submits their own report. (4) Developers should facilitate information sharing among different assurance providers. For example, third parties that conduct model evaluations (e.g., METR, Apollo Research, and the UK AI Safety Institute) should exchange information about evaluation protocols, results, and challenges. (5) Developers should support research on assurance methodologies for frontier AI. However, it is important that such research remains independent and is not influenced by industry interests.

#### 3.3.3 The need for frontier AI regulation

Although it may be in developers’ own interest to set up an internal audit function, we should not rely on voluntary adoption. Developers should not be able to “grade their own homework.” We will eventually need frontier AI regulation (Anderljung, Barnhart et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0006); Schuett et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0284)), which may also include the requirement to have an internal audit function,[24](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0024_note_23 "Link to note") similar to other industries (e.g., European Banking Authority, [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0103)).[25](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0025_note_24 "Link to note") The EU AI Act does not contain any rules on internal audit (see Mahler, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0192); Novelli et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0230); Schuett, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0281)), though the Code of Practice that will concretize the requirements on GPAI model providers may do (European Commission, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0104)). It remains to be seen if upcoming AI regulation in the United Kingdom will be any different (DSIT, [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0085); [2023b](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0087)). Although the NIST AI risk management framework does not mention internal audit either, it seems to describe its role in the Govern 1.5 function: “ongoing monitoring and periodic review of the risk management process and its outcomes are planned and organizational roles and responsibilities clearly defined, including determining the frequency of periodic review” (NIST, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0227)). Despite the standard's voluntary nature, it might become the basis for future AI regulation, similar to the NIST cybersecurity standards (NIST, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0225)). The recent Executive Order on Safe, Secure, and Trustworthy AI, in which NIST plays a key role, is a promising sign in this direction (The White House, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0324)).

## 4 CONCLUSION

### 4.1 Main contributions

This article has made three main contributions. First, it has introduced internal audit to the frontier AI governance discourse. Although internal audit is a well-established governance mechanism in many other industries, it has been largely neglected in the debate around the governance of frontier AI development. Inversely, the article has introduced the key governance challenges of frontier AI development to the field of risk science (Aven, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0024)). Second, the article has advanced the argument that frontier AI developers need an internal audit function. It has discussed how internal audit could address some of the key governance challenges in frontier AI development, while acknowledging key limitations. It has also suggested ways in which best practices from internal IT audits can be applied to internal AI audits and how internal AI audits can be aligned with other assurance activities. Third, the article contributes to the creation of best practices in frontier AI governance (Schuett et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0283)). In particular, it can inform ongoing initiatives by industry organizations (e.g., the FMF), standard-setting bodies (e.g., NIST and ISO/IEC), and government authorities (e.g., the AI Office in the EU or DSIT in the United Kingdom).

### 4.2 Questions for further research

At the same time, the article has left many questions unanswered. In particular, it has not engaged with the more practical question of how exactly frontier AI developers should set up an internal audit function and what that function would do on a day-to-day basis. Future research could adapt the IIA's Global Internal Audit Standards to the context of frontier AI development (IIA, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0153)). Once the first developer has set up an internal audit function, scholars could turn toward more empirical research on its effectiveness, drawing from similar research from other industries (e.g., Carcello et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0055); Lenz & Hahn, [2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0182); Oussii & Boulila Taktak, [2018](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0245)). To address some of the challenges involved in transferring best practices from internal IT audits to AI (Table [2](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-tbl-0002 "Link to table")), it will be particularly useful to gather empirical evidence. An industry case study similar to the one that Mökander and Floridi ([2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0210)) have conducted on ethics-based auditing could be a first step. The article has also not made any concrete policy recommendations. Scholars could review the regulatory concept in other domains—where rules on internal audit are often set out in guidelines, not legislation (e.g., European Banking Authority, [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0103))—to learn lessons for frontier AI regulations (Anderljung, Barnhart et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0006); Schuett et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0284)). Such work could directly inform the regulatory debate on frontier AI regulation in the UK (DSIT, [2023c](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0088)). Similarly, scholars could suggest specifications on internal audit for the NIST AI risk management framework (Barrett et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0029); NIST, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0227), [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0228)) or harmonized standards for the EU AI Act (McFadden et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0198); Schuett, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0281); Soler Garrido et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0300)). In general, I wish to encourage more scholars, especially from the field of risk analysis, to contribute to the development of best practices in frontier AI governance (Schuett et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0283)). Collaborations with researchers from industry may be particularly fruitful, though scholars should pay special attention to concerns around industry capture (Whittaker, [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0325)).

### 4.3 An opportunity for frontier AI developers

Frontier AI developers that have an internal audit function might be perceived as being more responsible. As they transition from startups to more mature companies, many stakeholders (business partners, regulators, etc.) expect them to follow best practices in corporate governance. By setting up an internal audit function, developers would signal that they take risk governance seriously, which might increase their perceived legitimacy.[26](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-note-0026_note_25 "Link to note") If they do it before regulators mandate action, they would also signal proactivity. As none of the frontier AI developers seems to have an internal audit function, the first developer would likely get most of the PR benefits. They have an opportunity to be ahead of the curve.

In light of rapid progress in AI research and development, frontier AI developers need to strengthen their risk governance. Instead of reinventing the wheel, they should follow existing best practices. Although this might not be sufficient, they should not skip this obvious first step.

## ACKNOWLEDGMENTS

I am grateful for valuable comments and feedback from Caroline Baumoehl, Malcolm Murray, Leonie Koessler, Markus Anderljung, James Ginns, Lennart Heim, Ben Garfinkel, and Jide Alaga. I would also like to thank three anonymous reviewers whose comments have greatly improved the manuscript. All remaining errors are my own.

## CONFLICT OF INTEREST STATEMENT

The author declares no conflicts of interest.

## 

REFERENCES

- Abbott, L. J., Daugherty, B., Parker, S., & Peters, G. F. (2016). Internal audit quality and financial reporting quality: The joint importance of independence and competence. *Journal of Accounting Research*, 54(1), 3–40. [https://doi.org/10.1111/1475-679X.12099](https://doi.org/10.1111/1475-679X.12099)
- Alaga, J., & Schuett, J. (2023). *Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers*. arXiv. [https://arxiv.org/abs/2310.00374](https://arxiv.org/abs/2310.00374)
- Altman, S. (2023). *Planning for AGI and beyond*. OpenAI. [https://perma.cc/3A67-Z38F](https://perma.cc/3A67-Z38F)
- Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). *Concrete problems in AI safety*. arXiv. [https://arxiv.org/abs/1606.06565](https://arxiv.org/abs/1606.06565)
- Anderljung, M., & Hazell, J. (2023). *Protecting society from AI misuse: When are restrictions on capabilities warranted?* arXiv. [https://arxiv.org/abs/2303.09377](https://arxiv.org/abs/2303.09377)
- Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O'Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, T., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., … Wolf, K. (2023). *Frontier AI regulation: Managing emerging risks to public safety*. arXiv. [https://arxiv.org/abs/2307.03718](https://arxiv.org/abs/2307.03718)
- Anderljung, M., Smith, E. T., O'Brien, J., Soder, L., Bucknall, B., Bluemke, E., Schuett, J., Trager, R., Strahm, L., & Chowdhury, R. (2023). *Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework*. arXiv. [https://arxiv.org/abs/2311.14711](https://arxiv.org/abs/2311.14711)
- Anil, C., Durmus, E., Sharma, M., Benton, J., Kundu, S., Batson, J., Rimsky, N., Tong, M., Mu, J., Ford, D., Mosconi, F., Agrawal, R., Schaeffer, R., Bashkansky, N., Svenningsen, S., Lambert, M., Radhakrishnan, A., Denison, C., Hubinger, E. J., … Duvenaud, D. (2024). *Many-shot jailbreaking*. Anthropic. [https://perma.cc/NP7Q-KSKX](https://perma.cc/NP7Q-KSKX)
- Anthropic. (2023a, September 19). *Anthropic's Responsible Scaling Policy (Version 1.0)*. Anthropic. [https://perma.cc/R4MG-6W4H](https://perma.cc/R4MG-6W4H)
- Anthropic. (2023b, July 25). *Frontier model security*. Anthropic. [https://perma.cc/6HQ4-XV73](https://perma.cc/6HQ4-XV73)
- Anthropic. (2023c, September 19). *The long-term benefit trust*. Anthropic. [https://perma.cc/RPZ3-QT52](https://perma.cc/RPZ3-QT52)
- Anthropic. (2023d, July 26). *Frontier threats red teaming for AI safety*. Anthropic. [https://perma.cc/9QST-7SLE](https://perma.cc/9QST-7SLE)
- Anthropic. (2023e, October 4). *Challenges in evaluating AI systems*. Anthropic. [https://perma.cc/L5NB-8Q4W](https://perma.cc/L5NB-8Q4W)
- Anthropic. (2024a, April 9). *Measuring the persuasiveness of language models*. Anthropic. [https://perma.cc/YVQ8-YBSY](https://perma.cc/YVQ8-YBSY)
- Anthropic. (2024b, March 4). *The Claude 3 model family: Opus, sonnet, haiku*. Anthropic. [https://perma.cc/XBD5-3GV7](https://perma.cc/XBD5-3GV7)
- Anthropic. (2024c, June 21). *Claude 3.5 Sonnet model card addendum*. Anthropic. [https://perma.cc/73AP-Z9PT](https://perma.cc/73AP-Z9PT)
- Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper, S., Sourbut, O., Edelman, B. L., Zhang, Z., Günther, M., Korinek, A., Hernandez-Orallo, J., Hammond, L., Bigelow, E., Pan, A., Langosco, L., … Krueger, D. (2024). *Foundational challenges in assuring alignment and safety of large language models*. arXiv. [https://arxiv.org/abs/2404.09932](https://arxiv.org/abs/2404.09932)
- Arena, M., & Azzone, G. (2009). Identifying organizational drivers of internal audit effectiveness. *International Journal of Auditing*, 13(1), 43–60. [https://doi.org/10.1111/j.1099-1123.2008.00392.x](https://doi.org/10.1111/j.1099-1123.2008.00392.x)
- Armstrong, S., Bostrom, N., & Shulman, C. (2016). Racing to the precipice: A model of artificial intelligence development. *AI & Society*, 31(2), 201–206. [https://doi.org/10.1007/s00146-015-0590-y](https://doi.org/10.1007/s00146-015-0590-y)
- Arndorfer, I., & Minto, A. (2015). *The “Four Lines of Defence Model” for financial institutions*. Financial Stability Institute, Bank for International Settlements. [https://perma.cc/UP35-KEYJ](https://perma.cc/UP35-KEYJ)
- van Asselt, M. B. A., & Renn, O. (2011). Risk governance. *Journal of Risk Research*, 14(4), 431–449. [https://doi.org/10.1080/13669877.2011.553730](https://doi.org/10.1080/13669877.2011.553730)
- Aven, T. (2013). On the meaning of a black swan in a risk context. *Safety Science*, 57, 44–51. [https://doi.org/10.1016/j.ssci.2013.01.016](https://doi.org/10.1016/j.ssci.2013.01.016)
- Aven, T. (2016). Risk assessment and risk management: Review of recent advances on their foundation. *European Journal of Operational Research*, 253(1), 1–13. [https://doi.org/10.1016/j.ejor.2015.12.023](https://doi.org/10.1016/j.ejor.2015.12.023)
- Aven, T. (2018). An emerging new risk analysis science: Foundations and implications. *Risk Analysis*, 38(5), 876–888. [https://doi.org/10.1111/risa.12899](https://doi.org/10.1111/risa.12899)
- Avin, S., Belfield, H., Brundage, M., Krueger, G., Wang, J., Weller, A., Anderljung, M., Krawczuk, I., Krueger, D., Lebensold, J., Maharaj, T., & Zilberman, N. (2021). Filling gaps in trustworthy development of AI. *Science*, 374(6573), 1327–1329. [https://doi.org/10.1126/science.abi7176](https://doi.org/10.1126/science.abi7176)
- Bahri, Y., Dyer, E., Kaplan, J., Lee, J., & Sharma, U. (2021). *Explaining neural scaling laws*. arXiv. [https://arxiv.org/abs/2102.06701](https://arxiv.org/abs/2102.06701)
- Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., … Kaplan, J. (2022). *Constitutional AI: Harmlessness from AI feedback*. arXiv. [https://arxiv.org/abs/2212.08073](https://arxiv.org/abs/2212.08073)
- Bantleon, U., d'Arcy, A., Eulerich, M., Hucke, A., Pedell, B., & Ratzinger-Sakel, N. V. S. (2021). Coordination challenges in implementing the three lines of defense model. *International Journal of Auditing*, 25(1), 59–74. [https://doi.org/10.1111/ijau.12201](https://doi.org/10.1111/ijau.12201)
- Barrett, A. M., Newman, J., Nonnecke, B., Hendrycks, D., Murphy, & Jackson, K. (2023). AI risk-management standards profile for general-purpose AI systems (GPAIS) and foundation models. Center for Long-Term Cybersecurity. [https://perma.cc/8W6P-2UUK](https://perma.cc/8W6P-2UUK)
- Bateman, J., Baer, D., Bell, S. A., Brown, G. O., Cuéllar, M.-F., Ganguli, D., Henderson, P., Kotila, B., Lessig, L., Lundblad, N. B., Napolitano, J., Raji, D., Seger, E., Sheehan, M., Skowron, A., Solaiman, I., Toner, H., & Zvyagina, P. (2024). *Beyond open vs. closed: Emerging consensus and key questions for foundation AI model governance*. Carnegie Endowment for International Peace. [https://perma.cc/5PQK-E39U](https://perma.cc/5PQK-E39U)
- Baum, S. D. (2024). Assessing the risk of takeover catastrophe from large language models. *Risk Analysis*. [https://doi.org/10.1111/risa.14353](https://doi.org/10.1111/risa.14353)
- Bengio, Y., Hinton, G., Yao, A., Song, D., Abbeel, P., Harari, Y. N., Zhang, Y.-Q., Xue, L., Shalev-Shwartz, S., Hadfield, G., Clune, J., Maharaj, T., Hutter, F., Baydin, A. G., McIlraith, S., Gao, Q., Acharya, A., Krueger, D., Dragan, A., … Mindermann, S. (2023). Managing extreme AI risks in an era of rapid progress. *Science*, 384(6698), 842–845. [https://doi.org/10.1126/science.adn0117](https://doi.org/10.1126/science.adn0117)
- Berglund, L., Stickland, A. C., Balesni, M., Kaufmann, M., Tong, M., Korbak, T., Kokotajlo, D., & Evans, O. (2023). *Taken out of context: On measuring situational awareness in LLMs*. arXiv. [http://arxiv.org/abs/2309.00667](http://arxiv.org/abs/2309.00667)
- Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., Manassra, W., Dhariwal, P., Chu, C., Jiao, Y., & Ramesh, A. (2023). *Improving image generation with better captions*. OpenAI. [https://perma.cc/5DYB-ZJQ5](https://perma.cc/5DYB-ZJQ5)
- Betti, N., & Sarens, G. (2021). Understanding the internal audit function in a digitalised business environment. *Journal of Accounting & Organizational Change*, 17(2), 197–216. [https://doi.org/10.1108/JAOC-11-2019-0114](https://doi.org/10.1108/JAOC-11-2019-0114)
- Bhatnagar, S., Alexandrova, A., Avin, S., Cave, S., Cheke, L., Crosby, M., Feyereisl, J., Halina, M., Loe, B. S., Ó hÉigeartaigh, S., Martínez-Plumed, F., Price, H., Shevlin, H., Weller, A., Winfield, A., & Hernández-Orallo, J. (2018). Mapping intelligence: Requirements and possibilities. In V. C. Müller (Ed.), Philosophy and theory of artificial intelligence 2017 (pp. 117–135). Springer. [https://doi.org/10.1007/978-3-319-96448-5\_13](https://doi.org/10.1007/978-3-319-96448-5_13)
- Birhane, A., Isaac, W., Prabhakaran, V., Diaz, M., Elish, M. C., Gabriel, I., & Mohamed, S. (2022). Power to the people? Opportunities and challenges for participatory AI. In ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (pp. 1–8). ACM. [https://doi.org/10.1145/3551624.3555290](https://doi.org/10.1145/3551624.3555290)
- Birhane, A., Ruane, E., Laurent, T., S Brown, M., Flowers, J., Ventresque, A., & L Dancy, C. (2022). The forgotten margins of AI ethics. *ACM Conference on Fairness, Accountability, and Transparency* (pp. 948–958). ACM. [https://doi.org/10.1145/3531146.3533157](https://doi.org/10.1145/3531146.3533157)
- Birhane, A., Steed, R., Ojewale, V., Vecchione, B., & Raji, I. D. (2024). AI auditing: The broken bus on the road to AI accountability. In *IEEE Conference on Secure and Trustworthy Machine Learning* (pp. 612-643). IEEE. [https://doi.org/10.1109/SaTML59370.2024.00037](https://doi.org/10.1109/SaTML59370.2024.00037)
- Bjørkelo, B. (2013). Workplace bullying after whistleblowing: Future research and implications. *Journal of Managerial Psychology*, 28(3), 306–323. [https://doi.org/10.1108/02683941311321178](https://doi.org/10.1108/02683941311321178)
- Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., … Liang, P. (2021). *On the opportunities and risks of foundation models*. arXiv. [https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258)
- Bostrom, N. (2011). Information hazards: A typology of potential harms from knowledge. *Review of Contemporary Philosophy*, 10, 44–79.
- Bowman, S. R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., Lukošiūtė, K., Askell, A., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Olah, C., Amodei, D., Amodei, D., Drain, D., Li, D., Tran-Johnson, E., … Kaplan, J. (2022). *Measuring progress on scalable oversight for large language models*. arXiv. [http://arxiv.org/abs/2211.03540](http://arxiv.org/abs/2211.03540)
- Bromiley, P., McShane, M., Nair, A., & Rustambekov, E. (2015). Enterprise risk management: Review, critique, and research directions. *Long Range Planning*, 48(4), 265–276. [https://doi.org/10.1016/j.lrp.2014.07.005](https://doi.org/10.1016/j.lrp.2014.07.005)
- Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., & Ramesh, A. (2024, February 15). *Video generation models as world simulators*. OpenAI. [https://perma.cc/Z85Z-JZVB](https://perma.cc/Z85Z-JZVB)
- Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A., Scharre, P., Zeitzoff, T., Filar, B., Anderson, H., Roff, H., Allen, G. C., Steinhardt, J., Flynn, C., Ó hÉigeartaigh, S., Beard, S., Belfield, H., Farquhar, S., … Amodei, D. (2018). *The malicious use of artificial intelligence: Forecasting, prevention, and mitigation*. arXiv. [https://arxiv.org/abs/1802.07228](https://arxiv.org/abs/1802.07228)
- Brundage, M., Avin, S., Wang, J., Belfield, H., Krueger, G., Hadfield, G., Khlaaf, H., Yang, J., Toner, H., Fong, R., Maharaj, T., Koh, P. W., Hooker, S., Leung, J., Trask, A., Bluemke, E., Lebensold, J., O'Keefe, C., Koren, M., … Anderljung, M. (2020). *Toward trustworthy AI development: Mechanisms for supporting verifiable claims*. arXiv. [https://arxiv.org/abs/2004.07213](https://arxiv.org/abs/2004.07213)
- Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., & Zhang, Y. (2023). *Sparks of artificial general intelligence: Early experiments with GPT-4*. arXiv. [https://arxiv.org/abs/2303.12712](https://arxiv.org/abs/2303.12712)
- Büchling, M., Cerbone, D., & Maroun, W. (2023). Assurance, risk and governance: An international perspective ( 3rd ed.). Juta.
- Bucknall, B. S., & Trager, R. F. (2023). Structured access for third-party research on frontier AI models: Investigating researchers’ model access requirements. Centre for the Governance of AI. [https://perma.cc/2FAH-URD6](https://perma.cc/2FAH-URD6)
- Burden, J. (2024). *Evaluating AI evaluation: Perils and prospects*. arXiv. [http://arxiv.org/abs/2407.09221](http://arxiv.org/abs/2407.09221)
- Burnell, R., Schellaert, W., Burden, J., Ullman, T. D., Martinez-Plumed, F., Tenenbaum, J. B., Rutar, D., Cheke, L. G., Sohl-Dickstein, J., Mitchell, M., Kiela, D., Shanahan, M., Voorhees, E. M., Cohn, A. G., Leibo, J. Z., & Hernandez-Orallo, J. (2023). Rethink reporting of evaluation results in AI. *Science*, 380(6641), 136–138. [https://doi.org/10.1126/science.adf6369](https://doi.org/10.1126/science.adf6369)
- Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., Sutskever, I., & Wu, J. (2023). *Weak-to-strong generalization: Eliciting strong capabilities with weak supervision*. arXiv. [http://arxiv.org/abs/2312.09390](http://arxiv.org/abs/2312.09390)
- Cai, T., Wang, X., Ma, T., Chen, X., & Zhou, D. (2023). *Large language models as tool makers*. arXiv. [http://arxiv.org/abs/2305.17126](http://arxiv.org/abs/2305.17126)
- Carcello, J. V., Eulerich, M., Masli, A., & Wood, D. A. (2020). Are internal audits associated with reductions in perceived risk? *Auditing: A Journal of Practice & Theory*, 39(3), 55–73. [https://doi.org/10.2308/ajpt-19-036](https://doi.org/10.2308/ajpt-19-036)
- Carlsmith, J. (2023). *Scheming AIs: Will AIs fake alignment during training in order to get power?* arXiv. [https://arxiv.org/abs/2311.08379](https://arxiv.org/abs/2311.08379)
- Carranza, A., Pai, D., Schaeffer, R., Tandon, A., & Koyejo, S. (2023). *Deceptive alignment monitoring*. arXiv. [https://arxiv.org/abs/2307.10569](https://arxiv.org/abs/2307.10569)
- Cascarino, R. (2015). Internal auditing: An integrated approach ( 3rd ed.). Juta.
- Castelvecchi, D. (2016). Can we open the black box of AI? *Nature*, 538(7623), 20–23. [https://doi.org/10.1038/538020a](https://doi.org/10.1038/538020a)
- Center for AI Safety. (2023). *Statement on AI risk*. Center for AI Safety. [https://perma.cc/4ZPL-JQ4W](https://perma.cc/4ZPL-JQ4W)
- Chambers, A. D., & Odar, M. (2015). A new vision for internal audit. *Managerial Auditing Journal*, 30(1), 34–55. [https://doi.org/10.1108/MAJ-08-2014-1073](https://doi.org/10.1108/MAJ-08-2014-1073)
- Chan, A. (2024, April 9). Evaluating predictions of model behaviour. Centre for the Governance of AI. [https://perma.cc/YN9D-9HV3](https://perma.cc/YN9D-9HV3)
- Chan, A., Salganik, R., Markelius, A., Pang, C., Rajkumar, N., Krasheninnikov, D., Langosco, L., He, Z., Duan, Y., Carroll, M., Lin, M., Mayhew, A., Collins, K., Molamohammadi, M., Burden, J., Zhao, W., Rismani, S., Voudouris, K., Bhatt, U., … Maharaj, T. (2023). Harms from increasingly agentic algorithmic systems. In *ACM Conference on Fairness, Accountability, and Transparency* (pp. 651–666). ACM. [https://doi.org/10.1145/3593013.3594033](https://doi.org/10.1145/3593013.3594033)
- Chan, A., Ezell, C., Kaufmann, M., Wei, K., Hammond, L., Bradley, H., Bluemke, E., Rajkumar, N., Krueger, D., Kolt, N., Heim, L., & Anderljung, M. (2024). Visibility into AI agents. In *ACM Conference on Fairness, Accountability, and Transparency* (pp. 958–973). ACM. [https://doi.org/10.1145/3630106.3658948](https://doi.org/10.1145/3630106.3658948)
- Chang, W., Chen, E., Mellers, B., & Tetlock, P. (2016). Developing expert political judgment: The impact of training and practice on judgmental accuracy in geopolitical forecasting tournaments. *Judgment and Decision Making*, 11(5), 509–526. [https://doi.org/10.1017/S1930297500004599](https://doi.org/10.1017/S1930297500004599)
- Christ, M. H., Eulerich, M., Krane, R., & Wood, D. A. (2021). New frontiers for internal audit research. *Accounting Perspectives*, 20(4), 449–475. [https://doi.org/10.1111/1911-3838.12272](https://doi.org/10.1111/1911-3838.12272)
- Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., & Amodei, D. (2017). *Deep reinforcement learning from human preferences*. arXiv. [https://arxiv.org/abs/1706.03741](https://arxiv.org/abs/1706.03741)
- Christiano, P., Shlegeris, B., & Amodei, D. (2018). *Supervising strong learners by amplifying weak experts*. arXiv. [http://arxiv.org/abs/1810.08575](http://arxiv.org/abs/1810.08575)
- Cihon, P., Schuett, J., & Baum, S. D. (2021). Corporate governance of artificial intelligence in the public interest. *Information*, 12(7), 275. [https://doi.org/10.3390/info12070275](https://doi.org/10.3390/info12070275)
- Clark, J. (2022). *Import AI 310: AlphaZero learned Chess like humans learn Chess; capability emergence in language models; demoscene AI*. Import AI. [https://perma.cc/K4FG-ZXMX](https://perma.cc/K4FG-ZXMX)
- Clifford, B. (2023, December 17). Preventing AI misuse: Current techniques. Centre for the Governance of AI. [https://perma.cc/6XLS-2ZNQ](https://perma.cc/6XLS-2ZNQ)
- Coetzee, G. P., du Bruyn, R., Fourie, H., & Plant, K. (2024). Internal auditing: An introduction ( 7th ed.). LexisNexis.
- Coetzee, P., & Lubbe, D. (2014). Improving the efficiency and effectiveness of risk-based internal audit engagements. *International Journal of Auditing*, 18(2), 115–125. [https://doi.org/10.1111/ijau.12016](https://doi.org/10.1111/ijau.12016)
- Cohen, M. K., Kolt, N., Bengio, Y., Hadfield, G. K., & Russell, S. (2024). Regulating advanced artificial agents: Governance frameworks should address the prospect of AI systems that cannot be safely tested. *Science*, 384(6691), 36–38. [https://doi.org/10.1126/science.adl0625](https://doi.org/10.1126/science.adl0625)
- Coram, P., Ferguson, C., & Moroney, R. (2008). Internal audit, alternative internal audit structures and the level of misappropriation of assets fraud. *Accounting & Finance*, 48(4), 543–559. [https://doi.org/10.1111/j.1467-629X.2007.00247.x](https://doi.org/10.1111/j.1467-629X.2007.00247.x)
- Couceiro, B., Pedrosa, I., & Marini, A. (2020). State of the art of artificial intelligence in internal audit context. In *Iberian Conference on Information Systems and Technologies (CISTI)* (pp. 1–7). IEEE. [https://doi.org/10.23919/CISTI49556.2020.9140863](https://doi.org/10.23919/CISTI49556.2020.9140863)
- Coulter, M., & Bensinger, G. (2023, February 9). Alphabet shares dive after Google AI chatbot Bard flubs answer in ad. Reuters. [https://perma.cc/PQ9P-5JAR](https://perma.cc/PQ9P-5JAR)
- Davidson, T., Denain, J.-S., Villalobos, P., & Bas, G. (2023). *AI capabilities can be significantly improved without expensive retraining*. arXiv. [http://arxiv.org/abs/2312.07413](http://arxiv.org/abs/2312.07413)
- Davies, H., & Zhivitskaya, M. (2018). Three lines of defence: A robust organising framework, or just lines in the sand? *Global Policy*, 9(S1), 34–42. [https://doi.org/10.1111/1758-5899.12568](https://doi.org/10.1111/1758-5899.12568)
- De Haes, S., & Van Grembergen, W. (2009). An exploratory study into IT governance implementations and its impact on business/IT alignment. *Information Systems Management*, 26(2), 123–137. [https://doi.org/10.1080/10580530902794786](https://doi.org/10.1080/10580530902794786)
- Decaux, L., & Sarens, G. (2015). Implementing combined assurance: Insights from multiple case studies. *Managerial Auditing Journal*, 30(1), 56–79. [https://doi.org/10.1108/MAJ-08-2014-1074](https://doi.org/10.1108/MAJ-08-2014-1074)
- Delgado, F., Yang, S., Madaio, M., & Yang, Q. (2023). The participatory turn in AI design: Theoretical foundations and the current state of practice. In *ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization* (pp. 1–23). ACM. [https://doi.org/10.1145/3617694.3623261](https://doi.org/10.1145/3617694.3623261)
- Drogalas, G., Pazarskis, M., Anagnostopoulou, E., & Papachristou, A. (2017). The effect of internal audit effectiveness, auditor responsibility and training in fraud detection. *Journal of Accounting and Management Information Systems*, 16(4), 434–454. [https://doi.org/10.24818/jamis.2017.04001](https://doi.org/10.24818/jamis.2017.04001)
- DSIT. (2022). *A pro-innovation approach to AI regulation*. DSIT. [https://perma.cc/W7TF-VYQL](https://perma.cc/W7TF-VYQL)
- DSIT. (2023a). *AI Safety Summit: Introduction*. DSIT. [https://perma.cc/TJA2-SA76](https://perma.cc/TJA2-SA76)
- DSIT. (2023b). *Capabilities and risks from frontier AI*. DSIT. [https://perma.cc/U9L6-UCBS](https://perma.cc/U9L6-UCBS)
- DSIT. (2023c). *Emerging processes for frontier AI safety*. DSIT. [https://perma.cc/Y9AY-5B4E](https://perma.cc/Y9AY-5B4E)
- DSIT. (2023d). *The Bletchley declaration by countries attending the AI safety summit, 1–2 November 2023*. DSIT. [https://perma.cc/EZ66-6KHE](https://perma.cc/EZ66-6KHE)
- DSIT. (2024a). *Seoul ministerial statement for advancing AI safety, innovation and inclusivity: AI Seoul summit 2024*. DSIT. [https://perma.cc/VG5H-YC3T](https://perma.cc/VG5H-YC3T)
- DSIT. (2024b). *Frontier AI safety commitments, AI Seoul summit 2024*. DSIT. [https://perma.cc/Y9ZR-AXRG](https://perma.cc/Y9ZR-AXRG)
- Duhigg, C. (2023, December 1). *The inside story of Microsoft's partnership with OpenAI*. The New Yorker. [https://perma.cc/NL8J-Z5GZ](https://perma.cc/NL8J-Z5GZ)
- Dungan, J., Waytz, A., & Young, L. (2015). The psychology of whistleblowing. *Current Opinion in Psychology*, 6, 129–133. [https://doi.org/10.1016/j.copsyc.2015.07.005](https://doi.org/10.1016/j.copsyc.2015.07.005)
- Egan, J., & Heim, L. (2023). *Oversight for frontier AI through a know-your-customer scheme for compute providers*. arXiv. [https://arxiv.org/abs/2310.13625](https://arxiv.org/abs/2310.13625)
- Eloundou, T., Manning, S., Mishkin, P., & Rock, D. (2024). GPTs are GPTs: Labor market impact potential of LLMs. *Science*, 384(6702), 1306–1308. [https://doi.org/10.1126/science.adj0998](https://doi.org/10.1126/science.adj0998)
- El-Sayed, S., Akbulut, C., McCroskery, A., Keeling, G., Kenton, Z., Jalan, Z., Marchal, N., Manzini, A., Shevlane, T., Vallor, S., Susser, D., Franklin, M., Bridgers, S., Law, H., Rahtz, M., Shanahan, M., Tessler, M. H., Douillard, A., Everitt, T., & Brown, S. (2024). *A mechanism-based approach to mitigating harms from persuasive generative AI*. arXiv. [http://arxiv.org/abs/2404.15058](http://arxiv.org/abs/2404.15058)
- Emett, S. A., Eulerich, M., Lipinski, E., Prien, N., & Wood, D. A. (2024). Leveraging ChatGPT for enhancing the internal audit process: A real-world example from a large multinational company. *Accounting Horizons*, 1–11. [https://doi.org/10.2139/ssrn.4514238](https://doi.org/10.2139/ssrn.4514238)
- Enriques, L., & Zetzsche, D. A. (2013). The risky business of regulating risk management in listed companies. *European Company and Financial Law Review*, 103(3), 271–303. [https://doi.org/10.1515/ecfr-2013-0271](https://doi.org/10.1515/ecfr-2013-0271)
- Erasmus, L., & Coetzee, P. (2018). Drivers of stakeholders’ view of internal audit effectiveness: Management versus audit committee. *Managerial Auditing Journal*, 33(1), 90–114. [https://doi.org/10.1108/MAJ-05-2017-1558](https://doi.org/10.1108/MAJ-05-2017-1558)
- von Eschenbach, W. J. (2021). Transparency and the black box problem: Why we do not trust AI. *Philosophy & Technology*, 34(4), 1607–1622. [https://doi.org/10.1007/s13347-021-00477-0](https://doi.org/10.1007/s13347-021-00477-0)
- Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., Lacey, K., Goodwin, A., Marek, Y., & Rombach, R. (2024). *Scaling rectified flow transformers for high-resolution image synthesis*. arXiv. [http://arxiv.org/abs/2403.03206](http://arxiv.org/abs/2403.03206)
- Eulerich, A., & Eulerich, M. (2020). What is the value of internal auditing? A literature review on qualitative and quantitative perspectives. *Maandblad Voor Accountancy En Bedrijfseconomie*, 94(3/4), 83–92. [https://doi.org/10.5117/mab.94.50375](https://doi.org/10.5117/mab.94.50375)
- European Banking Authority. (2021). *Guidelines on internal governance under Directive 2013/36/EU* (EBA/GL/2021/05). European Banking Authority. [https://perma.cc/RCD8-V99V](https://perma.cc/RCD8-V99V)
- European Commission. (2024, July 30). *AI Act: Participate in the drawing-up of the first general-purpose AI code of practice*. European Commission. [https://perma.cc/KFA2-QD5N](https://perma.cc/KFA2-QD5N)
- Falco, G., Shneiderman, B., Badger, J., Carrier, R., Dahbura, A., Danks, D., Eling, M., Goodloe, A., Gupta, J., Hart, C., Jirotka, M., Johnson, H., LaPointe, C., Llorens, A. J., Mackworth, A. K., Maple, C., Pálsson, S. E., Pasquale, F., Winfield, A., & Yeong, Z. K. (2021). Governing AI safety through independent audits. *Nature Machine Intelligence*, 3(7), 566–571. [https://doi.org/10.1038/s42256-021-00370-7](https://doi.org/10.1038/s42256-021-00370-7)
- Fang, R., Bindu, R., Gupta, A., & Kang, D. (2024). *LLM agents can autonomously exploit one-day vulnerabilities*. arXiv. [https://arxiv.org/abs/2404.08144](https://arxiv.org/abs/2404.08144)
- Fjelland, R. (2020). Why general artificial intelligence will not be realized. *Humanities and Social Sciences Communications*, 7, 10. [https://doi.org/10.1057/s41599-020-0494-4](https://doi.org/10.1057/s41599-020-0494-4)
- Forte, J., & Barac, K. (2015). Combined assurance: A systematic process. *Southern African Journal of Accountability and Auditing Research*, 17(2), 71–83. [https://perma.cc/42XM-JWVE](https://perma.cc/42XM-JWVE)
- Frank, M. R., Autor, D., Bessen, J. E., Brynjolfsson, E., Cebrian, M., Deming, D. J., Feldman, M., Groh, M., Lobo, J., Moro, E., Wang, D., Youn, H., & Rahwan, I. (2019). Toward understanding the impact of artificial intelligence on labor. *PNAS*, 116(14), 6531–6539. [https://doi.org/10.1073/pnas.1900949116](https://doi.org/10.1073/pnas.1900949116)
- Frontier Model Forum. (2023). *Advancing safe AI development*. Frontier Model Forum. [https://perma.cc/LK7G-ZPCP](https://perma.cc/LK7G-ZPCP)
- Gabriel, I. (2020). Artificial intelligence, values, and alignment. *Minds and Machines*, 30(3), 411–437. [https://doi.org/10.1007/s11023-020-09539-2](https://doi.org/10.1007/s11023-020-09539-2)
- Gabriel, I., Manzini, A., Keeling, G., Hendricks, L. A., Rieser, V., Iqbal, H., Tomašev, N., Ktena, I., Kenton, Z., Rodriguez, M., El-Sayed, S., Brown, S., Akbulut, C., Trask, A., Hughes, E., Bergman, A. S., Shelby, R., Marchal, N., Griffin, C., … Manyika, J. (2024). *The ethics of advanced AI assistants*. arXiv. [https://arxiv.org/abs/2404.16244](https://arxiv.org/abs/2404.16244)
- Gade, P., Lermen, S., Rogers-Smith, C., & Ladish, J. (2023). *BadLlama: Cheaply removing safety fine-tuning from Llama 2-Chat 13B*. arXiv. [http://arxiv.org/abs/2311.00117](http://arxiv.org/abs/2311.00117)
- Ganguli, D., Hernandez, D., Lovitt, L., Askell, A., Bai, Y., Chen, A., Conerly, T., Dassarma, N., Drain, D., Elhage, N., El Showk, S., Fort, S., Hatfield-Dodds, Z., Henighan, T., Johnston, S., Jones, A., Joseph, N., Kernian, J., Kravec, S., … Clark, J. (2022). Predictability and surprise in large generative models. In *ACM Conference on Fairness, Accountability, and Transparency* (pp. 1747–1764). ACM. [https://doi.org/10.1145/3531146.3533229](https://doi.org/10.1145/3531146.3533229)
- Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., Jones, A., Bowman, S., Chen, A., Conerly, T., DasSarma, N., Drain, D., Elhage, N., El-Showk, S., Fort, S., … Clark, J. (2022). *Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned*. arXiv. [https://arxiv.org/abs/2209.07858](https://arxiv.org/abs/2209.07858)
- Goertzel, B. (2014). Artificial general intelligence: Concept, state of the art, and future prospects. *Journal of Artificial General Intelligence*, 5(1), 1–48. [https://doi.org/10.2478/jagi-2014-0001](https://doi.org/10.2478/jagi-2014-0001)
- B. Goertzel, & C. Pennachin (Eds.). (2007). Artificial general intelligence. Springer. [https://doi.org/10.1007/978-3-540-68677-4](https://doi.org/10.1007/978-3-540-68677-4)
- Goldstein, J. A., Sastry, G., Musser, M., DiResta, R., Gentzel, M., & Sedova, K. (2023). *Generative language models and automated influence operations: Emerging threats and potential mitigations*. arXiv. [http://arxiv.org/abs/2301.04246](http://arxiv.org/abs/2301.04246)
- Google DeepMind. (2023a, October 27). *AI safety summit: An update on our approach to safety and responsibility*. Google DeepMind. [https://perma.cc/EJ9S-HDFY](https://perma.cc/EJ9S-HDFY)
- Google DeepMind. (2023b). *Responsibility & safety*. Google DeepMind. [https://perma.cc/LLR2-PT9J](https://perma.cc/LLR2-PT9J)
- Google DeepMind. (2024a). *Frontier safety framework (Version 1.0)*. Google DeepMind. [https://perma.cc/3C44-RSAN](https://perma.cc/3C44-RSAN)
- Google DeepMind. (2024b). *Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context*. arXiv. [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530)
- Gopal, A., Helm-Burger, N., Justen, L., Soice, E. H., Tzeng, T., Jeyapragasan, G., Grimm, S., Mueller, B., & Esvelt, K. M. (2023). *Will releasing the weights of future large language models grant widespread access to pandemic agents?* arXiv. [https://arxiv.org/abs/2310.18233](https://arxiv.org/abs/2310.18233)
- Gruetzemacher, R., Chan, A., Frazier, K., Manning, C., Los, Š., Fox, J., Hernández-Orallo, J., Burden, J., Franklin, M., Ghuidhir, C. N., Bailey, M., Eth, D., Pilditch, T., & Kilian, K. (2023). *An international consortium for evaluations of societal-scale risks from advanced AI*. arXiv. [https://arxiv.org/abs/2310.14455](https://arxiv.org/abs/2310.14455)
- Guénin-Paracini, H., Malsch, B., & Tremblay, M.-S. (2015). On the operational reality of auditors’ independence: Lessons from the field. *Auditing: A Journal of Practice & Theory*, 34(2), 201–236. [https://doi.org/10.2308/ajpt-50905](https://doi.org/10.2308/ajpt-50905)
- Gutierrez, C. I., Aguirre, A., Uuk, R., Boine, C. C., & Franklin, M. (2023). A proposal for a definition of general purpose artificial intelligence systems. *Digital Society*, 2(3), 36. [https://doi.org/10.1007/s44206-023-00068-w](https://doi.org/10.1007/s44206-023-00068-w)
- Gwern. (2020, May 28). *The scaling hypothesis*. Gwern. [https://perma.cc/A4YJ-567Q](https://perma.cc/A4YJ-567Q)
- Hackenburg, K., & Margetts, H. (2024). Evaluating the persuasive influence of political microtargeting with large language models, *PNAS*, 121(24). [https://doi.org/10.1073/pnas.2403116121](https://doi.org/10.1073/pnas.2403116121)
- Hacker, P., Engel, A., & Mauer, M. (2023). Regulating ChatGPT and other large generative AI models. In *ACM Conference on Fairness, Accountability, and Transparency* (pp. 1112–1123). ACM. [https://doi.org/10.1145/3593013.3594067](https://doi.org/10.1145/3593013.3594067)
- Hagendorff, T. (2020). The ethics of AI ethics: An evaluation of guidelines. *Minds and Machines*, 30(1), 99–120. [https://doi.org/10.1007/s11023-020-09517-8](https://doi.org/10.1007/s11023-020-09517-8)
- Hagendorff, T. (2024). Deception abilities emerged in large language models. *PNAS*, 121(24), e2317967121. [https://doi.org/10.1073/pnas.2317967121](https://doi.org/10.1073/pnas.2317967121)
- Hazell, J. (2023). *Spear phishing with large language models*. arXiv. [https://arxiv.org/abs/2305.06972](https://arxiv.org/abs/2305.06972)
- Heim, L., & Koessler, L. (2024). *Training compute thresholds: Features and functions in AI regulation*. arXiv. [http://arxiv.org/abs/2405.10799](http://arxiv.org/abs/2405.10799)
- Helfrich, G. (2024). The harms of terminology: Why we should reject so-called “frontier AI”. *AI and Ethics*, 4, 1–7. [https://doi.org/10.1007/s43681-024-00438-1](https://doi.org/10.1007/s43681-024-00438-1)
- Hendrycks, D., Carlini, N., Schulman, J., & Steinhardt, J. (2021). *Unsolved problems in ML safety*. arXiv. [https://arxiv.org/abs/2109.13916](https://arxiv.org/abs/2109.13916)
- Hendrycks, D., Mazeika, M., & Woodside, T. (2023). *An overview of catastrophic AI risks*. arXiv. [https://arxiv.org/abs/2306.12001](https://arxiv.org/abs/2306.12001)
- Hermanson, D. R., Hill, M. C., & Ivancevich, D. M. (2000). Information technology-related activities of internal auditors. *Journal of Information Systems*, 14(s-1), 39–53. [https://doi.org/10.2308/jis.2000.14.s-1.39](https://doi.org/10.2308/jis.2000.14.s-1.39)
- Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., & Zhou, Y. (2017). *Deep learning scaling is predictable, empirically*. arXiv. [http://arxiv.org/abs/1712.00409](http://arxiv.org/abs/1712.00409)
- Ho, L., Barnhart, J., Trager, R., Bengio, Y., Brundage, M., Carnegie, A., Chowdhury, R., Dafoe, A., Hadfield, G., Levi, M., & Snidal, D. (2023). *International institutions for advanced AI*. arXiv. [http://arxiv.org/abs/2307.04699](http://arxiv.org/abs/2307.04699)
- Hooker, S. (2024). *On the limitations of compute thresholds as a governance strategy*. arXiv. [http://arxiv.org/abs/2407.05694](http://arxiv.org/abs/2407.05694)
- Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., … Sifre, L. (2022). *Training compute-optimal large language models*. arXiv. [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)
- Hubinger, E. (2023, March 15). *Towards understanding-based safety evaluations* \[Online forum post\]. AI Alignment Forum. [https://perma.cc/LM58-MZKA](https://perma.cc/LM58-MZKA)
- Hubinger, E. (2024, January 13). *Introducing alignment stress-testing at Anthropic* \[Online forum post\]. AI Alignment Forum. [https://perma.cc/M8Q2-CDRS](https://perma.cc/M8Q2-CDRS)
- Hubinger, E., Denison, C., Mu, J., Lambert, M., Tong, M., MacDiarmid, M., Lanham, T., Ziegler, D. M., Maxwell, T., Cheng, N., Jermyn, A., Askell, A., Radhakrishnan, A., Anil, C., Duvenaud, D., Ganguli, D., Barez, F., Clark, J., Ndousse, K., … Perez, E. (2024). *Sleeper agents: Training deceptive LLMs that persist through safety training*. arXiv. [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)
- Huibers, S. C. J. (2015). Combined assurance: One language, one voice, one view. IIA Research Foundation, Global Internal Audit Common Body of Knowledge. [https://perma.cc/D7YM-9GSY](https://perma.cc/D7YM-9GSY)
- IEEE. (2008). *Standard for software reviews and audits* (IEEE Standard No. 1028-2008). IEEE. [https://doi.org/10.1109/IEEESTD.2008.4601584](https://doi.org/10.1109/IEEESTD.2008.4601584)
- IIA. (2013). *The three lines of defense in effective risk management and control*. IIA. [https://perma.cc/NQM2-DD7V](https://perma.cc/NQM2-DD7V)
- IIA. (2017a). *Artificial intelligence: Considerations for the profession of internal auditing (Part I)*. IIA. [https://perma.cc/K8WQ-VNFZ](https://perma.cc/K8WQ-VNFZ)
- IIA. (2017b). *The IIA's artificial intelligence auditing framework: Practical applications (Part A)*. IIA. [https://perma.cc/U93U-LN75](https://perma.cc/U93U-LN75)
- IIA. (2018). *The IIA's artificial intelligence auditing framework: Practical applications (Part B)*. IIA. [https://perma.cc/826X-Y3L7](https://perma.cc/826X-Y3L7)
- IIA. (2020). *The IIA's three lines model: An update of the three lines of defense*. IIA. [https://perma.cc/GAB5-DMN3](https://perma.cc/GAB5-DMN3)
- IIA. (2022). *Combined assurance: Aligning assurance for effective risk management*. IIA. [https://perma.cc/A65C-8Q4F](https://perma.cc/A65C-8Q4F)
- IIA. (2023). *Certified internal auditor*. IIA. [https://perma.cc/6PE7-CGFW](https://perma.cc/6PE7-CGFW)
- IIA. (2024). *Global internal audit standards*. IIA. [https://perma.cc/2SV3-YEF3](https://perma.cc/2SV3-YEF3)
- Irving, G., Christiano, P., & Amodei, D. (2018). *AI safety via debate*. arXiv. [http://arxiv.org/abs/1805.00899](http://arxiv.org/abs/1805.00899)
- ISACA. (2018). *Auditing artificial intelligence*. ISACA. [https://perma.cc/J53N-5P5F](https://perma.cc/J53N-5P5F)
- ISO & IEC. (2024). *Information technology: Governance of IT for the organization* (ISO/IEC Standard No. 38500:2024). [https://www.iso.org/standard/81684.html](https://www.iso.org/standard/81684.html)
- Järviniemi, O., & Hubinger, E. (2024). *Uncovering deceptive tendencies in language models: A simulated company AI assistant*. arXiv. [http://arxiv.org/abs/2405.01576](http://arxiv.org/abs/2405.01576)
- Ji, J., Qiu, T., Chen, B., Zhang, B., Lou, H., Wang, K., Duan, Y., He, Z., Zhou, J., Zhang, Z., Zeng, F., Ng, K. Y., Dai, J., Pan, X., O'Gara, A., Lei, Y., Xu, H., Tse, B., Fu, J., & Gao, W. (2023). *AI alignment: A comprehensive survey*. arXiv. [https://arxiv.org/abs/2310.19852](https://arxiv.org/abs/2310.19852)
- Jiang, L., Messier Jr., W. F., & Wood, D. A. (2020). The association between internal audit operations-related services and firm operating performance. *Auditing: A Journal of Practice & Theory*, 39(1), 101–124. [https://doi.org/10.2308/ajpt-52565](https://doi.org/10.2308/ajpt-52565)
- Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. *Nature Machine Intelligence*, 1(9), 389–399. [https://doi.org/10.1038/s42256-019-0088-2](https://doi.org/10.1038/s42256-019-0088-2)
- Jones, E. (2023, July 17). Explainer: What is a foundation model? Ada Lovelace Institute. [https://perma.cc/JQP5-S5G5](https://perma.cc/JQP5-S5G5)
- Jubb, P. B. (1999). Whistleblowing: A restrictive definition and interpretation. *Journal of Business Ethics*, 21(1), 77–94. [https://doi.org/10.1023/A:1005922701763](https://doi.org/10.1023/A:1005922701763)
- Kahyaoglu, S. B., & Aksoy, T. (2021). Artificial intelligence in internal audit and risk assessment. In U. Hacioglu & T. Aksoy (Eds.), Financial ecosystem and strategy in the digital era: Global approaches and new opportunities (pp. 179–192). Springer. [https://doi.org/10.1007/978-3-030-72624-9\_8](https://doi.org/10.1007/978-3-030-72624-9_8)
- Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). *Scaling laws for neural language models*. arXiv. [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)
- Kapoor, S., Bommasani, R., Klyman, K., Longpre, S., Ramaswami, A., Cihon, P., Hopkins, A., Bankston, K., Biderman, S., Bogen, M., Chowdhury, R., Engler, A., Henderson, P., Jernite, Y., Lazar, S., Maffulli, S., Nelson, A., Pineau, J., Skowron, A., … Narayanan, A. (2024). *On the societal impact of open foundation models*. arXiv. [https://arxiv.org/abs/2403.07918](https://arxiv.org/abs/2403.07918)
- Karanja, E., & Rosso, M. A. (2017). The chief risk officer: A study of roles and responsibilities. *Risk Management*, 19(2), 103–130. [https://doi.org/10.1057/s41283-017-0014-z](https://doi.org/10.1057/s41283-017-0014-z)
- Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik, V., & Irving, G. (2021). *Alignment of language agents*. arXiv. [https://arxiv.org/abs/2103.14659](https://arxiv.org/abs/2103.14659)
- Kenton, Z., Siegel, N. Y., Kramár, J., Brown-Cohen, J., Albanie, S., Bulian, J., Agarwal, R., Lindner, D., Tang, Y., Goodman, N. D., & Shah, R. (2024). *On scalable oversight with weak LLMs judging strong LLMs*. arXiv. [http://arxiv.org/abs/2407.04622](http://arxiv.org/abs/2407.04622)
- Kinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R., Wijk, H., Burget, J., Ho, A., Barnes, E., & Christiano, P. (2023). *Evaluating language-model agents on realistic autonomous tasks*. arXiv. [https://arxiv.org/abs/2312.11671](https://arxiv.org/abs/2312.11671)
- Klinke, A., & Renn, O. (2021). The coming of age of risk governance. *Risk Analysis*, 41(3), 544–557. [https://doi.org/10.1111/risa.13383](https://doi.org/10.1111/risa.13383)
- Koessler, L., & Schuett, J. (2023). *Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries*. arXiv. [https://arxiv.org/abs/2307.08823](https://arxiv.org/abs/2307.08823)
- Koessler, L., Schuett, J., & Anderljung, M. (2024). *Risk thresholds for frontier AI*. arXiv. [http://arxiv.org/abs/2406.14713](http://arxiv.org/abs/2406.14713)
- Kolt, N. (2023). Algorithmic black swans. *Washington University Law Review*, 101, 1177–1240. [https://perma.cc/WRZ8-5R52](https://perma.cc/WRZ8-5R52)
- Kolt, N., Anderljung, M., Barnhart, J., Brass, A., Esvelt, K., Hadfield, G. K., Heim, L., Rodriguez, M., Sandbrink, J. B., & Woodside, T. (2024). *Responsible reporting for frontier AI development*. arXiv. [https://arxiv.org/abs/2404.02675](https://arxiv.org/abs/2404.02675)
- Kotb, A., Elbardan, H., & Halabi, H. (2020). Mapping of internal audit research: A post-Enron structured literature review. *Accounting, Auditing & Accountability Journal*, 33(8), 1969–1996. [https://doi.org/10.1108/AAAJ-07-2018-3581](https://doi.org/10.1108/AAAJ-07-2018-3581)
- Krakovna, V., & Kramar, J. (2023). *Power-seeking can be probable and predictive for trained agents*. arXiv. [https://arxiv.org/abs/2304.06528](https://arxiv.org/abs/2304.06528)
- Kurzweil, R. (1990). The age of intelligent machines. MIT Press.
- Lambert, N., Castricato, L., von Werra, L., & Havrilla, A. (2022, December 9). Illustrating reinforcement learning from human feedback (RLHF). Hugging Face. [https://perma.cc/R9HU-TQ9X](https://perma.cc/R9HU-TQ9X)
- Leech, T. J., & Hanlon, L. C. (2016). Three lines of defense versus five lines of assurance: Elevating the role of the board and CEO in risk governance. In R. Leblanc (Ed.), The handbook of board governance: A comprehensive guide for public, private and not-for-profit board members (pp. 335–355). Wiley. [https://doi.org/10.1002/9781119245445.ch17](https://doi.org/10.1002/9781119245445.ch17)
- Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., & Legg, S. (2018). *Scalable agent alignment via reward modeling: A research direction*. arXiv. [http://arxiv.org/abs/1811.07871](http://arxiv.org/abs/1811.07871)
- Lenz, R., & Hahn, U. (2015). A synthesis of empirical internal audit effectiveness literature pointing to new research opportunities. *Managerial Auditing Journal*, 30(1), 5–33. [https://doi.org/10.1108/MAJ-08-2014-1072](https://doi.org/10.1108/MAJ-08-2014-1072)
- Lermen, S., Rogers-Smith, C., & Ladish, J. (2023). *LoRA fine-tuning efficiently undoes safety training in Llama 2-Chat 70B*. arXiv. [http://arxiv.org/abs/2310.20624](http://arxiv.org/abs/2310.20624)
- Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., Ré, C., Acosta-Navas, D., Hudson, D. A., … Koreeda, Y. (2022). *Holistic evaluation of language models*. arXiv. [http://arxiv.org/abs/2211.09110](http://arxiv.org/abs/2211.09110)
- Li, H., Lam, H. K. S., Ho, W., & Yeung, A. C. L. (2022). The impact of chief risk officer appointments on firm risk and operational efficiency. *Journal of Operations Management*, 68(3), 241–269. [https://doi.org/10.1002/joom.1175](https://doi.org/10.1002/joom.1175)
- Lin, S., Pizzini, M., Vargus, M., & Bardhan, I. R. (2011). The role of the internal audit function in the disclosure of material weaknesses. *The Accounting Review*, 86(1), 287–323. [https://doi.org/10.2308/accr.00000016](https://doi.org/10.2308/accr.00000016)
- Lohn, A. J., & Jackson, K. A. (2022). Will AI make cyber swords or shields: A few mathematical models of technological progress. Center for Security and Emerging Technology. [https://doi.org/10.51593/2022CA002](https://doi.org/10.51593/2022CA002)
- Lohn, A. J., & Musser, M. (2022). AI and compute: How much longer can computing power drive artificial intelligence progress? Center for Security and Emerging Technology. [https://doi.org/10.51593/2021CA009](https://doi.org/10.51593/2021CA009)
- Longpre, S., Kapoor, S., Klyman, K., Ramaswami, A., Bommasani, R., Blili-Hamelin, B., Huang, Y., Skowron, A., Yong, Z.-X., Kotha, S., Zeng, Y., Shi, W., Yang, X., Southen, R., Robey, A., Chao, P., Yang, D., Jia, R., Kang, D., … Henderson, P. (2024). *A safe harbor for AI evaluation and red teaming*. arXiv. [https://arxiv.org/abs/2403.04893](https://arxiv.org/abs/2403.04893)
- Lundqvist, S. A. (2015). Why firms implement risk governance: Stepping beyond traditional risk management to enterprise risk management. *Journal of Accounting and Public Policy*, 34(5), 441–466. [https://doi.org/10.1016/j.jaccpubpol.2015.05.002](https://doi.org/10.1016/j.jaccpubpol.2015.05.002)
- MacDiarmid, M., Maxwell, T., Schiefer, N., Mu, J., Kaplan, J., Duvenaud, D., Bowman, S., Tamkin, A., Perez, E., Sharma, M., Denison, C., & Hubinger, E. (2024, April 23). Simple probes can catch sleeper agents. Anthropic. [https://perma.cc/2BPK-BQNK](https://perma.cc/2BPK-BQNK)
- Mahler, T. (2022). Between risk management and proportionality: The risk-based approach in the EU's Artificial Intelligence Act proposal. In L. Colonna & S. Greenstein (Eds.), Nordic yearbook of law and informatics 2020-2021: Law in the era of artificial intelligence (pp. 247–270). [https://doi.org/10.53292/208f5901.38a67238](https://doi.org/10.53292/208f5901.38a67238)
- Marchal, N., Xu, R., Elasmar, R., Gabriel, I., Goldberg, B., & Isaac, W. (2024). *Generative AI misuse: A taxonomy of tactics and insights from real-world data*. arXiv. [http://arxiv.org/abs/2406.13843](http://arxiv.org/abs/2406.13843)
- Maslej, N., Fattorini, L., Perrault, R., Parli, V., Reuel, A., Brynjolfsson, E., Etchemendy, J., Ligett, K., Lyons, T., Manyika, J., Niebles, J. C., Shoham, Y., Wald, R., & Clark, J. (2024). *The AI index 2024 annual report*. Institute for Human-Centered AI, Stanford University. [https://perma.cc/EV87-JNS8](https://perma.cc/EV87-JNS8)
- Ma'ayan, Y., & Carmeli, A. (2016). Internal audits as a source of ethical behavior, efficiency, and effectiveness in work units. *Journal of Business Ethics*, 137(2), 347–363. [https://doi.org/10.1007/s10551-015-2561-0](https://doi.org/10.1007/s10551-015-2561-0)
- McCarthy, J. (2007). *What is artificial intelligence?* Stanford University. [https://perma.cc/QL9Y-AY8A](https://perma.cc/QL9Y-AY8A)
- McCarthy, J., Minsky, M. L., Rochester, N., & Shannon, C. E. (1955). A proposal for the Dartmouth summer research project on artificial intelligence. *AI Magazine*, 27, 12. [https://perma.cc/S9DU-GWFF](https://perma.cc/S9DU-GWFF)
- McFadden, M., Jones, K., Taylor, E., & Osborn, G. (2021). Harmonising artificial intelligence: The role of standards in the EU AI regulation. Oxford Information Labs. [https://perma.cc/X3AZ-5H7C](https://perma.cc/X3AZ-5H7C)
- McShane, M. (2018). Enterprise risk management: History and a design science proposal. *The Journal of Risk Finance*, 19(2), 137–153. [https://doi.org/10.1108/JRF-03-2017-0048](https://doi.org/10.1108/JRF-03-2017-0048)
- Merhout, J. W., & Havelka, D. (2008). Information technology auditing: A value-added IT governance partnership between IT management and audit. *Communications of the Association for Information Systems*, 23, 463–482. [https://doi.org/10.17705/1CAIS.02326](https://doi.org/10.17705/1CAIS.02326)
- Meta. (2024). *Llama 3 model card*. GitHub. [https://perma.cc/6D5U-DDSV](https://perma.cc/6D5U-DDSV)
- METR. (2023, March 17). *Update on ARC's recent eval efforts*. METR. [https://perma.cc/87KZ-GN56](https://perma.cc/87KZ-GN56)
- Meyer, B. (2011, October 28). John McCarthy. *Communications of the ACM*. [https://perma.cc/49S8-3GM6](https://perma.cc/49S8-3GM6)
- Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozière, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., Grave, E., LeCun, Y., … Scialom, T. (2023). *Augmented language models: A survey*. arXiv. [http://arxiv.org/abs/2302.07842](http://arxiv.org/abs/2302.07842)
- Minsky, M. (1969). Semantic information processing. MIT Press.
- Mirsky, Y., Demontis, A., Kotak, J., Shankar, R., Gelei, D., Yang, L., Zhang, X., Lee, W., Elovici, Y., & Biggio, B. (2021). *The threat of offensive AI to organizations*. arXiv. [https://arxiv.org/abs/2106.15764](https://arxiv.org/abs/2106.15764)
- Mitchell, M. (2021). *Why AI is harder than we think*. arXiv. [https://arxiv.org/abs/2104.12871](https://arxiv.org/abs/2104.12871)
- Mitchell, M. (2024). Debates on the nature of artificial general intelligence. *Science*, 383(6689). [https://doi.org/10.1126/science.ado7069](https://doi.org/10.1126/science.ado7069)
- Mittelstadt, B. (2019). Principles alone cannot guarantee ethical AI. *Nature Machine Intelligence*, 1(11), 501–507. [https://doi.org/10.1038/s42256-019-0114-4](https://doi.org/10.1038/s42256-019-0114-4)
- Mohamed, S., Png, M.-T., & Isaac, W. (2020). Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence. *Philosophy & Technology*, 33(4), 659–684. [https://doi.org/10.1007/s13347-020-00405-8](https://doi.org/10.1007/s13347-020-00405-8)
- Mökander, J., & Floridi, L. (2023). Operationalising AI governance through ethics-based auditing: An industry case study. *AI and Ethics*, 3(2), 451–468. [https://doi.org/10.1007/s43681-022-00171-7](https://doi.org/10.1007/s43681-022-00171-7)
- Mökander, J., Morley, J., Taddeo, M., & Floridi, L. (2021). Ethics-based auditing of automated decision-making systems: Nature, scope, and limitations. *Science and Engineering Ethics*, 27(4), 44. [https://doi.org/10.1007/s11948-021-00319-4](https://doi.org/10.1007/s11948-021-00319-4)
- Mökander, J., Schuett, J., Kirk, H. R., & Floridi, L. (2023). Auditing large language models: A three-layered approach. *AI and Ethics*. [https://doi.org/10.1007/s43681-023-00289-2](https://doi.org/10.1007/s43681-023-00289-2)
- Morley, J., Floridi, L., Kinsey, L., & Elhalal, A. (2020). From what to how: An initial review of publicly available AI ethics tools, methods and research to translate principles into practices. *Science and Engineering Ethics*, 26(4), 2141–2168. [https://doi.org/10.1007/s11948-019-00165-5](https://doi.org/10.1007/s11948-019-00165-5)
- Morley, J., Kinsey, L., Elhalal, A., Garcia, F., Ziosi, M., & Floridi, L. (2023). Operationalising AI ethics: Barriers, enablers and next steps. *AI & Society*, 38(1), 411–423. [https://doi.org/10.1007/s00146-021-01308-8](https://doi.org/10.1007/s00146-021-01308-8)
- Morris, M. R., Sohl-dickstein, J., Fiedel, N., Warkentin, T., Dafoe, A., Faust, A., Farabet, C., & Legg, S. (2023). *Levels of AGI: Operationalizing progress on the path to AGI*. arXiv. [http://arxiv.org/abs/2311.02462](http://arxiv.org/abs/2311.02462)
- Mouton, C. A., Lucas, C., & Guest, E. (2023). The operational risks of AI in large-scale biological attacks: A red-team approach. RAND. [https://doi.org/10.7249/RRA2977-1](https://doi.org/10.7249/RRA2977-1)
- Nagy, A. L., & Cenker, W. J. (2002). An assessment of the newly defined internal audit function. *Managerial Auditing Journal*, 17(3), 130–137. [https://doi.org/10.1108/02686900210419912](https://doi.org/10.1108/02686900210419912)
- Nanda, N., Chan, L., Lieberum, T., Smith, J., & Steinhardt, J. (2023). *Progress measures for grokking via mechanistic interpretability*. arXiv. [http://arxiv.org/abs/2301.05217](http://arxiv.org/abs/2301.05217)
- Narayanan, A., & Kapoor, S. (2024, June 27). *AI scaling myths*. AI Snake Oil. [https://perma.cc/8VB8-UTAA](https://perma.cc/8VB8-UTAA)
- Nature Editorial Board. (2023). Stop talking about tomorrow's AI doomsday when AI poses risks today. *Nature*, 618(7967), 885–886. [https://doi.org/10.1038/d41586-023-02094-7](https://doi.org/10.1038/d41586-023-02094-7)
- Naudé, W., & Dimitri, N. (2020). The race for an artificial general intelligence: Implications for public policy. *AI & Society*, 35(2), 367–379. [https://doi.org/10.1007/s00146-019-00887-x](https://doi.org/10.1007/s00146-019-00887-x)
- Nevo, S., Lahav, D., Karpur, A., Alstott, J., & Matheny, J. (2024). *Securing AI model weights: Preventing theft and misuse of frontier models*. RAND. [https://doi.org/10.7249/RRA2849-1](https://doi.org/10.7249/RRA2849-1)
- Ngo, R., Chan, L., & Mindermann, S. (2022). *The alignment problem from a deep learning perspective*. arXiv. [https://arxiv.org/abs/2209.00626](https://arxiv.org/abs/2209.00626)
- Nilsson, N. J. (2009). The quest for artificial intelligence: A history of ideas and achievements. Cambridge University Press. [https://perma.cc/CQV7-N233](https://perma.cc/CQV7-N233)
- NIST. (2018). *Framework for improving critical infrastructure cybersecurity (Version 1.1)*. NIST. [https://doi.org/10.6028/NIST.CSWP.04162018](https://doi.org/10.6028/NIST.CSWP.04162018)
- NIST. (2022). *Secure software development framework (SSDF) (Version 1.1)*. NIST. [https://doi.org/10.6028/NIST.SP.800-218](https://doi.org/10.6028/NIST.SP.800-218)
- NIST. (2023). *Artificial intelligence risk management framework (AI RMF 1.0)*. NIST. [https://doi.org/10.6028/NIST.AI.100-1](https://doi.org/10.6028/NIST.AI.100-1)
- NIST. (2024). *Managing misuse risk for dual-use foundation models (AI 800-1 initial public draft)*. NIST. [https://doi.org/10.6028/NIST.AI.800-1.ipd](https://doi.org/10.6028/NIST.AI.800-1.ipd)
- Nordin, I. G. (2023). Narratives of internal audit: The Sisyphean work of becoming “independent”. *Critical Perspectives on Accounting*, 94, 102448. [https://doi.org/10.1016/j.cpa.2022.102448](https://doi.org/10.1016/j.cpa.2022.102448)
- Novelli, C., Casolari, F., Rotolo, A., Taddeo, M., & Floridi, L. (2023). Taking AI risks seriously: A new assessment model for the AI Act. *AI & Society*, 39, 2493–2497. [https://doi.org/10.1007/s00146-023-01723-z](https://doi.org/10.1007/s00146-023-01723-z)
- Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., & Carter, S. (2020). Zoom in: An introduction to circuits. *Distill*, 5(3). [https://doi.org/10.23915/distill.00024.001](https://doi.org/10.23915/distill.00024.001)
- OpenAI. (2019, March 11). *OpenAI LP*. OpenAI. [https://perma.cc/CR43-UD5H](https://perma.cc/CR43-UD5H)
- OpenAI. (2023a). *DALL·E 3 system card*. OpenAI. [https://perma.cc/PV2T-5x3A](https://perma.cc/PV2T-5x3A)
- OpenAI. (2023b). *GPT-4 technical report*. arXiv. [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)
- OpenAI. (2023c, October 26). *Frontier risk and preparedness*. OpenAI. [https://perma.cc/HJ6G-EVBP](https://perma.cc/HJ6G-EVBP)
- OpenAI. (2023d, November 17). *OpenAI announces leadership transition*. OpenAI. [https://perma.cc/PN5F-MQN3](https://perma.cc/PN5F-MQN3)
- OpenAI. (2023e, October 26). *OpenAI's approach to frontier risk*. OpenAI. [https://perma.cc/9YGS-NZVX](https://perma.cc/9YGS-NZVX)
- OpenAI. (2023f, December 18). *Preparedness framework (Beta)*. OpenAI. [https://perma.cc/Y5DU-LZNX](https://perma.cc/Y5DU-LZNX)
- OpenAI. (2023g, November 29). *Sam Altman returns as CEO, OpenAI has a new initial board*. OpenAI. [https://perma.cc/CH54-8YJE](https://perma.cc/CH54-8YJE)
- OpenAI. (2024a, January 31). *Building an early warning system for LLM-aided biological threat creation*. OpenAI. [https://perma.cc/2EN3-KALK](https://perma.cc/2EN3-KALK)
- OpenAI. (2024b, March 8). *Review completed & Altman, Brockman to continue to lead OpenAI*. OpenAI. [https://perma.cc/G4PE-9FC8](https://perma.cc/G4PE-9FC8)
- OpenAI. (2024c, August 8). *GPT-4o system card*. OpenAI. [https://perma.cc/U6C7-ALS4](https://perma.cc/U6C7-ALS4)
- OpenSSF. (2023). Safeguarding artifact integrity across any software supply chain. SLSA. [https://perma.cc/Y9UW-YYWZ](https://perma.cc/Y9UW-YYWZ)
- Our World in Data. (2023). *Computation used to train notable artificial intelligence systems*. Our World in Data. [https://perma.cc/Z7x4-86XC](https://perma.cc/Z7x4-86XC)
- Oussii, A. A., & Boulila Taktak, N. (2018). The impact of internal audit function characteristics on internal control quality. *Managerial Auditing Journal*, 33(5), 450–469. [https://doi.org/10.1108/MAJ-06-2017-1579](https://doi.org/10.1108/MAJ-06-2017-1579)
- O'Brien, J., Ee, S., & Williams, Z. (2023). *Deployment corrections: An incident response framework for frontier AI models*. arXiv. [https://arxiv.org/abs/2310.00328](https://arxiv.org/abs/2310.00328)
- Pacchiardi, L., Chan, A. J., Mindermann, S., Moscovitz, I., Pan, A. Y., Gal, Y., Evans, O., & Brauner, J. (2023). *How to catch an AI liar: Lie detection in black-box LLMs by asking unrelated questions*. arXiv. [http://arxiv.org/abs/2309.15840](http://arxiv.org/abs/2309.15840)
- Park, P. S., Goldstein, S., O'Gara, A., Chen, M., & Hendrycks, D. (2023). *AI deception: A survey of examples, risks, and potential solutions*. arXiv. [https://arxiv.org/abs/2308.14752](https://arxiv.org/abs/2308.14752)
- Partnership on AI. (2023). *PAI's guidance for safe foundation model deployment: A framework for collective action*. Partnership on AI. [https://perma.cc/W9GN-6QY3](https://perma.cc/W9GN-6QY3)
- Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N., & Irving, G. (2022). *Red teaming language models with language models*. arXiv. [https://arxiv.org/abs/2202.03286](https://arxiv.org/abs/2202.03286)
- Perrigo, B. (2023, February 17). *Bing's AI is threatening users. That's no laughing matter*. Time. [https://perma.cc/74ZT-ECG4](https://perma.cc/74ZT-ECG4)
- Petropoulos, F., Apiletti, D., Assimakopoulos, V., Babai, M. Z., Barrow, D. K., Ben Taieb, S., Bergmeir, C., Bessa, R. J., Bijak, J., Boylan, J. E., Browell, J., Carnevale, C., Castle, J. L., Cirillo, P., Clements, M. P., Cordeiro, C., Oliveira, F. L. C., De Baets, S., Dokumentov, A., … Ziel, F. (2022). Forecasting: Theory and practice. *International Journal of Forecasting*, 38(3), 705–871. [https://doi.org/10.1016/j.ijforecast.2021.11.001](https://doi.org/10.1016/j.ijforecast.2021.11.001)
- Phuong, M., Aitchison, M., Catt, E., Cogan, S., Kaskasoli, A., Krakovna, V., Lindner, D., Rahtz, M., Assael, Y., Hodkinson, S., Howard, H., Lieberum, T., Kumar, R., Raad, M. A., Webson, A., Ho, L., Lin, S., Farquhar, S., Hutter, M., … Shevlane, T. (2024). *Evaluating frontier models for dangerous capabilities*. arXiv. [http://arxiv.org/abs/2403.13793](http://arxiv.org/abs/2403.13793)
- Power, M. (1984). The audit explosion. Demos. [https://perma.cc/3HHY-4XWH](https://perma.cc/3HHY-4XWH)
- Prinsloo, A., & Maroun, W. (2021). An exploratory study on the components and quality of combined assurance in an integrated or a sustainability reporting setting. *Sustainability Accounting Management and Policy Journal*, 12(1), 1–29. [https://doi.org/10.1108/SAMPJ-05-2019-0205](https://doi.org/10.1108/SAMPJ-05-2019-0205)
- PwC. (2015). *Covering your bases: Implementing appropriate levels of combined assurance*. PwC. [https://perma.cc/H2BS-U7ZD](https://perma.cc/H2BS-U7ZD)
- Rae, A., Alexander, R., & McDermid, J. (2014). Fixing the cracks in the crystal ball: A maturity model for quantitative risk assessment. *Reliability Engineering & System Safety*, 125, 67–81. [https://doi.org/10.1016/j.ress.2013.09.008](https://doi.org/10.1016/j.ress.2013.09.008)
- Raji, I. D., & Buolamwini, J. (2019). Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial AI products. In *AAAI/ACM Conference on AI, Ethics, and Society* (pp. 429–435). ACM. [https://doi.org/10.1145/3306618.3314244](https://doi.org/10.1145/3306618.3314244)
- Raji, I. D., Smart, A., White, R. N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020). Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In *Conference on Fairness, Accountability, and Transparency* (pp. 33–44). ACM. [https://doi.org/10.1145/3351095.3372873](https://doi.org/10.1145/3351095.3372873)
- Raji, I. D., Xu, P., Honigsberg, C., & Ho, D. (2022). Outsider oversight: Designing a third party audit ecosystem for AI governance. In *AAAI/ACM Conference on AI, Ethics, and Society* (pp. 557–571). ACM. [https://doi.org/10.1145/3514094.3534181](https://doi.org/10.1145/3514094.3534181)
- Rando, J., Paleka, D., Lindner, D., Heim, L., & Tramèr, F. (2022). *Red-teaming the stable diffusion safety filter*. arXiv. [https://arxiv.org/abs/2210.04610](https://arxiv.org/abs/2210.04610)
- Rogers, A., & Luccioni, A. S. (2023). *Position: Key claims in LLM research have a long tail of footnotes*. arXiv. [https://arxiv.org/abs/2308.07120](https://arxiv.org/abs/2308.07120)
- Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). *High-resolution image synthesis with latent diffusion models*. arXiv. [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)
- Roose, K. (2023, February 16). A conversation with Bing's chatbot left me deeply unsettled. *The New York Times*. [https://perma.cc/2BJP-9QZ8](https://perma.cc/2BJP-9QZ8)
- Roussy, M. (2013). Internal auditors’ roles: From watchdogs to helpers and protectors of the top manager. *Critical Perspectives on Accounting*, 24(7), 550–571. [https://doi.org/10.1016/j.cpa.2013.08.004](https://doi.org/10.1016/j.cpa.2013.08.004)
- Roussy, M., & Brivot, M. (2016). Internal audit quality: A polysemous notion? *Accounting Auditing & Accountability*, 29(5), 714–738. [https://doi.org/10.1108/AAAJ-10-2014-1843](https://doi.org/10.1108/AAAJ-10-2014-1843)
- Roussy, M., & Perron, A. (2018). New perspectives in internal audit research: A structured literature review. *Accounting Perspectives*, 17(3), 345–385. [https://doi.org/10.1111/1911-3838.12180](https://doi.org/10.1111/1911-3838.12180)
- Roussy, M., & Rodrigue, M. (2018). Internal audit: Is the ‘third line of defense’ effective as a form of governance? An exploratory study of the impression management techniques chief audit executives use in their annual accountability to the audit committee. *Journal of Business Ethics*, 151(3), 853–869. [https://doi.org/10.1007/s10551-016-3263-y](https://doi.org/10.1007/s10551-016-3263-y)
- Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. *Nature Machine Intelligence*, 1(5), 206–215. [https://doi.org/10.1038/s42256-019-0048-x](https://doi.org/10.1038/s42256-019-0048-x)
- Russell, S., & Norvig, P. (2021). Artificial intelligence: A modern approach ( 4th ed.). Pearson.
- Røyksund, M., & Flage, R. (2019). When is a risk assessment deficient according to an uncertainty-based risk perspective? *Risk Analysis*, 39(4), 761–776. [https://doi.org/10.1111/risa.13195](https://doi.org/10.1111/risa.13195)
- Sætra, H. S., & Danaher, J. (2023). Resolving the battle of short- vs. long-term AI risks. *AI and Ethics*. [https://doi.org/10.1007/s43681-023-00336-y](https://doi.org/10.1007/s43681-023-00336-y)
- Sandbrink, J. B. (2023). *Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools*. arXiv. [https://arxiv.org/abs/2306.13952](https://arxiv.org/abs/2306.13952)
- Sarens, G., De Beelde, I., & Everaert, P. (2009). Internal audit: A comfort provider to the audit committee. *The British Accounting Review*, 41(2), 90–106. [https://doi.org/10.1016/j.bar.2009.02.002](https://doi.org/10.1016/j.bar.2009.02.002)
- Sarens, G., Decaux, L., & Lenz, R. (2012). Combined assurance: Case studies on a holistic approach to organizational governance. IIA Research Foundation.
- Sastry, G., Heim, L., Belfield, H., Anderljung, M., Brundage, M., Hazell, J., O'Keefe, C., Hadfield, G. K., Ngo, R., Pilz, K., Gor, G., Bluemke, E., Shoker, S., Egan, J., Trager, R. F., Avin, S., Weller, A., Bengio, Y., & Coyle, D. (2024). *Computing power and the governance of artificial intelligence*. arXiv. [https://arxiv.org/abs/2402.08797](https://arxiv.org/abs/2402.08797)
- Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., … Wolf, T. (2022). *Bloom: A 176B-parameter open-access multilingual language model*. arXiv. [http://arxiv.org/abs/2211.05100](http://arxiv.org/abs/2211.05100)
- Schaeffer, R., Miranda, B., & Koyejo, S. (2023). Are emergent abilities of large language models a mirage? In *Conference on Neural Information Processing Systems*. [https://arxiv.org/abs/2304.15004](https://arxiv.org/abs/2304.15004)
- Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., & Scialom, T. (2023). *Toolformer: Language models can teach themselves to use tools*. arXiv. [http://arxiv.org/abs/2302.04761](http://arxiv.org/abs/2302.04761)
- Schuett, J. (2023). Defining the scope of AI regulations. *Law, Innovation and Technology*, 15(1), 60–82. [https://doi.org/10.1080/17579961.2023.2184135](https://doi.org/10.1080/17579961.2023.2184135)
- Schuett, J. (2024). Risk management in the Artificial Intelligence Act. *European Journal of Risk Regulation*, 15, 367–385. [https://doi.org/10.1017/err.2023.1](https://doi.org/10.1017/err.2023.1)
- Schuett, J. (2023c). Three lines of defense against risks from AI. *AI & Society*. [https://doi.org/10.1007/s00146-023-01811-0](https://doi.org/10.1007/s00146-023-01811-0)
- Schuett, J., Dreksler, N., Anderljung, M., McCaffary, D., Heim, L., Bluemke, E., & Garfinkel, B. (2023). *Towards best practices in AGI safety and governance: A survey of expert opinion*. arXiv. [https://arxiv.org/abs/2305.07153](https://arxiv.org/abs/2305.07153)
- Schuett, J., Reuel, A., & Carlier, A. (2024). How to design an AI ethics board. *AI and Ethics*. [https://doi.org/10.1007/s43681-023-00409-y](https://doi.org/10.1007/s43681-023-00409-y)
- Schuett, J., Anderljung, M., Koessler, L., Carlier, A., & Garfinkel, B. (2024). From principles to rules: A regulatory approach for frontier AI. In P. Hacker, A. Engel, S. Hammer, & B. Mittelstadt (Eds.), The Oxford handbook on the foundations and regulation of generative AI. Oxford University Press. [https://arxiv.org/abs/2407.07300](https://arxiv.org/abs/2407.07300)
- Seger, E., Ovadya, A., Siddarth, D., Garfinkel, B., & Dafoe, A. (2023). Democratising AI: Multiple meanings, goals, and methods. In *AAAI/ACM Conference on AI, Ethics, and Society* (pp. 715–722). ACM. [https://doi.org/10.1145/3600211.3604693](https://doi.org/10.1145/3600211.3604693)
- Seger, E., Dreksler, N., Moulange, R., Dardaman, E., Schuett, J., Wei, K., Winter, C., Arnold, M., Ó hÉigeartaigh, S., Korinek, A., Anderljung, M., Bucknall, B., Chan, A., Stafford, E., Koessler, L., Ovadya, A., Garfinkel, B., Bluemke, E., Aird, M., … Gupta, A. (2023). *Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives*. arXiv. [https://arxiv.org/abs/2311.09227](https://arxiv.org/abs/2311.09227)
- Senft, S., & Gallegos, F. (2008). Information technology control and audit, third edition ( 3rd ed.). Auerbach. [https://doi.org/10.1201/9781420065541](https://doi.org/10.1201/9781420065541)
- Sevilla, J., Besiroglu, T., Cottier, B., You, J., Roldán, E., Villalobos, P., & Erdil, E. (2024, August 20). *Can AI scaling continue through 2030?* Epoch. [https://perma.cc/PC4B-YDME](https://perma.cc/PC4B-YDME)
- Shavit, Y., Agarwal, S., Brundage, M., Adler, S., O'Keefe, C., Campbell, R., Lee, T., Mishkin, P., Eloundou, T., Hickey, A., Slama, K., Ahmad, L., McMillan, P., Beutel, A., Passos, A., & Robinson, D. G. (2023). Practices for governing agentic AI systems. OpenAI. [https://perma.cc/62PZ-7K4F](https://perma.cc/62PZ-7K4F)
- Shevlane, T. (2022). Structured access: An emerging paradigm for safe AI deployment. In J. B. Bullock, Y.-C. Chen, J. Himmelreich, V. M. Hudson, A. Korinek, M. M. Young, & B. Zhang (Eds.), The Oxford handbook of AI governance. Oxford University Press. [https://doi.org/10.1093/oxfordhb/9780197579329.013.39](https://doi.org/10.1093/oxfordhb/9780197579329.013.39)
- Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., Kokotajlo, D., Marchal, N., Anderljung, M., Kolt, N., Ho, L., Siddarth, D., Avin, S., Hawkins, W., Kim, B., Gabriel, I., Bolina, V., Clark, J., Bengio, Y., … Dafoe, A. (2023). *Model evaluation for extreme risks*. arXiv. [https://arxiv.org/abs/2305.15324](https://arxiv.org/abs/2305.15324)
- Shrestha, Y. R., von Krogh, G., & Feuerriegel, S. (2023). Building open-source AI. *Nature Computational Science*, 3(11), 908–911. [https://doi.org/10.1038/s43588-023-00540-0](https://doi.org/10.1038/s43588-023-00540-0)
- Singer, P., & Tse, Y. F. (2023). AI ethics: The case for including animals. *AI and Ethics*, 3(2), 539–551. [https://doi.org/10.1007/s43681-022-00187-z](https://doi.org/10.1007/s43681-022-00187-z)
- Slattery, P., Saeri, A. K., Grundy, E. A. C., Graham, J., Noetel, M., Uuk, R., Dao, J., Pour, S., & Thompson, N. (2024). *The AI risk repository: A comprehensive meta-review, database, and taxonomy of risks from artificial intelligence*. arXiv. [http://arxiv.org/abs/2408.12622](http://arxiv.org/abs/2408.12622)
- Soh, D. S. B., & Martinov-Bennie, N. (2011). The internal audit function: Perceptions of internal audit roles, effectiveness and evaluation. *Managerial Auditing Journal*, 26(7), 605–622. [https://doi.org/10.1108/02686901111151332](https://doi.org/10.1108/02686901111151332)
- Soice, E. H., Rocha, R., Cordova, K., Specter, M., & Esvelt, K. M. (2023). *Can large language models democratize access to dual-use biotechnology?* arXiv. [https://arxiv.org/abs/2306.03809](https://arxiv.org/abs/2306.03809)
- Solaiman, I. (2023). *The gradient of generative AI release: Methods and considerations*. arXiv. [https://arxiv.org/abs/2302.04844](https://arxiv.org/abs/2302.04844)
- Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W., Kreps, S., McCain, M., Newhouse, A., Blazakis, J., McGuffie, K., & Wang, J. (2019). *Release strategies and the social impacts of language models*. arXiv. [http://arxiv.org/abs/1908.09203](http://arxiv.org/abs/1908.09203)
- Soler Garrido, J., Fano Yela, D., Panigutti, C., Junklewitz, H., Hamon, R., Evas, T., André, A.-A., & Scalzo, S. (2023). Analysis of the preliminary AI standardisation work plan in support of the AI Act. European Commission, Joint Research Centre. [https://doi.org/10.2760/5847](https://doi.org/10.2760/5847)
- Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. W., Safaya, A., Tazarv, A., … Wu, Z. (2022). *Beyond the imitation game: Quantifying and extrapolating the capabilities of language models*. arXiv. [http://arxiv.org/abs/2206.04615](http://arxiv.org/abs/2206.04615)
- Stafford, T., Deitz, G., & Li, Y. (2018). The role of internal audit and user training in information security policy compliance. *Managerial Auditing Journal*, 33(4), 410–424. [https://doi.org/10.1108/MAJ-07-2017-1596](https://doi.org/10.1108/MAJ-07-2017-1596)
- Steinbart, P. J., Raschke, R. L., Gal, G., & Dilla, W. N. (2012). The relationship between internal audit and information security: An exploratory investigation. *International Journal of Accounting Information Systems*, 13(3), 228–243. [https://doi.org/10.1016/j.accinf.2012.06.007](https://doi.org/10.1016/j.accinf.2012.06.007)
- Stewart, J., & Subramaniam, N. (2010). Internal audit independence and objectivity: Emerging research opportunities. *Managerial Auditing Journal*, 25(4), 328–360. [https://doi.org/10.1108/02686901011034162](https://doi.org/10.1108/02686901011034162)
- Stoel, D., Havelka, D., & Merhout, J. W. (2012). An analysis of attributes that impact information technology audit quality: A study of IT and financial audit practitioners. *International Journal of Accounting Information Systems*, 13(1), 60–79. [https://doi.org/10.1016/j.accinf.2011.11.001](https://doi.org/10.1016/j.accinf.2011.11.001)
- Taleb, N. N. (2007). The black swan: The impact of the highly improbable. Random House.
- Tetlock, P. E. (2005). Expert political judgment: How good is it? How can we know? Princeton University Press.
- Tetlock, P. E., & Gardner, D. (2015). Superforecasting: The art and science of prediction. Penguin Random House.
- Thekdi, S. A., & Aven, T. (2022). Risk analysis under attack: How risk science can address the legal, social, and reputational liabilities faced by risk analysts. *Risk Analysis*, 43(6), 1212–1221. [https://doi.org/10.1111/risa.13984](https://doi.org/10.1111/risa.13984)
- Trager, R., Harack, B., Reuel, A., Carnegie, A., Heim, L., Ho, L., Kreps, S., Lall, R., Larter, O., Ó hÉigeartaigh, S., Staffell, S., & Villalobos, J. J. (2023). *International governance of civilian AI: A jurisdictional certification approach*. arXiv. [https://arxiv.org/abs/2308.15514](https://arxiv.org/abs/2308.15514)
- Turner, A. M., Smith, L., Shah, R., Critch, A., & Tadepalli, P. (2023). *Optimal policies tend to seek power*. arXiv. [https://arxiv.org/abs/1912.01683](https://arxiv.org/abs/1912.01683)
- Turner, A. M., & Tadepalli, P. (2022). *Parametrically retargetable decision-makers tend to seek power*. arXiv. [https://arxiv.org/abs/2206.13477](https://arxiv.org/abs/2206.13477)
- Urbina, F., Lentzos, F., Invernizzi, C., & Ekins, S. (2022). Dual use of artificial-intelligence-powered drug discovery. *Nature Machine Intelligence*, 4(3), 189–191. [https://doi.org/10.1038/s42256-022-00465-9](https://doi.org/10.1038/s42256-022-00465-9)
- Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., & Ho, A. (2024). *Will we run out of data? An analysis of the limits of scaling datasets in machine learning*. arXiv. [https://arxiv.org/abs/2211.04325](https://arxiv.org/abs/2211.04325)
- Vipra, J., & Korinek, A. (2023). Market concentration implications of foundation models. arXiv. [https://arxiv.org/abs/2311.01550](https://arxiv.org/abs/2311.01550)
- Wang, P. (2019). On defining artificial intelligence. *Journal of Artificial General Intelligence*, 10(2), 1–37. [https://doi.org/10.2478/jagi-2019-0002](https://doi.org/10.2478/jagi-2019-0002)
- Wassie, F. A., & Lakatos, L. P. (2024). Artificial intelligence and the future of the internal audit function. *Humanities and Social Sciences Communications*, 11(386), 1–13. [https://doi.org/10.1057/s41599-024-02905-w](https://doi.org/10.1057/s41599-024-02905-w)
- Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., & Fedus, W. (2022). *Emergent abilities of large language models*. arXiv. [https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)
- Weidenmier, M. L., & Ramamoorti, S. (2006). Research opportunities in information technology and internal auditing. *Journal of Information Systems*, 20(1), 205–219. [https://doi.org/10.2308/jis.2006.20.1.205](https://doi.org/10.2308/jis.2006.20.1.205)
- Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., … Gabriel, I. (2021). *Ethical and social risks of harm from language models*. arXiv. [https://arxiv.org/abs/2112.04359](https://arxiv.org/abs/2112.04359)
- Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, J., Bergman, S., Kay, J., Griffin, C., Bariach, B., Gabriel, I., Rieser, V., & Isaac, W. (2023). *Sociotechnical safety evaluation of generative AI systems*. arXiv. [https://arxiv.org/abs/2310.11986](https://arxiv.org/abs/2310.11986)
- Weidinger, L., Barnhart, J., Brennan, J., Butterfield, C., Young, S., Hawkins, W., Hendricks, L. A., Comanescu, R., Chang, O., Rodriguez, M., Beroshi, J., Bloxwich, D., Proleev, L., Chen, J., Farquhar, S., Ho, L., Gabriel, I., Dafoe, A., & Isaac, W. (2024). *Holistic safety and responsibility evaluations of advanced AI models*. arXiv. [http://arxiv.org/abs/2404.14068](http://arxiv.org/abs/2404.14068)
- van der Weij, T., Hofstätter, F., Jaffe, O., Brown, S. F., & Ward, F. R. (2024). *AI sandbagging: Language models can strategically underperform on evaluations*. arXiv. [https://arxiv.org/abs/2406.07358](https://arxiv.org/abs/2406.07358)
- The White House. (2023). *Safe, secure, and trustworthy development and use of artificial intelligence* (Executive Order 14110). The White House. [https://perma.cc/5HCL-LDMT](https://perma.cc/5HCL-LDMT)
- Whittaker, M. (2021). The steep cost of capture. *Interactions*, 28(6), 50–55. [https://doi.org/10.1145/3488666](https://doi.org/10.1145/3488666)
- van Wynsberghe, A. (2021). Sustainable AI: AI for sustainability and the sustainability of AI. *AI and Ethics*, 1(3), 213–218. [https://doi.org/10.1007/s43681-021-00043-6](https://doi.org/10.1007/s43681-021-00043-6)
- Yong, Z.-X., Menghini, C., & Bach, S. H. (2023). *Low-resource languages jailbreak GPT-4*. arXiv. [http://arxiv.org/abs/2310.02446](http://arxiv.org/abs/2310.02446)
- Zald, M. N. (1969). The power and functions of boards of directors: A theoretical synthesis. *American Journal of Sociology*, 75(1), 97–111. [https://doi.org/10.1086/224747](https://doi.org/10.1086/224747)
- Zhang, A. K., Perry, N., Dulepet, R., Ji, J., Lin, J. W., Jones, E., Menders, C., Hussein, G., Liu, S., Jasper, D., Peetathawatchai, P., Glenn, A., Sivashankar, V., Zamoshchin, D., Glikbarg, L., Askaryar, D., Yang, M., Zhang, T., Alluri, R., … Liang, P. (2024). *Cybench: A framework for evaluating cybersecurity capabilities and risk of language models*. arXiv. [https://arxiv.org/abs/2408.08926](https://arxiv.org/abs/2408.08926)
- Zhan, Q., Fang, R., Bindu, R., Gupta, A., Hashimoto, T., & Kang, D. (2023). *Removing RLHF protections in GPT-4 via fine-tuning*. arXiv. [https://arxiv.org/abs/2311.05553](https://arxiv.org/abs/2311.05553)
- Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). *A survey of large language models*. arXiv. [https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223)
- Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., & Irving, G. (2019). *Fine-tuning language models from human preferences*. arXiv. [https://arxiv.org/abs/1909.08593](https://arxiv.org/abs/1909.08593)

- 1 More precisely, we can empirically observe that increasing the size of the training dataset (*D*), the number of parameters (*P*), and the amount of training compute (*C*) leads to a predictable reduction in the test loss (*L*), that is, the difference between the model's predicted output and the actual target value on an unseen dataset. The relationship between the properties *D*, *P*, *C*, and *L* follows a precise power law (Bahri et al., [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0026); Kaplan et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0164)).
- 2 But note that some people claim that emergent capabilities may not be a fundamental property of scaling AI models (Schaeffer et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0278)). Others have pointed out that the concept is poorly defined (Rogers & Luccioni, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0262)).
- 3 But note that some people think that the risks are overblown (Nature Editorial Board, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0220); Sætra & Danaher, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0272)) or at least that the current level of risk is low (Baum, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0031)).
- 4 For more information on risk governance, see van Asselt and Renn ([2011](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0021)), Lundqvist ([2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0190)), and Klinke and Renn ([2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0170)).
- 5 Note that METR does not see itself as an auditor, but as a research organization that seeks to advance the science of model evaluations. But as METR has been commissioned by OpenAI and Anthropic to evaluate GPT-4 and Claude models (METR, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0202)), I consider them an auditor for the purposes of this article.
- 6 Birhane et al. ([2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0039)) distinguished between whether or not there is a contractual relationship between the auditor and the company. According to this interpretation, METR would be considered an internal auditor.
- 7 But note that this changes over time. As famously put by John McCarthy: “as soon as it works, no one calls it AI any more” (Meyer, [2011](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0203)). Also note that this approach does not work in a regulatory context where a more precise definition is necessary (Schuett, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0280)).
- 8 The definition could be operationalized by defining a training compute threshold, that is, a certain amount of computational resources required to train a model (Heim & Koessler, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0132); Hooker, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0139); Koessler et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0173)). A threshold suggested in the literature (Anderljung, Barnhart et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0006); Sastry et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0276)) and used in the recent Executive Order on Safe, Secure, and Trustworthy AI (The White House, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0324)) is 10<sup>26</sup> floating point operations (FLOP). This would be more than any existing model (Our World in Data, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0244)).
- 9 Shevlane et al. ([2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0292)) defined “frontier AI models” as “models that are both (a) close to, or exceeding, the average capabilities of the most capable existing models, and (b) different from other models, either in terms of scale, design (e.g., different architectures or alignment techniques), or their resulting mix of capabilities and behaviors.”
- 10 Anderljung et al. (2023) defined “frontier AI models” as “highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety.”
- 11 Phuong et al. ([2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0253)) defined “frontier AI models” as “the leading edge of general-purpose AI models.”
- 12 Note that I excluded literature that follows the second interpretation of internal audit (Section 2.1), such as Raji et al. ([2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0259)) and Birhane et al. ([2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0039)).
- 13 For more information on the Combined Assurance Framework, see PwC ([2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0256)), Huibers ([2015](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0144)), Forte and Barac ([2016](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0108)), Prinsloo and Maroun ([2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0255)), IIA ([2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0151)).
- 14 Although this article focuses on risks, internal audit could also help to address other ethical challenges. For example, many AI companies have ethics principles (Hagendorff, [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0129); Jobin et al., [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0160)), but “principles alone cannot guarantee ethical AI” (Mittelstadt, [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0208)). Internal audit could evaluate the extent to which principles are put into practice (Morley et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0213), [2023](https://doi.org/10.1007/s00146-021-01308-8)). It could also advise the board on ethical matters. For example, it could ensure that developers pay enough attention to their impact on historically marginalized communities (Birhane, Isaac et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0037); Birhane, Ruane et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0038); Mohamed et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0209)), the environment (van Wynsberghe, [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0327)), and animals (Singer & Tse, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0294)).
- 15 For more information on the first three governance challenges, see Anderljung, Barnhart et al. ([2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0006)). Note that the five challenges are neither mutually exclusive nor collectively exhaustive. They were selected because they seem particularly relevant for the purposes of this article.
- 16 For a more skeptical perspective, see Schaeffer et al. ([2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0278)).
- 17 It is worth noting that techniques like reinforcement learning from human feedback (RLHF) (Christiano et al., [2017](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0067); Lampert et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0179); Ziegler et al., [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0332)) and reinforcement learning from AI feedback (RLAIF), more commonly known as “constitutional AI” (Bai et al., [2022](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0027)), represent major advancements.
- 18 For an overview of different deployment modes, see Solaiman ([2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0298)).
- 19 Note that the following is based on public information. It is possible that developers have not made certain structures public or used different terms.
- 20 Note that this refers to evaluations that are conducted at various checkpoints during the training process. Before deploying a new model, a separate team might also conduct “assurance evaluations,” using prompts unavailable to the development team (Weidinger et al., [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0322)). Some developers also commission external model evaluations. For example, METR evaluated both GPT-4 and Claude models (METR, [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0202)).
- 21 This is a result of the black box nature of current AI models (Castelvecchi, [2016](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0059); Rudin, [2019](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0269); von Eschenbach, [2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0100)). But note that there is an emerging research field called “mechanistic interpretability” that tries to understand how models work by studying their weights and activations (Nanda et al., [2023](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0218); Olah et al., [2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0231)).
- 22 For more information on verifying claims about responsible AI development, see Brundage et al. ([2020](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0047)) and Avin et al. ([2021](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0025)).
- 23 But note that Anthropic has recently set up a new team which has the explicit mandate to empirically demonstrate ways in which its alignment strategies could fail (Hubinger, [2024](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0142)).
- 24 For a critical perspective on mandating risk governance best practices, see Enriques and Zetzsche ([2013](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0098)).
- 25 In other industries, the requirement to have an internal audit function is often described as an abstract principle, not a prescriptive rule. This principle is then specified at the sub-regulatory level. For more information on how prescriptive frontier AI regulations should be, see Schuett et al. (forthcoming).
- 26 In the context of external audit, there is a concern that it is more important to an organization's legitimacy that it is *seen* to be audited than that there is any real substance to the audit (Power, [1984](https://onlinelibrary.wiley.com/doi/10.1111/#risa17665-bib-0254)). By analogy, the same concern might apply to internal audit.

## 

Citing Literature

[![](https://onlinelibrary.wiley.com/cms/asset/acf1b302-4d6e-4ec9-880d-54e95ae42a61/risa.v45.1.cover.jpg)](https://onlinelibrary.wiley.com/toc/15396924/0/0 "View Early View articles")

- [

## Figures

](https://onlinelibrary.wiley.com/doi/10.1111/#pane-pcw-figures)
- [

## References

](https://onlinelibrary.wiley.com/doi/10.1111/#pane-pcw-references)
- [

## Related

](https://onlinelibrary.wiley.com/doi/10.1111/#pane-pcw-related)
- [

## Information

](https://onlinelibrary.wiley.com/doi/10.1111/#pane-pcw-details)

- Abbott, L. J., Daugherty, B., Parker, S., & Peters, G. F. (2016). Internal audit quality and financial reporting quality: The joint importance of independence and competence. *Journal of Accounting Research*, 54(1), 3–40. [https://doi.org/10.1111/1475-679X.12099](https://doi.org/10.1111/1475-679X.12099)
- Alaga, J., & Schuett, J. (2023). *Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers*. arXiv. [https://arxiv.org/abs/2310.00374](https://arxiv.org/abs/2310.00374)
- Altman, S. (2023). *Planning for AGI and beyond*. OpenAI. [https://perma.cc/3A67-Z38F](https://perma.cc/3A67-Z38F)
- Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). *Concrete problems in AI safety*. arXiv. [https://arxiv.org/abs/1606.06565](https://arxiv.org/abs/1606.06565)
- Anderljung, M., & Hazell, J. (2023). *Protecting society from AI misuse: When are restrictions on capabilities warranted?* arXiv. [https://arxiv.org/abs/2303.09377](https://arxiv.org/abs/2303.09377)
- Anderljung, M., Barnhart, J., Korinek, A., Leung, J., O'Keefe, C., Whittlestone, J., Avin, S., Brundage, M., Bullock, J., Cass-Beggs, D., Chang, B., Collins, T., Fist, T., Hadfield, G., Hayes, A., Ho, L., Hooker, S., Horvitz, E., Kolt, N., … Wolf, K. (2023). *Frontier AI regulation: Managing emerging risks to public safety*. arXiv. [https://arxiv.org/abs/2307.03718](https://arxiv.org/abs/2307.03718)
- Anderljung, M., Smith, E. T., O'Brien, J., Soder, L., Bucknall, B., Bluemke, E., Schuett, J., Trager, R., Strahm, L., & Chowdhury, R. (2023). *Towards publicly accountable frontier LLMs: Building an external scrutiny ecosystem under the ASPIRE framework*. arXiv. [https://arxiv.org/abs/2311.14711](https://arxiv.org/abs/2311.14711)
- Anil, C., Durmus, E., Sharma, M., Benton, J., Kundu, S., Batson, J., Rimsky, N., Tong, M., Mu, J., Ford, D., Mosconi, F., Agrawal, R., Schaeffer, R., Bashkansky, N., Svenningsen, S., Lambert, M., Radhakrishnan, A., Denison, C., Hubinger, E. J., … Duvenaud, D. (2024). *Many-shot jailbreaking*. Anthropic. [https://perma.cc/NP7Q-KSKX](https://perma.cc/NP7Q-KSKX)
- Anthropic. (2023a, September 19). *Anthropic's Responsible Scaling Policy (Version 1.0)*. Anthropic. [https://perma.cc/R4MG-6W4H](https://perma.cc/R4MG-6W4H)
- Anthropic. (2023b, July 25). *Frontier model security*. Anthropic. [https://perma.cc/6HQ4-XV73](https://perma.cc/6HQ4-XV73)
- Anthropic. (2023c, September 19). *The long-term benefit trust*. Anthropic. [https://perma.cc/RPZ3-QT52](https://perma.cc/RPZ3-QT52)
- Anthropic. (2023d, July 26). *Frontier threats red teaming for AI safety*. Anthropic. [https://perma.cc/9QST-7SLE](https://perma.cc/9QST-7SLE)
- Anthropic. (2023e, October 4). *Challenges in evaluating AI systems*. Anthropic. [https://perma.cc/L5NB-8Q4W](https://perma.cc/L5NB-8Q4W)
- Anthropic. (2024a, April 9). *Measuring the persuasiveness of language models*. Anthropic. [https://perma.cc/YVQ8-YBSY](https://perma.cc/YVQ8-YBSY)
- Anthropic. (2024b, March 4). *The Claude 3 model family: Opus, sonnet, haiku*. Anthropic. [https://perma.cc/XBD5-3GV7](https://perma.cc/XBD5-3GV7)
- Anthropic. (2024c, June 21). *Claude 3.5 Sonnet model card addendum*. Anthropic. [https://perma.cc/73AP-Z9PT](https://perma.cc/73AP-Z9PT)
- Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper, S., Sourbut, O., Edelman, B. L., Zhang, Z., Günther, M., Korinek, A., Hernandez-Orallo, J., Hammond, L., Bigelow, E., Pan, A., Langosco, L., … Krueger, D. (2024). *Foundational challenges in assuring alignment and safety of large language models*. arXiv. [https://arxiv.org/abs/2404.09932](https://arxiv.org/abs/2404.09932)
- Arena, M., & Azzone, G. (2009). Identifying organizational drivers of internal audit effectiveness. *International Journal of Auditing*, 13(1), 43–60. [https://doi.org/10.1111/j.1099-1123.2008.00392.x](https://doi.org/10.1111/j.1099-1123.2008.00392.x)
- Armstrong, S., Bostrom, N., & Shulman, C. (2016). Racing to the precipice: A model of artificial intelligence development. *AI & Society*, 31(2), 201–206. [https://doi.org/10.1007/s00146-015-0590-y](https://doi.org/10.1007/s00146-015-0590-y)
- Arndorfer, I., & Minto, A. (2015). *The “Four Lines of Defence Model” for financial institutions*. Financial Stability Institute, Bank for International Settlements. [https://perma.cc/UP35-KEYJ](https://perma.cc/UP35-KEYJ)
- van Asselt, M. B. A., & Renn, O. (2011). Risk governance. *Journal of Risk Research*, 14(4), 431–449. [https://doi.org/10.1080/13669877.2011.553730](https://doi.org/10.1080/13669877.2011.553730)
- Aven, T. (2013). On the meaning of a black swan in a risk context. *Safety Science*, 57, 44–51. [https://doi.org/10.1016/j.ssci.2013.01.016](https://doi.org/10.1016/j.ssci.2013.01.016)
- Aven, T. (2016). Risk assessment and risk management: Review of recent advances on their foundation. *European Journal of Operational Research*, 253(1), 1–13. [https://doi.org/10.1016/j.ejor.2015.12.023](https://doi.org/10.1016/j.ejor.2015.12.023)
- Aven, T. (2018). An emerging new risk analysis science: Foundations and implications. *Risk Analysis*, 38(5), 876–888. [https://doi.org/10.1111/risa.12899](https://doi.org/10.1111/risa.12899)
- Avin, S., Belfield, H., Brundage, M., Krueger, G., Wang, J., Weller, A., Anderljung, M., Krawczuk, I., Krueger, D., Lebensold, J., Maharaj, T., & Zilberman, N. (2021). Filling gaps in trustworthy development of AI. *Science*, 374(6573), 1327–1329. [https://doi.org/10.1126/science.abi7176](https://doi.org/10.1126/science.abi7176)
- Bahri, Y., Dyer, E., Kaplan, J., Lee, J., & Sharma, U. (2021). *Explaining neural scaling laws*. arXiv. [https://arxiv.org/abs/2102.06701](https://arxiv.org/abs/2102.06701)
- Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., … Kaplan, J. (2022). *Constitutional AI: Harmlessness from AI feedback*. arXiv. [https://arxiv.org/abs/2212.08073](https://arxiv.org/abs/2212.08073)
- Bantleon, U., d'Arcy, A., Eulerich, M., Hucke, A., Pedell, B., & Ratzinger-Sakel, N. V. S. (2021). Coordination challenges in implementing the three lines of defense model. *International Journal of Auditing*, 25(1), 59–74. [https://doi.org/10.1111/ijau.12201](https://doi.org/10.1111/ijau.12201)
- Barrett, A. M., Newman, J., Nonnecke, B., Hendrycks, D., Murphy, & Jackson, K. (2023). AI risk-management standards profile for general-purpose AI systems (GPAIS) and foundation models. Center for Long-Term Cybersecurity. [https://perma.cc/8W6P-2UUK](https://perma.cc/8W6P-2UUK)
- Bateman, J., Baer, D., Bell, S. A., Brown, G. O., Cuéllar, M.-F., Ganguli, D., Henderson, P., Kotila, B., Lessig, L., Lundblad, N. B., Napolitano, J., Raji, D., Seger, E., Sheehan, M., Skowron, A., Solaiman, I., Toner, H., & Zvyagina, P. (2024). *Beyond open vs. closed: Emerging consensus and key questions for foundation AI model governance*. Carnegie Endowment for International Peace. [https://perma.cc/5PQK-E39U](https://perma.cc/5PQK-E39U)
- Baum, S. D. (2024). Assessing the risk of takeover catastrophe from large language models. *Risk Analysis*. [https://doi.org/10.1111/risa.14353](https://doi.org/10.1111/risa.14353)
- Bengio, Y., Hinton, G., Yao, A., Song, D., Abbeel, P., Harari, Y. N., Zhang, Y.-Q., Xue, L., Shalev-Shwartz, S., Hadfield, G., Clune, J., Maharaj, T., Hutter, F., Baydin, A. G., McIlraith, S., Gao, Q., Acharya, A., Krueger, D., Dragan, A., … Mindermann, S. (2023). Managing extreme AI risks in an era of rapid progress. *Science*, 384(6698), 842–845. [https://doi.org/10.1126/science.adn0117](https://doi.org/10.1126/science.adn0117)
- Berglund, L., Stickland, A. C., Balesni, M., Kaufmann, M., Tong, M., Korbak, T., Kokotajlo, D., & Evans, O. (2023). *Taken out of context: On measuring situational awareness in LLMs*. arXiv. [http://arxiv.org/abs/2309.00667](http://arxiv.org/abs/2309.00667)
- Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., Manassra, W., Dhariwal, P., Chu, C., Jiao, Y., & Ramesh, A. (2023). *Improving image generation with better captions*. OpenAI. [https://perma.cc/5DYB-ZJQ5](https://perma.cc/5DYB-ZJQ5)
- Betti, N., & Sarens, G. (2021). Understanding the internal audit function in a digitalised business environment. *Journal of Accounting & Organizational Change*, 17(2), 197–216. [https://doi.org/10.1108/JAOC-11-2019-0114](https://doi.org/10.1108/JAOC-11-2019-0114)
- Bhatnagar, S., Alexandrova, A., Avin, S., Cave, S., Cheke, L., Crosby, M., Feyereisl, J., Halina, M., Loe, B. S., Ó hÉigeartaigh, S., Martínez-Plumed, F., Price, H., Shevlin, H., Weller, A., Winfield, A., & Hernández-Orallo, J. (2018). Mapping intelligence: Requirements and possibilities. In V. C. Müller (Ed.), Philosophy and theory of artificial intelligence 2017 (pp. 117–135). Springer. [https://doi.org/10.1007/978-3-319-96448-5\_13](https://doi.org/10.1007/978-3-319-96448-5_13)
- Birhane, A., Isaac, W., Prabhakaran, V., Diaz, M., Elish, M. C., Gabriel, I., & Mohamed, S. (2022). Power to the people? Opportunities and challenges for participatory AI. In ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (pp. 1–8). ACM. [https://doi.org/10.1145/3551624.3555290](https://doi.org/10.1145/3551624.3555290)
- Birhane, A., Ruane, E., Laurent, T., S Brown, M., Flowers, J., Ventresque, A., & L Dancy, C. (2022). The forgotten margins of AI ethics. *ACM Conference on Fairness, Accountability, and Transparency* (pp. 948–958). ACM. [https://doi.org/10.1145/3531146.3533157](https://doi.org/10.1145/3531146.3533157)
- Birhane, A., Steed, R., Ojewale, V., Vecchione, B., & Raji, I. D. (2024). AI auditing: The broken bus on the road to AI accountability. In *IEEE Conference on Secure and Trustworthy Machine Learning* (pp. 612-643). IEEE. [https://doi.org/10.1109/SaTML59370.2024.00037](https://doi.org/10.1109/SaTML59370.2024.00037)
- Bjørkelo, B. (2013). Workplace bullying after whistleblowing: Future research and implications. *Journal of Managerial Psychology*, 28(3), 306–323. [https://doi.org/10.1108/02683941311321178](https://doi.org/10.1108/02683941311321178)
- Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., … Liang, P. (2021). *On the opportunities and risks of foundation models*. arXiv. [https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258)
- Bostrom, N. (2011). Information hazards: A typology of potential harms from knowledge. *Review of Contemporary Philosophy*, 10, 44–79.
- Bowman, S. R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., Lukošiūtė, K., Askell, A., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Olah, C., Amodei, D., Amodei, D., Drain, D., Li, D., Tran-Johnson, E., … Kaplan, J. (2022). *Measuring progress on scalable oversight for large language models*. arXiv. [http://arxiv.org/abs/2211.03540](http://arxiv.org/abs/2211.03540)
- Bromiley, P., McShane, M., Nair, A., & Rustambekov, E. (2015). Enterprise risk management: Review, critique, and research directions. *Long Range Planning*, 48(4), 265–276. [https://doi.org/10.1016/j.lrp.2014.07.005](https://doi.org/10.1016/j.lrp.2014.07.005)
- Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., & Ramesh, A. (2024, February 15). *Video generation models as world simulators*. OpenAI. [https://perma.cc/Z85Z-JZVB](https://perma.cc/Z85Z-JZVB)
- Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A., Scharre, P., Zeitzoff, T., Filar, B., Anderson, H., Roff, H., Allen, G. C., Steinhardt, J., Flynn, C., Ó hÉigeartaigh, S., Beard, S., Belfield, H., Farquhar, S., … Amodei, D. (2018). *The malicious use of artificial intelligence: Forecasting, prevention, and mitigation*. arXiv. [https://arxiv.org/abs/1802.07228](https://arxiv.org/abs/1802.07228)
- Brundage, M., Avin, S., Wang, J., Belfield, H., Krueger, G., Hadfield, G., Khlaaf, H., Yang, J., Toner, H., Fong, R., Maharaj, T., Koh, P. W., Hooker, S., Leung, J., Trask, A., Bluemke, E., Lebensold, J., O'Keefe, C., Koren, M., … Anderljung, M. (2020). *Toward trustworthy AI development: Mechanisms for supporting verifiable claims*. arXiv. [https://arxiv.org/abs/2004.07213](https://arxiv.org/abs/2004.07213)
- Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., & Zhang, Y. (2023). *Sparks of artificial general intelligence: Early experiments with GPT-4*. arXiv. [https://arxiv.org/abs/2303.12712](https://arxiv.org/abs/2303.12712)
- Büchling, M., Cerbone, D., & Maroun, W. (2023). Assurance, risk and governance: An international perspective ( 3rd ed.). Juta.
- Bucknall, B. S., & Trager, R. F. (2023). Structured access for third-party research on frontier AI models: Investigating researchers’ model access requirements. Centre for the Governance of AI. [https://perma.cc/2FAH-URD6](https://perma.cc/2FAH-URD6)
- Burden, J. (2024). *Evaluating AI evaluation: Perils and prospects*. arXiv. [http://arxiv.org/abs/2407.09221](http://arxiv.org/abs/2407.09221)
- Burnell, R., Schellaert, W., Burden, J., Ullman, T. D., Martinez-Plumed, F., Tenenbaum, J. B., Rutar, D., Cheke, L. G., Sohl-Dickstein, J., Mitchell, M., Kiela, D., Shanahan, M., Voorhees, E. M., Cohn, A. G., Leibo, J. Z., & Hernandez-Orallo, J. (2023). Rethink reporting of evaluation results in AI. *Science*, 380(6641), 136–138. [https://doi.org/10.1126/science.adf6369](https://doi.org/10.1126/science.adf6369)
- Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., Sutskever, I., & Wu, J. (2023). *Weak-to-strong generalization: Eliciting strong capabilities with weak supervision*. arXiv. [http://arxiv.org/abs/2312.09390](http://arxiv.org/abs/2312.09390)
- Cai, T., Wang, X., Ma, T., Chen, X., & Zhou, D. (2023). *Large language models as tool makers*. arXiv. [http://arxiv.org/abs/2305.17126](http://arxiv.org/abs/2305.17126)
- Carcello, J. V., Eulerich, M., Masli, A., & Wood, D. A. (2020). Are internal audits associated with reductions in perceived risk? *Auditing: A Journal of Practice & Theory*, 39(3), 55–73. [https://doi.org/10.2308/ajpt-19-036](https://doi.org/10.2308/ajpt-19-036)
- Carlsmith, J. (2023). *Scheming AIs: Will AIs fake alignment during training in order to get power?* arXiv. [https://arxiv.org/abs/2311.08379](https://arxiv.org/abs/2311.08379)
- Carranza, A., Pai, D., Schaeffer, R., Tandon, A., & Koyejo, S. (2023). *Deceptive alignment monitoring*. arXiv. [https://arxiv.org/abs/2307.10569](https://arxiv.org/abs/2307.10569)
- Cascarino, R. (2015). Internal auditing: An integrated approach ( 3rd ed.). Juta.
- Castelvecchi, D. (2016). Can we open the black box of AI? *Nature*, 538(7623), 20–23. [https://doi.org/10.1038/538020a](https://doi.org/10.1038/538020a)
- Center for AI Safety. (2023). *Statement on AI risk*. Center for AI Safety. [https://perma.cc/4ZPL-JQ4W](https://perma.cc/4ZPL-JQ4W)
- Chambers, A. D., & Odar, M. (2015). A new vision for internal audit. *Managerial Auditing Journal*, 30(1), 34–55. [https://doi.org/10.1108/MAJ-08-2014-1073](https://doi.org/10.1108/MAJ-08-2014-1073)
- Chan, A. (2024, April 9). Evaluating predictions of model behaviour. Centre for the Governance of AI. [https://perma.cc/YN9D-9HV3](https://perma.cc/YN9D-9HV3)
- Chan, A., Salganik, R., Markelius, A., Pang, C., Rajkumar, N., Krasheninnikov, D., Langosco, L., He, Z., Duan, Y., Carroll, M., Lin, M., Mayhew, A., Collins, K., Molamohammadi, M., Burden, J., Zhao, W., Rismani, S., Voudouris, K., Bhatt, U., … Maharaj, T. (2023). Harms from increasingly agentic algorithmic systems. In *ACM Conference on Fairness, Accountability, and Transparency* (pp. 651–666). ACM. [https://doi.org/10.1145/3593013.3594033](https://doi.org/10.1145/3593013.3594033)
- Chan, A., Ezell, C., Kaufmann, M., Wei, K., Hammond, L., Bradley, H., Bluemke, E., Rajkumar, N., Krueger, D., Kolt, N., Heim, L., & Anderljung, M. (2024). Visibility into AI agents. In *ACM Conference on Fairness, Accountability, and Transparency* (pp. 958–973). ACM. [https://doi.org/10.1145/3630106.3658948](https://doi.org/10.1145/3630106.3658948)
- Chang, W., Chen, E., Mellers, B., & Tetlock, P. (2016). Developing expert political judgment: The impact of training and practice on judgmental accuracy in geopolitical forecasting tournaments. *Judgment and Decision Making*, 11(5), 509–526. [https://doi.org/10.1017/S1930297500004599](https://doi.org/10.1017/S1930297500004599)
- Christ, M. H., Eulerich, M., Krane, R., & Wood, D. A. (2021). New frontiers for internal audit research. *Accounting Perspectives*, 20(4), 449–475. [https://doi.org/10.1111/1911-3838.12272](https://doi.org/10.1111/1911-3838.12272)
- Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., & Amodei, D. (2017). *Deep reinforcement learning from human preferences*. arXiv. [https://arxiv.org/abs/1706.03741](https://arxiv.org/abs/1706.03741)
- Christiano, P., Shlegeris, B., & Amodei, D. (2018). *Supervising strong learners by amplifying weak experts*. arXiv. [http://arxiv.org/abs/1810.08575](http://arxiv.org/abs/1810.08575)
- Cihon, P., Schuett, J., & Baum, S. D. (2021). Corporate governance of artificial intelligence in the public interest. *Information*, 12(7), 275. [https://doi.org/10.3390/info12070275](https://doi.org/10.3390/info12070275)
- Clark, J. (2022). *Import AI 310: AlphaZero learned Chess like humans learn Chess; capability emergence in language models; demoscene AI*. Import AI. [https://perma.cc/K4FG-ZXMX](https://perma.cc/K4FG-ZXMX)
- Clifford, B. (2023, December 17). Preventing AI misuse: Current techniques. Centre for the Governance of AI. [https://perma.cc/6XLS-2ZNQ](https://perma.cc/6XLS-2ZNQ)
- Coetzee, G. P., du Bruyn, R., Fourie, H., & Plant, K. (2024). Internal auditing: An introduction ( 7th ed.). LexisNexis.
- Coetzee, P., & Lubbe, D. (2014). Improving the efficiency and effectiveness of risk-based internal audit engagements. *International Journal of Auditing*, 18(2), 115–125. [https://doi.org/10.1111/ijau.12016](https://doi.org/10.1111/ijau.12016)
- Cohen, M. K., Kolt, N., Bengio, Y., Hadfield, G. K., & Russell, S. (2024). Regulating advanced artificial agents: Governance frameworks should address the prospect of AI systems that cannot be safely tested. *Science*, 384(6691), 36–38. [https://doi.org/10.1126/science.adl0625](https://doi.org/10.1126/science.adl0625)
- Coram, P., Ferguson, C., & Moroney, R. (2008). Internal audit, alternative internal audit structures and the level of misappropriation of assets fraud. *Accounting & Finance*, 48(4), 543–559. [https://doi.org/10.1111/j.1467-629X.2007.00247.x](https://doi.org/10.1111/j.1467-629X.2007.00247.x)
- Couceiro, B., Pedrosa, I., & Marini, A. (2020). State of the art of artificial intelligence in internal audit context. In *Iberian Conference on Information Systems and Technologies (CISTI)* (pp. 1–7). IEEE. [https://doi.org/10.23919/CISTI49556.2020.9140863](https://doi.org/10.23919/CISTI49556.2020.9140863)
- Coulter, M., & Bensinger, G. (2023, February 9). Alphabet shares dive after Google AI chatbot Bard flubs answer in ad. Reuters. [https://perma.cc/PQ9P-5JAR](https://perma.cc/PQ9P-5JAR)
- Davidson, T., Denain, J.-S., Villalobos, P., & Bas, G. (2023). *AI capabilities can be significantly improved without expensive retraining*. arXiv. [http://arxiv.org/abs/2312.07413](http://arxiv.org/abs/2312.07413)
- Davies, H., & Zhivitskaya, M. (2018). Three lines of defence: A robust organising framework, or just lines in the sand? *Global Policy*, 9(S1), 34–42. [https://doi.org/10.1111/1758-5899.12568](https://doi.org/10.1111/1758-5899.12568)
- De Haes, S., & Van Grembergen, W. (2009). An exploratory study into IT governance implementations and its impact on business/IT alignment. *Information Systems Management*, 26(2), 123–137. [https://doi.org/10.1080/10580530902794786](https://doi.org/10.1080/10580530902794786)
- Decaux, L., & Sarens, G. (2015). Implementing combined assurance: Insights from multiple case studies. *Managerial Auditing Journal*, 30(1), 56–79. [https://doi.org/10.1108/MAJ-08-2014-1074](https://doi.org/10.1108/MAJ-08-2014-1074)
- Delgado, F., Yang, S., Madaio, M., & Yang, Q. (2023). The participatory turn in AI design: Theoretical foundations and the current state of practice. In *ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization* (pp. 1–23). ACM. [https://doi.org/10.1145/3617694.3623261](https://doi.org/10.1145/3617694.3623261)
- Drogalas, G., Pazarskis, M., Anagnostopoulou, E., & Papachristou, A. (2017). The effect of internal audit effectiveness, auditor responsibility and training in fraud detection. *Journal of Accounting and Management Information Systems*, 16(4), 434–454. [https://doi.org/10.24818/jamis.2017.04001](https://doi.org/10.24818/jamis.2017.04001)
- DSIT. (2022). *A pro-innovation approach to AI regulation*. DSIT. [https://perma.cc/W7TF-VYQL](https://perma.cc/W7TF-VYQL)
- DSIT. (2023a). *AI Safety Summit: Introduction*. DSIT. [https://perma.cc/TJA2-SA76](https://perma.cc/TJA2-SA76)
- DSIT. (2023b). *Capabilities and risks from frontier AI*. DSIT. [https://perma.cc/U9L6-UCBS](https://perma.cc/U9L6-UCBS)
- DSIT. (2023c). *Emerging processes for frontier AI safety*. DSIT. [https://perma.cc/Y9AY-5B4E](https://perma.cc/Y9AY-5B4E)
- DSIT. (2023d). *The Bletchley declaration by countries attending the AI safety summit, 1–2 November 2023*. DSIT. [https://perma.cc/EZ66-6KHE](https://perma.cc/EZ66-6KHE)
- DSIT. (2024a). *Seoul ministerial statement for advancing AI safety, innovation and inclusivity: AI Seoul summit 2024*. DSIT. [https://perma.cc/VG5H-YC3T](https://perma.cc/VG5H-YC3T)
- DSIT. (2024b). *Frontier AI safety commitments, AI Seoul summit 2024*. DSIT. [https://perma.cc/Y9ZR-AXRG](https://perma.cc/Y9ZR-AXRG)
- Duhigg, C. (2023, December 1). *The inside story of Microsoft's partnership with OpenAI*. The New Yorker. [https://perma.cc/NL8J-Z5GZ](https://perma.cc/NL8J-Z5GZ)
- Dungan, J., Waytz, A., & Young, L. (2015). The psychology of whistleblowing. *Current Opinion in Psychology*, 6, 129–133. [https://doi.org/10.1016/j.copsyc.2015.07.005](https://doi.org/10.1016/j.copsyc.2015.07.005)
- Egan, J., & Heim, L. (2023). *Oversight for frontier AI through a know-your-customer scheme for compute providers*. arXiv. [https://arxiv.org/abs/2310.13625](https://arxiv.org/abs/2310.13625)
- Eloundou, T., Manning, S., Mishkin, P., & Rock, D. (2024). GPTs are GPTs: Labor market impact potential of LLMs. *Science*, 384(6702), 1306–1308. [https://doi.org/10.1126/science.adj0998](https://doi.org/10.1126/science.adj0998)
- El-Sayed, S., Akbulut, C., McCroskery, A., Keeling, G., Kenton, Z., Jalan, Z., Marchal, N., Manzini, A., Shevlane, T., Vallor, S., Susser, D., Franklin, M., Bridgers, S., Law, H., Rahtz, M., Shanahan, M., Tessler, M. H., Douillard, A., Everitt, T., & Brown, S. (2024). *A mechanism-based approach to mitigating harms from persuasive generative AI*. arXiv. [http://arxiv.org/abs/2404.15058](http://arxiv.org/abs/2404.15058)
- Emett, S. A., Eulerich, M., Lipinski, E., Prien, N., & Wood, D. A. (2024). Leveraging ChatGPT for enhancing the internal audit process: A real-world example from a large multinational company. *Accounting Horizons*, 1–11. [https://doi.org/10.2139/ssrn.4514238](https://doi.org/10.2139/ssrn.4514238)
- Enriques, L., & Zetzsche, D. A. (2013). The risky business of regulating risk management in listed companies. *European Company and Financial Law Review*, 103(3), 271–303. [https://doi.org/10.1515/ecfr-2013-0271](https://doi.org/10.1515/ecfr-2013-0271)
- Erasmus, L., & Coetzee, P. (2018). Drivers of stakeholders’ view of internal audit effectiveness: Management versus audit committee. *Managerial Auditing Journal*, 33(1), 90–114. [https://doi.org/10.1108/MAJ-05-2017-1558](https://doi.org/10.1108/MAJ-05-2017-1558)
- von Eschenbach, W. J. (2021). Transparency and the black box problem: Why we do not trust AI. *Philosophy & Technology*, 34(4), 1607–1622. [https://doi.org/10.1007/s13347-021-00477-0](https://doi.org/10.1007/s13347-021-00477-0)
- Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., Lacey, K., Goodwin, A., Marek, Y., & Rombach, R. (2024). *Scaling rectified flow transformers for high-resolution image synthesis*. arXiv. [http://arxiv.org/abs/2403.03206](http://arxiv.org/abs/2403.03206)
- Eulerich, A., & Eulerich, M. (2020). What is the value of internal auditing? A literature review on qualitative and quantitative perspectives. *Maandblad Voor Accountancy En Bedrijfseconomie*, 94(3/4), 83–92. [https://doi.org/10.5117/mab.94.50375](https://doi.org/10.5117/mab.94.50375)
- European Banking Authority. (2021). *Guidelines on internal governance under Directive 2013/36/EU* (EBA/GL/2021/05). European Banking Authority. [https://perma.cc/RCD8-V99V](https://perma.cc/RCD8-V99V)
- European Commission. (2024, July 30). *AI Act: Participate in the drawing-up of the first general-purpose AI code of practice*. European Commission. [https://perma.cc/KFA2-QD5N](https://perma.cc/KFA2-QD5N)
- Falco, G., Shneiderman, B., Badger, J., Carrier, R., Dahbura, A., Danks, D., Eling, M., Goodloe, A., Gupta, J., Hart, C., Jirotka, M., Johnson, H., LaPointe, C., Llorens, A. J., Mackworth, A. K., Maple, C., Pálsson, S. E., Pasquale, F., Winfield, A., & Yeong, Z. K. (2021). Governing AI safety through independent audits. *Nature Machine Intelligence*, 3(7), 566–571. [https://doi.org/10.1038/s42256-021-00370-7](https://doi.org/10.1038/s42256-021-00370-7)
- Fang, R., Bindu, R., Gupta, A., & Kang, D. (2024). *LLM agents can autonomously exploit one-day vulnerabilities*. arXiv. [https://arxiv.org/abs/2404.08144](https://arxiv.org/abs/2404.08144)
- Fjelland, R. (2020). Why general artificial intelligence will not be realized. *Humanities and Social Sciences Communications*, 7, 10. [https://doi.org/10.1057/s41599-020-0494-4](https://doi.org/10.1057/s41599-020-0494-4)
- Forte, J., & Barac, K. (2015). Combined assurance: A systematic process. *Southern African Journal of Accountability and Auditing Research*, 17(2), 71–83. [https://perma.cc/42XM-JWVE](https://perma.cc/42XM-JWVE)
- Frank, M. R., Autor, D., Bessen, J. E., Brynjolfsson, E., Cebrian, M., Deming, D. J., Feldman, M., Groh, M., Lobo, J., Moro, E., Wang, D., Youn, H., & Rahwan, I. (2019). Toward understanding the impact of artificial intelligence on labor. *PNAS*, 116(14), 6531–6539. [https://doi.org/10.1073/pnas.1900949116](https://doi.org/10.1073/pnas.1900949116)
- Frontier Model Forum. (2023). *Advancing safe AI development*. Frontier Model Forum. [https://perma.cc/LK7G-ZPCP](https://perma.cc/LK7G-ZPCP)
- Gabriel, I. (2020). Artificial intelligence, values, and alignment. *Minds and Machines*, 30(3), 411–437. [https://doi.org/10.1007/s11023-020-09539-2](https://doi.org/10.1007/s11023-020-09539-2)
- Gabriel, I., Manzini, A., Keeling, G., Hendricks, L. A., Rieser, V., Iqbal, H., Tomašev, N., Ktena, I., Kenton, Z., Rodriguez, M., El-Sayed, S., Brown, S., Akbulut, C., Trask, A., Hughes, E., Bergman, A. S., Shelby, R., Marchal, N., Griffin, C., … Manyika, J. (2024). *The ethics of advanced AI assistants*. arXiv. [https://arxiv.org/abs/2404.16244](https://arxiv.org/abs/2404.16244)
- Gade, P., Lermen, S., Rogers-Smith, C., & Ladish, J. (2023). *BadLlama: Cheaply removing safety fine-tuning from Llama 2-Chat 13B*. arXiv. [http://arxiv.org/abs/2311.00117](http://arxiv.org/abs/2311.00117)
- Ganguli, D., Hernandez, D., Lovitt, L., Askell, A., Bai, Y., Chen, A., Conerly, T., Dassarma, N., Drain, D., Elhage, N., El Showk, S., Fort, S., Hatfield-Dodds, Z., Henighan, T., Johnston, S., Jones, A., Joseph, N., Kernian, J., Kravec, S., … Clark, J. (2022). Predictability and surprise in large generative models. In *ACM Conference on Fairness, Accountability, and Transparency* (pp. 1747–1764). ACM. [https://doi.org/10.1145/3531146.3533229](https://doi.org/10.1145/3531146.3533229)
- Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., Jones, A., Bowman, S., Chen, A., Conerly, T., DasSarma, N., Drain, D., Elhage, N., El-Showk, S., Fort, S., … Clark, J. (2022). *Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned*. arXiv. [https://arxiv.org/abs/2209.07858](https://arxiv.org/abs/2209.07858)
- Goertzel, B. (2014). Artificial general intelligence: Concept, state of the art, and future prospects. *Journal of Artificial General Intelligence*, 5(1), 1–48. [https://doi.org/10.2478/jagi-2014-0001](https://doi.org/10.2478/jagi-2014-0001)
- B. Goertzel, & C. Pennachin (Eds.). (2007). Artificial general intelligence. Springer. [https://doi.org/10.1007/978-3-540-68677-4](https://doi.org/10.1007/978-3-540-68677-4)
- Goldstein, J. A., Sastry, G., Musser, M., DiResta, R., Gentzel, M., & Sedova, K. (2023). *Generative language models and automated influence operations: Emerging threats and potential mitigations*. arXiv. [http://arxiv.org/abs/2301.04246](http://arxiv.org/abs/2301.04246)
- Google DeepMind. (2023a, October 27). *AI safety summit: An update on our approach to safety and responsibility*. Google DeepMind. [https://perma.cc/EJ9S-HDFY](https://perma.cc/EJ9S-HDFY)
- Google DeepMind. (2023b). *Responsibility & safety*. Google DeepMind. [https://perma.cc/LLR2-PT9J](https://perma.cc/LLR2-PT9J)
- Google DeepMind. (2024a). *Frontier safety framework (Version 1.0)*. Google DeepMind. [https://perma.cc/3C44-RSAN](https://perma.cc/3C44-RSAN)
- Google DeepMind. (2024b). *Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context*. arXiv. [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530)
- Gopal, A., Helm-Burger, N., Justen, L., Soice, E. H., Tzeng, T., Jeyapragasan, G., Grimm, S., Mueller, B., & Esvelt, K. M. (2023). *Will releasing the weights of future large language models grant widespread access to pandemic agents?* arXiv. [https://arxiv.org/abs/2310.18233](https://arxiv.org/abs/2310.18233)
- Gruetzemacher, R., Chan, A., Frazier, K., Manning, C., Los, Š., Fox, J., Hernández-Orallo, J., Burden, J., Franklin, M., Ghuidhir, C. N., Bailey, M., Eth, D., Pilditch, T., & Kilian, K. (2023). *An international consortium for evaluations of societal-scale risks from advanced AI*. arXiv. [https://arxiv.org/abs/2310.14455](https://arxiv.org/abs/2310.14455)
- Guénin-Paracini, H., Malsch, B., & Tremblay, M.-S. (2015). On the operational reality of auditors’ independence: Lessons from the field. *Auditing: A Journal of Practice & Theory*, 34(2), 201–236. [https://doi.org/10.2308/ajpt-50905](https://doi.org/10.2308/ajpt-50905)
- Gutierrez, C. I., Aguirre, A., Uuk, R., Boine, C. C., & Franklin, M. (2023). A proposal for a definition of general purpose artificial intelligence systems. *Digital Society*, 2(3), 36. [https://doi.org/10.1007/s44206-023-00068-w](https://doi.org/10.1007/s44206-023-00068-w)
- Gwern. (2020, May 28). *The scaling hypothesis*. Gwern. [https://perma.cc/A4YJ-567Q](https://perma.cc/A4YJ-567Q)
- Hackenburg, K., & Margetts, H. (2024). Evaluating the persuasive influence of political microtargeting with large language models, *PNAS*, 121(24). [https://doi.org/10.1073/pnas.2403116121](https://doi.org/10.1073/pnas.2403116121)
- Hacker, P., Engel, A., & Mauer, M. (2023). Regulating ChatGPT and other large generative AI models. In *ACM Conference on Fairness, Accountability, and Transparency* (pp. 1112–1123). ACM. [https://doi.org/10.1145/3593013.3594067](https://doi.org/10.1145/3593013.3594067)
- Hagendorff, T. (2020). The ethics of AI ethics: An evaluation of guidelines. *Minds and Machines*, 30(1), 99–120. [https://doi.org/10.1007/s11023-020-09517-8](https://doi.org/10.1007/s11023-020-09517-8)
- Hagendorff, T. (2024). Deception abilities emerged in large language models. *PNAS*, 121(24), e2317967121. [https://doi.org/10.1073/pnas.2317967121](https://doi.org/10.1073/pnas.2317967121)
- Hazell, J. (2023). *Spear phishing with large language models*. arXiv. [https://arxiv.org/abs/2305.06972](https://arxiv.org/abs/2305.06972)
- Heim, L., & Koessler, L. (2024). *Training compute thresholds: Features and functions in AI regulation*. arXiv. [http://arxiv.org/abs/2405.10799](http://arxiv.org/abs/2405.10799)
- Helfrich, G. (2024). The harms of terminology: Why we should reject so-called “frontier AI”. *AI and Ethics*, 4, 1–7. [https://doi.org/10.1007/s43681-024-00438-1](https://doi.org/10.1007/s43681-024-00438-1)
- Hendrycks, D., Carlini, N., Schulman, J., & Steinhardt, J. (2021). *Unsolved problems in ML safety*. arXiv. [https://arxiv.org/abs/2109.13916](https://arxiv.org/abs/2109.13916)
- Hendrycks, D., Mazeika, M., & Woodside, T. (2023). *An overview of catastrophic AI risks*. arXiv. [https://arxiv.org/abs/2306.12001](https://arxiv.org/abs/2306.12001)
- Hermanson, D. R., Hill, M. C., & Ivancevich, D. M. (2000). Information technology-related activities of internal auditors. *Journal of Information Systems*, 14(s-1), 39–53. [https://doi.org/10.2308/jis.2000.14.s-1.39](https://doi.org/10.2308/jis.2000.14.s-1.39)
- Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., & Zhou, Y. (2017). *Deep learning scaling is predictable, empirically*. arXiv. [http://arxiv.org/abs/1712.00409](http://arxiv.org/abs/1712.00409)
- Ho, L., Barnhart, J., Trager, R., Bengio, Y., Brundage, M., Carnegie, A., Chowdhury, R., Dafoe, A., Hadfield, G., Levi, M., & Snidal, D. (2023). *International institutions for advanced AI*. arXiv. [http://arxiv.org/abs/2307.04699](http://arxiv.org/abs/2307.04699)
- Hooker, S. (2024). *On the limitations of compute thresholds as a governance strategy*. arXiv. [http://arxiv.org/abs/2407.05694](http://arxiv.org/abs/2407.05694)
- Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., … Sifre, L. (2022). *Training compute-optimal large language models*. arXiv. [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)
- Hubinger, E. (2023, March 15). *Towards understanding-based safety evaluations* \[Online forum post\]. AI Alignment Forum. [https://perma.cc/LM58-MZKA](https://perma.cc/LM58-MZKA)
- Hubinger, E. (2024, January 13). *Introducing alignment stress-testing at Anthropic* \[Online forum post\]. AI Alignment Forum. [https://perma.cc/M8Q2-CDRS](https://perma.cc/M8Q2-CDRS)
- Hubinger, E., Denison, C., Mu, J., Lambert, M., Tong, M., MacDiarmid, M., Lanham, T., Ziegler, D. M., Maxwell, T., Cheng, N., Jermyn, A., Askell, A., Radhakrishnan, A., Anil, C., Duvenaud, D., Ganguli, D., Barez, F., Clark, J., Ndousse, K., … Perez, E. (2024). *Sleeper agents: Training deceptive LLMs that persist through safety training*. arXiv. [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)
- Huibers, S. C. J. (2015). Combined assurance: One language, one voice, one view. IIA Research Foundation, Global Internal Audit Common Body of Knowledge. [https://perma.cc/D7YM-9GSY](https://perma.cc/D7YM-9GSY)
- IEEE. (2008). *Standard for software reviews and audits* (IEEE Standard No. 1028-2008). IEEE. [https://doi.org/10.1109/IEEESTD.2008.4601584](https://doi.org/10.1109/IEEESTD.2008.4601584)
- IIA. (2013). *The three lines of defense in effective risk management and control*. IIA. [https://perma.cc/NQM2-DD7V](https://perma.cc/NQM2-DD7V)
- IIA. (2017a). *Artificial intelligence: Considerations for the profession of internal auditing (Part I)*. IIA. [https://perma.cc/K8WQ-VNFZ](https://perma.cc/K8WQ-VNFZ)
- IIA. (2017b). *The IIA's artificial intelligence auditing framework: Practical applications (Part A)*. IIA. [https://perma.cc/U93U-LN75](https://perma.cc/U93U-LN75)
- IIA. (2018). *The IIA's artificial intelligence auditing framework: Practical applications (Part B)*. IIA. [https://perma.cc/826X-Y3L7](https://perma.cc/826X-Y3L7)
- IIA. (2020). *The IIA's three lines model: An update of the three lines of defense*. IIA. [https://perma.cc/GAB5-DMN3](https://perma.cc/GAB5-DMN3)
- IIA. (2022). *Combined assurance: Aligning assurance for effective risk management*. IIA. [https://perma.cc/A65C-8Q4F](https://perma.cc/A65C-8Q4F)
- IIA. (2023). *Certified internal auditor*. IIA. [https://perma.cc/6PE7-CGFW](https://perma.cc/6PE7-CGFW)
- IIA. (2024). *Global internal audit standards*. IIA. [https://perma.cc/2SV3-YEF3](https://perma.cc/2SV3-YEF3)
- Irving, G., Christiano, P., & Amodei, D. (2018). *AI safety via debate*. arXiv. [http://arxiv.org/abs/1805.00899](http://arxiv.org/abs/1805.00899)
- ISACA. (2018). *Auditing artificial intelligence*. ISACA. [https://perma.cc/J53N-5P5F](https://perma.cc/J53N-5P5F)
- ISO & IEC. (2024). *Information technology: Governance of IT for the organization* (ISO/IEC Standard No. 38500:2024). [https://www.iso.org/standard/81684.html](https://www.iso.org/standard/81684.html)
- Järviniemi, O., & Hubinger, E. (2024). *Uncovering deceptive tendencies in language models: A simulated company AI assistant*. arXiv. [http://arxiv.org/abs/2405.01576](http://arxiv.org/abs/2405.01576)
- Ji, J., Qiu, T., Chen, B., Zhang, B., Lou, H., Wang, K., Duan, Y., He, Z., Zhou, J., Zhang, Z., Zeng, F., Ng, K. Y., Dai, J., Pan, X., O'Gara, A., Lei, Y., Xu, H., Tse, B., Fu, J., & Gao, W. (2023). *AI alignment: A comprehensive survey*. arXiv. [https://arxiv.org/abs/2310.19852](https://arxiv.org/abs/2310.19852)
- Jiang, L., Messier Jr., W. F., & Wood, D. A. (2020). The association between internal audit operations-related services and firm operating performance. *Auditing: A Journal of Practice & Theory*, 39(1), 101–124. [https://doi.org/10.2308/ajpt-52565](https://doi.org/10.2308/ajpt-52565)
- Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. *Nature Machine Intelligence*, 1(9), 389–399. [https://doi.org/10.1038/s42256-019-0088-2](https://doi.org/10.1038/s42256-019-0088-2)
- Jones, E. (2023, July 17). Explainer: What is a foundation model? Ada Lovelace Institute. [https://perma.cc/JQP5-S5G5](https://perma.cc/JQP5-S5G5)
- Jubb, P. B. (1999). Whistleblowing: A restrictive definition and interpretation. *Journal of Business Ethics*, 21(1), 77–94. [https://doi.org/10.1023/A:1005922701763](https://doi.org/10.1023/A:1005922701763)
- Kahyaoglu, S. B., & Aksoy, T. (2021). Artificial intelligence in internal audit and risk assessment. In U. Hacioglu & T. Aksoy (Eds.), Financial ecosystem and strategy in the digital era: Global approaches and new opportunities (pp. 179–192). Springer. [https://doi.org/10.1007/978-3-030-72624-9\_8](https://doi.org/10.1007/978-3-030-72624-9_8)
- Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). *Scaling laws for neural language models*. arXiv. [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)
- Kapoor, S., Bommasani, R., Klyman, K., Longpre, S., Ramaswami, A., Cihon, P., Hopkins, A., Bankston, K., Biderman, S., Bogen, M., Chowdhury, R., Engler, A., Henderson, P., Jernite, Y., Lazar, S., Maffulli, S., Nelson, A., Pineau, J., Skowron, A., … Narayanan, A. (2024). *On the societal impact of open foundation models*. arXiv. [https://arxiv.org/abs/2403.07918](https://arxiv.org/abs/2403.07918)
- Karanja, E., & Rosso, M. A. (2017). The chief risk officer: A study of roles and responsibilities. *Risk Management*, 19(2), 103–130. [https://doi.org/10.1057/s41283-017-0014-z](https://doi.org/10.1057/s41283-017-0014-z)
- Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik, V., & Irving, G. (2021). *Alignment of language agents*. arXiv. [https://arxiv.org/abs/2103.14659](https://arxiv.org/abs/2103.14659)
- Kenton, Z., Siegel, N. Y., Kramár, J., Brown-Cohen, J., Albanie, S., Bulian, J., Agarwal, R., Lindner, D., Tang, Y., Goodman, N. D., & Shah, R. (2024). *On scalable oversight with weak LLMs judging strong LLMs*. arXiv. [http://arxiv.org/abs/2407.04622](http://arxiv.org/abs/2407.04622)
- Kinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R., Wijk, H., Burget, J., Ho, A., Barnes, E., & Christiano, P. (2023). *Evaluating language-model agents on realistic autonomous tasks*. arXiv. [https://arxiv.org/abs/2312.11671](https://arxiv.org/abs/2312.11671)
- Klinke, A., & Renn, O. (2021). The coming of age of risk governance. *Risk Analysis*, 41(3), 544–557. [https://doi.org/10.1111/risa.13383](https://doi.org/10.1111/risa.13383)
- Koessler, L., & Schuett, J. (2023). *Risk assessment at AGI companies: A review of popular risk assessment techniques from other safety-critical industries*. arXiv. [https://arxiv.org/abs/2307.08823](https://arxiv.org/abs/2307.08823)
- Koessler, L., Schuett, J., & Anderljung, M. (2024). *Risk thresholds for frontier AI*. arXiv. [http://arxiv.org/abs/2406.14713](http://arxiv.org/abs/2406.14713)
- Kolt, N. (2023). Algorithmic black swans. *Washington University Law Review*, 101, 1177–1240. [https://perma.cc/WRZ8-5R52](https://perma.cc/WRZ8-5R52)
- Kolt, N., Anderljung, M., Barnhart, J., Brass, A., Esvelt, K., Hadfield, G. K., Heim, L., Rodriguez, M., Sandbrink, J. B., & Woodside, T. (2024). *Responsible reporting for frontier AI development*. arXiv. [https://arxiv.org/abs/2404.02675](https://arxiv.org/abs/2404.02675)
- Kotb, A., Elbardan, H., & Halabi, H. (2020). Mapping of internal audit research: A post-Enron structured literature review. *Accounting, Auditing & Accountability Journal*, 33(8), 1969–1996. [https://doi.org/10.1108/AAAJ-07-2018-3581](https://doi.org/10.1108/AAAJ-07-2018-3581)
- Krakovna, V., & Kramar, J. (2023). *Power-seeking can be probable and predictive for trained agents*. arXiv. [https://arxiv.org/abs/2304.06528](https://arxiv.org/abs/2304.06528)
- Kurzweil, R. (1990). The age of intelligent machines. MIT Press.
- Lambert, N., Castricato, L., von Werra, L., & Havrilla, A. (2022, December 9). Illustrating reinforcement learning from human feedback (RLHF). Hugging Face. [https://perma.cc/R9HU-TQ9X](https://perma.cc/R9HU-TQ9X)
- Leech, T. J., & Hanlon, L. C. (2016). Three lines of defense versus five lines of assurance: Elevating the role of the board and CEO in risk governance. In R. Leblanc (Ed.), The handbook of board governance: A comprehensive guide for public, private and not-for-profit board members (pp. 335–355). Wiley. [https://doi.org/10.1002/9781119245445.ch17](https://doi.org/10.1002/9781119245445.ch17)
- Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., & Legg, S. (2018). *Scalable agent alignment via reward modeling: A research direction*. arXiv. [http://arxiv.org/abs/1811.07871](http://arxiv.org/abs/1811.07871)
- Lenz, R., & Hahn, U. (2015). A synthesis of empirical internal audit effectiveness literature pointing to new research opportunities. *Managerial Auditing Journal*, 30(1), 5–33. [https://doi.org/10.1108/MAJ-08-2014-1072](https://doi.org/10.1108/MAJ-08-2014-1072)
- Lermen, S., Rogers-Smith, C., & Ladish, J. (2023). *LoRA fine-tuning efficiently undoes safety training in Llama 2-Chat 70B*. arXiv. [http://arxiv.org/abs/2310.20624](http://arxiv.org/abs/2310.20624)
- Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., Ré, C., Acosta-Navas, D., Hudson, D. A., … Koreeda, Y. (2022). *Holistic evaluation of language models*. arXiv. [http://arxiv.org/abs/2211.09110](http://arxiv.org/abs/2211.09110)
- Li, H., Lam, H. K. S., Ho, W., & Yeung, A. C. L. (2022). The impact of chief risk officer appointments on firm risk and operational efficiency. *Journal of Operations Management*, 68(3), 241–269. [https://doi.org/10.1002/joom.1175](https://doi.org/10.1002/joom.1175)
- Lin, S., Pizzini, M., Vargus, M., & Bardhan, I. R. (2011). The role of the internal audit function in the disclosure of material weaknesses. *The Accounting Review*, 86(1), 287–323. [https://doi.org/10.2308/accr.00000016](https://doi.org/10.2308/accr.00000016)
- Lohn, A. J., & Jackson, K. A. (2022). Will AI make cyber swords or shields: A few mathematical models of technological progress. Center for Security and Emerging Technology. [https://doi.org/10.51593/2022CA002](https://doi.org/10.51593/2022CA002)
- Lohn, A. J., & Musser, M. (2022). AI and compute: How much longer can computing power drive artificial intelligence progress? Center for Security and Emerging Technology. [https://doi.org/10.51593/2021CA009](https://doi.org/10.51593/2021CA009)
- Longpre, S., Kapoor, S., Klyman, K., Ramaswami, A., Bommasani, R., Blili-Hamelin, B., Huang, Y., Skowron, A., Yong, Z.-X., Kotha, S., Zeng, Y., Shi, W., Yang, X., Southen, R., Robey, A., Chao, P., Yang, D., Jia, R., Kang, D., … Henderson, P. (2024). *A safe harbor for AI evaluation and red teaming*. arXiv. [https://arxiv.org/abs/2403.04893](https://arxiv.org/abs/2403.04893)
- Lundqvist, S. A. (2015). Why firms implement risk governance: Stepping beyond traditional risk management to enterprise risk management. *Journal of Accounting and Public Policy*, 34(5), 441–466. [https://doi.org/10.1016/j.jaccpubpol.2015.05.002](https://doi.org/10.1016/j.jaccpubpol.2015.05.002)
- MacDiarmid, M., Maxwell, T., Schiefer, N., Mu, J., Kaplan, J., Duvenaud, D., Bowman, S., Tamkin, A., Perez, E., Sharma, M., Denison, C., & Hubinger, E. (2024, April 23). Simple probes can catch sleeper agents. Anthropic. [https://perma.cc/2BPK-BQNK](https://perma.cc/2BPK-BQNK)
- Mahler, T. (2022). Between risk management and proportionality: The risk-based approach in the EU's Artificial Intelligence Act proposal. In L. Colonna & S. Greenstein (Eds.), Nordic yearbook of law and informatics 2020-2021: Law in the era of artificial intelligence (pp. 247–270). [https://doi.org/10.53292/208f5901.38a67238](https://doi.org/10.53292/208f5901.38a67238)
- Marchal, N., Xu, R., Elasmar, R., Gabriel, I., Goldberg, B., & Isaac, W. (2024). *Generative AI misuse: A taxonomy of tactics and insights from real-world data*. arXiv. [http://arxiv.org/abs/2406.13843](http://arxiv.org/abs/2406.13843)
- Maslej, N., Fattorini, L., Perrault, R., Parli, V., Reuel, A., Brynjolfsson, E., Etchemendy, J., Ligett, K., Lyons, T., Manyika, J., Niebles, J. C., Shoham, Y., Wald, R., & Clark, J. (2024). *The AI index 2024 annual report*. Institute for Human-Centered AI, Stanford University. [https://perma.cc/EV87-JNS8](https://perma.cc/EV87-JNS8)
- Ma'ayan, Y., & Carmeli, A. (2016). Internal audits as a source of ethical behavior, efficiency, and effectiveness in work units. *Journal of Business Ethics*, 137(2), 347–363. [https://doi.org/10.1007/s10551-015-2561-0](https://doi.org/10.1007/s10551-015-2561-0)
- McCarthy, J. (2007). *What is artificial intelligence?* Stanford University. [https://perma.cc/QL9Y-AY8A](https://perma.cc/QL9Y-AY8A)
- McCarthy, J., Minsky, M. L., Rochester, N., & Shannon, C. E. (1955). A proposal for the Dartmouth summer research project on artificial intelligence. *AI Magazine*, 27, 12. [https://perma.cc/S9DU-GWFF](https://perma.cc/S9DU-GWFF)
- McFadden, M., Jones, K., Taylor, E., & Osborn, G. (2021). Harmonising artificial intelligence: The role of standards in the EU AI regulation. Oxford Information Labs. [https://perma.cc/X3AZ-5H7C](https://perma.cc/X3AZ-5H7C)
- McShane, M. (2018). Enterprise risk management: History and a design science proposal. *The Journal of Risk Finance*, 19(2), 137–153. [https://doi.org/10.1108/JRF-03-2017-0048](https://doi.org/10.1108/JRF-03-2017-0048)
- Merhout, J. W., & Havelka, D. (2008). Information technology auditing: A value-added IT governance partnership between IT management and audit. *Communications of the Association for Information Systems*, 23, 463–482. [https://doi.org/10.17705/1CAIS.02326](https://doi.org/10.17705/1CAIS.02326)
- Meta. (2024). *Llama 3 model card*. GitHub. [https://perma.cc/6D5U-DDSV](https://perma.cc/6D5U-DDSV)
- METR. (2023, March 17). *Update on ARC's recent eval efforts*. METR. [https://perma.cc/87KZ-GN56](https://perma.cc/87KZ-GN56)
- Meyer, B. (2011, October 28). John McCarthy. *Communications of the ACM*. [https://perma.cc/49S8-3GM6](https://perma.cc/49S8-3GM6)
- Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozière, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., Grave, E., LeCun, Y., … Scialom, T. (2023). *Augmented language models: A survey*. arXiv. [http://arxiv.org/abs/2302.07842](http://arxiv.org/abs/2302.07842)
- Minsky, M. (1969). Semantic information processing. MIT Press.
- Mirsky, Y., Demontis, A., Kotak, J., Shankar, R., Gelei, D., Yang, L., Zhang, X., Lee, W., Elovici, Y., & Biggio, B. (2021). *The threat of offensive AI to organizations*. arXiv. [https://arxiv.org/abs/2106.15764](https://arxiv.org/abs/2106.15764)
- Mitchell, M. (2021). *Why AI is harder than we think*. arXiv. [https://arxiv.org/abs/2104.12871](https://arxiv.org/abs/2104.12871)
- Mitchell, M. (2024). Debates on the nature of artificial general intelligence. *Science*, 383(6689). [https://doi.org/10.1126/science.ado7069](https://doi.org/10.1126/science.ado7069)
- Mittelstadt, B. (2019). Principles alone cannot guarantee ethical AI. *Nature Machine Intelligence*, 1(11), 501–507. [https://doi.org/10.1038/s42256-019-0114-4](https://doi.org/10.1038/s42256-019-0114-4)
- Mohamed, S., Png, M.-T., & Isaac, W. (2020). Decolonial AI: Decolonial theory as sociotechnical foresight in artificial intelligence. *Philosophy & Technology*, 33(4), 659–684. [https://doi.org/10.1007/s13347-020-00405-8](https://doi.org/10.1007/s13347-020-00405-8)
- Mökander, J., & Floridi, L. (2023). Operationalising AI governance through ethics-based auditing: An industry case study. *AI and Ethics*, 3(2), 451–468. [https://doi.org/10.1007/s43681-022-00171-7](https://doi.org/10.1007/s43681-022-00171-7)
- Mökander, J., Morley, J., Taddeo, M., & Floridi, L. (2021). Ethics-based auditing of automated decision-making systems: Nature, scope, and limitations. *Science and Engineering Ethics*, 27(4), 44. [https://doi.org/10.1007/s11948-021-00319-4](https://doi.org/10.1007/s11948-021-00319-4)
- Mökander, J., Schuett, J., Kirk, H. R., & Floridi, L. (2023). Auditing large language models: A three-layered approach. *AI and Ethics*. [https://doi.org/10.1007/s43681-023-00289-2](https://doi.org/10.1007/s43681-023-00289-2)
- Morley, J., Floridi, L., Kinsey, L., & Elhalal, A. (2020). From what to how: An initial review of publicly available AI ethics tools, methods and research to translate principles into practices. *Science and Engineering Ethics*, 26(4), 2141–2168. [https://doi.org/10.1007/s11948-019-00165-5](https://doi.org/10.1007/s11948-019-00165-5)
- Morley, J., Kinsey, L., Elhalal, A., Garcia, F., Ziosi, M., & Floridi, L. (2023). Operationalising AI ethics: Barriers, enablers and next steps. *AI & Society*, 38(1), 411–423. [https://doi.org/10.1007/s00146-021-01308-8](https://doi.org/10.1007/s00146-021-01308-8)
- Morris, M. R., Sohl-dickstein, J., Fiedel, N., Warkentin, T., Dafoe, A., Faust, A., Farabet, C., & Legg, S. (2023). *Levels of AGI: Operationalizing progress on the path to AGI*. arXiv. [http://arxiv.org/abs/2311.02462](http://arxiv.org/abs/2311.02462)
- Mouton, C. A., Lucas, C., & Guest, E. (2023). The operational risks of AI in large-scale biological attacks: A red-team approach. RAND. [https://doi.org/10.7249/RRA2977-1](https://doi.org/10.7249/RRA2977-1)
- Nagy, A. L., & Cenker, W. J. (2002). An assessment of the newly defined internal audit function. *Managerial Auditing Journal*, 17(3), 130–137. [https://doi.org/10.1108/02686900210419912](https://doi.org/10.1108/02686900210419912)
- Nanda, N., Chan, L., Lieberum, T., Smith, J., & Steinhardt, J. (2023). *Progress measures for grokking via mechanistic interpretability*. arXiv. [http://arxiv.org/abs/2301.05217](http://arxiv.org/abs/2301.05217)
- Narayanan, A., & Kapoor, S. (2024, June 27). *AI scaling myths*. AI Snake Oil. [https://perma.cc/8VB8-UTAA](https://perma.cc/8VB8-UTAA)
- Nature Editorial Board. (2023). Stop talking about tomorrow's AI doomsday when AI poses risks today. *Nature*, 618(7967), 885–886. [https://doi.org/10.1038/d41586-023-02094-7](https://doi.org/10.1038/d41586-023-02094-7)
- Naudé, W., & Dimitri, N. (2020). The race for an artificial general intelligence: Implications for public policy. *AI & Society*, 35(2), 367–379. [https://doi.org/10.1007/s00146-019-00887-x](https://doi.org/10.1007/s00146-019-00887-x)
- Nevo, S., Lahav, D., Karpur, A., Alstott, J., & Matheny, J. (2024). *Securing AI model weights: Preventing theft and misuse of frontier models*. RAND. [https://doi.org/10.7249/RRA2849-1](https://doi.org/10.7249/RRA2849-1)
- Ngo, R., Chan, L., & Mindermann, S. (2022). *The alignment problem from a deep learning perspective*. arXiv. [https://arxiv.org/abs/2209.00626](https://arxiv.org/abs/2209.00626)
- Nilsson, N. J. (2009). The quest for artificial intelligence: A history of ideas and achievements. Cambridge University Press. [https://perma.cc/CQV7-N233](https://perma.cc/CQV7-N233)
- NIST. (2018). *Framework for improving critical infrastructure cybersecurity (Version 1.1)*. NIST. [https://doi.org/10.6028/NIST.CSWP.04162018](https://doi.org/10.6028/NIST.CSWP.04162018)
- NIST. (2022). *Secure software development framework (SSDF) (Version 1.1)*. NIST. [https://doi.org/10.6028/NIST.SP.800-218](https://doi.org/10.6028/NIST.SP.800-218)
- NIST. (2023). *Artificial intelligence risk management framework (AI RMF 1.0)*. NIST. [https://doi.org/10.6028/NIST.AI.100-1](https://doi.org/10.6028/NIST.AI.100-1)
- NIST. (2024). *Managing misuse risk for dual-use foundation models (AI 800-1 initial public draft)*. NIST. [https://doi.org/10.6028/NIST.AI.800-1.ipd](https://doi.org/10.6028/NIST.AI.800-1.ipd)
- Nordin, I. G. (2023). Narratives of internal audit: The Sisyphean work of becoming “independent”. *Critical Perspectives on Accounting*, 94, 102448. [https://doi.org/10.1016/j.cpa.2022.102448](https://doi.org/10.1016/j.cpa.2022.102448)
- Novelli, C., Casolari, F., Rotolo, A., Taddeo, M., & Floridi, L. (2023). Taking AI risks seriously: A new assessment model for the AI Act. *AI & Society*, 39, 2493–2497. [https://doi.org/10.1007/s00146-023-01723-z](https://doi.org/10.1007/s00146-023-01723-z)
- Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., & Carter, S. (2020). Zoom in: An introduction to circuits. *Distill*, 5(3). [https://doi.org/10.23915/distill.00024.001](https://doi.org/10.23915/distill.00024.001)
- OpenAI. (2019, March 11). *OpenAI LP*. OpenAI. [https://perma.cc/CR43-UD5H](https://perma.cc/CR43-UD5H)
- OpenAI. (2023a). *DALL·E 3 system card*. OpenAI. [https://perma.cc/PV2T-5x3A](https://perma.cc/PV2T-5x3A)
- OpenAI. (2023b). *GPT-4 technical report*. arXiv. [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)
- OpenAI. (2023c, October 26). *Frontier risk and preparedness*. OpenAI. [https://perma.cc/HJ6G-EVBP](https://perma.cc/HJ6G-EVBP)
- OpenAI. (2023d, November 17). *OpenAI announces leadership transition*. OpenAI. [https://perma.cc/PN5F-MQN3](https://perma.cc/PN5F-MQN3)
- OpenAI. (2023e, October 26). *OpenAI's approach to frontier risk*. OpenAI. [https://perma.cc/9YGS-NZVX](https://perma.cc/9YGS-NZVX)
- OpenAI. (2023f, December 18). *Preparedness framework (Beta)*. OpenAI. [https://perma.cc/Y5DU-LZNX](https://perma.cc/Y5DU-LZNX)
- OpenAI. (2023g, November 29). *Sam Altman returns as CEO, OpenAI has a new initial board*. OpenAI. [https://perma.cc/CH54-8YJE](https://perma.cc/CH54-8YJE)
- OpenAI. (2024a, January 31). *Building an early warning system for LLM-aided biological threat creation*. OpenAI. [https://perma.cc/2EN3-KALK](https://perma.cc/2EN3-KALK)
- OpenAI. (2024b, March 8). *Review completed & Altman, Brockman to continue to lead OpenAI*. OpenAI. [https://perma.cc/G4PE-9FC8](https://perma.cc/G4PE-9FC8)
- OpenAI. (2024c, August 8). *GPT-4o system card*. OpenAI. [https://perma.cc/U6C7-ALS4](https://perma.cc/U6C7-ALS4)
- OpenSSF. (2023). Safeguarding artifact integrity across any software supply chain. SLSA. [https://perma.cc/Y9UW-YYWZ](https://perma.cc/Y9UW-YYWZ)
- Our World in Data. (2023). *Computation used to train notable artificial intelligence systems*. Our World in Data. [https://perma.cc/Z7x4-86XC](https://perma.cc/Z7x4-86XC)
- Oussii, A. A., & Boulila Taktak, N. (2018). The impact of internal audit function characteristics on internal control quality. *Managerial Auditing Journal*, 33(5), 450–469. [https://doi.org/10.1108/MAJ-06-2017-1579](https://doi.org/10.1108/MAJ-06-2017-1579)
- O'Brien, J., Ee, S., & Williams, Z. (2023). *Deployment corrections: An incident response framework for frontier AI models*. arXiv. [https://arxiv.org/abs/2310.00328](https://arxiv.org/abs/2310.00328)
- Pacchiardi, L., Chan, A. J., Mindermann, S., Moscovitz, I., Pan, A. Y., Gal, Y., Evans, O., & Brauner, J. (2023). *How to catch an AI liar: Lie detection in black-box LLMs by asking unrelated questions*. arXiv. [http://arxiv.org/abs/2309.15840](http://arxiv.org/abs/2309.15840)
- Park, P. S., Goldstein, S., O'Gara, A., Chen, M., & Hendrycks, D. (2023). *AI deception: A survey of examples, risks, and potential solutions*. arXiv. [https://arxiv.org/abs/2308.14752](https://arxiv.org/abs/2308.14752)
- Partnership on AI. (2023). *PAI's guidance for safe foundation model deployment: A framework for collective action*. Partnership on AI. [https://perma.cc/W9GN-6QY3](https://perma.cc/W9GN-6QY3)
- Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N., & Irving, G. (2022). *Red teaming language models with language models*. arXiv. [https://arxiv.org/abs/2202.03286](https://arxiv.org/abs/2202.03286)
- Perrigo, B. (2023, February 17). *Bing's AI is threatening users. That's no laughing matter*. Time. [https://perma.cc/74ZT-ECG4](https://perma.cc/74ZT-ECG4)
- Petropoulos, F., Apiletti, D., Assimakopoulos, V., Babai, M. Z., Barrow, D. K., Ben Taieb, S., Bergmeir, C., Bessa, R. J., Bijak, J., Boylan, J. E., Browell, J., Carnevale, C., Castle, J. L., Cirillo, P., Clements, M. P., Cordeiro, C., Oliveira, F. L. C., De Baets, S., Dokumentov, A., … Ziel, F. (2022). Forecasting: Theory and practice. *International Journal of Forecasting*, 38(3), 705–871. [https://doi.org/10.1016/j.ijforecast.2021.11.001](https://doi.org/10.1016/j.ijforecast.2021.11.001)
- Phuong, M., Aitchison, M., Catt, E., Cogan, S., Kaskasoli, A., Krakovna, V., Lindner, D., Rahtz, M., Assael, Y., Hodkinson, S., Howard, H., Lieberum, T., Kumar, R., Raad, M. A., Webson, A., Ho, L., Lin, S., Farquhar, S., Hutter, M., … Shevlane, T. (2024). *Evaluating frontier models for dangerous capabilities*. arXiv. [http://arxiv.org/abs/2403.13793](http://arxiv.org/abs/2403.13793)
- Power, M. (1984). The audit explosion. Demos. [https://perma.cc/3HHY-4XWH](https://perma.cc/3HHY-4XWH)
- Prinsloo, A., & Maroun, W. (2021). An exploratory study on the components and quality of combined assurance in an integrated or a sustainability reporting setting. *Sustainability Accounting Management and Policy Journal*, 12(1), 1–29. [https://doi.org/10.1108/SAMPJ-05-2019-0205](https://doi.org/10.1108/SAMPJ-05-2019-0205)
- PwC. (2015). *Covering your bases: Implementing appropriate levels of combined assurance*. PwC. [https://perma.cc/H2BS-U7ZD](https://perma.cc/H2BS-U7ZD)
- Rae, A., Alexander, R., & McDermid, J. (2014). Fixing the cracks in the crystal ball: A maturity model for quantitative risk assessment. *Reliability Engineering & System Safety*, 125, 67–81. [https://doi.org/10.1016/j.ress.2013.09.008](https://doi.org/10.1016/j.ress.2013.09.008)
- Raji, I. D., & Buolamwini, J. (2019). Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial AI products. In *AAAI/ACM Conference on AI, Ethics, and Society* (pp. 429–435). ACM. [https://doi.org/10.1145/3306618.3314244](https://doi.org/10.1145/3306618.3314244)
- Raji, I. D., Smart, A., White, R. N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020). Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing. In *Conference on Fairness, Accountability, and Transparency* (pp. 33–44). ACM. [https://doi.org/10.1145/3351095.3372873](https://doi.org/10.1145/3351095.3372873)
- Raji, I. D., Xu, P., Honigsberg, C., & Ho, D. (2022). Outsider oversight: Designing a third party audit ecosystem for AI governance. In *AAAI/ACM Conference on AI, Ethics, and Society* (pp. 557–571). ACM. [https://doi.org/10.1145/3514094.3534181](https://doi.org/10.1145/3514094.3534181)
- Rando, J., Paleka, D., Lindner, D., Heim, L., & Tramèr, F. (2022). *Red-teaming the stable diffusion safety filter*. arXiv. [https://arxiv.org/abs/2210.04610](https://arxiv.org/abs/2210.04610)
- Rogers, A., & Luccioni, A. S. (2023). *Position: Key claims in LLM research have a long tail of footnotes*. arXiv. [https://arxiv.org/abs/2308.07120](https://arxiv.org/abs/2308.07120)
- Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). *High-resolution image synthesis with latent diffusion models*. arXiv. [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)
- Roose, K. (2023, February 16). A conversation with Bing's chatbot left me deeply unsettled. *The New York Times*. [https://perma.cc/2BJP-9QZ8](https://perma.cc/2BJP-9QZ8)
- Roussy, M. (2013). Internal auditors’ roles: From watchdogs to helpers and protectors of the top manager. *Critical Perspectives on Accounting*, 24(7), 550–571. [https://doi.org/10.1016/j.cpa.2013.08.004](https://doi.org/10.1016/j.cpa.2013.08.004)
- Roussy, M., & Brivot, M. (2016). Internal audit quality: A polysemous notion? *Accounting Auditing & Accountability*, 29(5), 714–738. [https://doi.org/10.1108/AAAJ-10-2014-1843](https://doi.org/10.1108/AAAJ-10-2014-1843)
- Roussy, M., & Perron, A. (2018). New perspectives in internal audit research: A structured literature review. *Accounting Perspectives*, 17(3), 345–385. [https://doi.org/10.1111/1911-3838.12180](https://doi.org/10.1111/1911-3838.12180)
- Roussy, M., & Rodrigue, M. (2018). Internal audit: Is the ‘third line of defense’ effective as a form of governance? An exploratory study of the impression management techniques chief audit executives use in their annual accountability to the audit committee. *Journal of Business Ethics*, 151(3), 853–869. [https://doi.org/10.1007/s10551-016-3263-y](https://doi.org/10.1007/s10551-016-3263-y)
- Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. *Nature Machine Intelligence*, 1(5), 206–215. [https://doi.org/10.1038/s42256-019-0048-x](https://doi.org/10.1038/s42256-019-0048-x)
- Russell, S., & Norvig, P. (2021). Artificial intelligence: A modern approach ( 4th ed.). Pearson.
- Røyksund, M., & Flage, R. (2019). When is a risk assessment deficient according to an uncertainty-based risk perspective? *Risk Analysis*, 39(4), 761–776. [https://doi.org/10.1111/risa.13195](https://doi.org/10.1111/risa.13195)
- Sætra, H. S., & Danaher, J. (2023). Resolving the battle of short- vs. long-term AI risks. *AI and Ethics*. [https://doi.org/10.1007/s43681-023-00336-y](https://doi.org/10.1007/s43681-023-00336-y)
- Sandbrink, J. B. (2023). *Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools*. arXiv. [https://arxiv.org/abs/2306.13952](https://arxiv.org/abs/2306.13952)
- Sarens, G., De Beelde, I., & Everaert, P. (2009). Internal audit: A comfort provider to the audit committee. *The British Accounting Review*, 41(2), 90–106. [https://doi.org/10.1016/j.bar.2009.02.002](https://doi.org/10.1016/j.bar.2009.02.002)
- Sarens, G., Decaux, L., & Lenz, R. (2012). Combined assurance: Case studies on a holistic approach to organizational governance. IIA Research Foundation.
- Sastry, G., Heim, L., Belfield, H., Anderljung, M., Brundage, M., Hazell, J., O'Keefe, C., Hadfield, G. K., Ngo, R., Pilz, K., Gor, G., Bluemke, E., Shoker, S., Egan, J., Trager, R. F., Avin, S., Weller, A., Bengio, Y., & Coyle, D. (2024). *Computing power and the governance of artificial intelligence*. arXiv. [https://arxiv.org/abs/2402.08797](https://arxiv.org/abs/2402.08797)
- Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., … Wolf, T. (2022). *Bloom: A 176B-parameter open-access multilingual language model*. arXiv. [http://arxiv.org/abs/2211.05100](http://arxiv.org/abs/2211.05100)
- Schaeffer, R., Miranda, B., & Koyejo, S. (2023). Are emergent abilities of large language models a mirage? In *Conference on Neural Information Processing Systems*. [https://arxiv.org/abs/2304.15004](https://arxiv.org/abs/2304.15004)
- Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., & Scialom, T. (2023). *Toolformer: Language models can teach themselves to use tools*. arXiv. [http://arxiv.org/abs/2302.04761](http://arxiv.org/abs/2302.04761)
- Schuett, J. (2023). Defining the scope of AI regulations. *Law, Innovation and Technology*, 15(1), 60–82. [https://doi.org/10.1080/17579961.2023.2184135](https://doi.org/10.1080/17579961.2023.2184135)
- Schuett, J. (2024). Risk management in the Artificial Intelligence Act. *European Journal of Risk Regulation*, 15, 367–385. [https://doi.org/10.1017/err.2023.1](https://doi.org/10.1017/err.2023.1)
- Schuett, J. (2023c). Three lines of defense against risks from AI. *AI & Society*. [https://doi.org/10.1007/s00146-023-01811-0](https://doi.org/10.1007/s00146-023-01811-0)
- Schuett, J., Dreksler, N., Anderljung, M., McCaffary, D., Heim, L., Bluemke, E., & Garfinkel, B. (2023). *Towards best practices in AGI safety and governance: A survey of expert opinion*. arXiv. [https://arxiv.org/abs/2305.07153](https://arxiv.org/abs/2305.07153)
- Schuett, J., Reuel, A., & Carlier, A. (2024). How to design an AI ethics board. *AI and Ethics*. [https://doi.org/10.1007/s43681-023-00409-y](https://doi.org/10.1007/s43681-023-00409-y)
- Schuett, J., Anderljung, M., Koessler, L., Carlier, A., & Garfinkel, B. (2024). From principles to rules: A regulatory approach for frontier AI. In P. Hacker, A. Engel, S. Hammer, & B. Mittelstadt (Eds.), The Oxford handbook on the foundations and regulation of generative AI. Oxford University Press. [https://arxiv.org/abs/2407.07300](https://arxiv.org/abs/2407.07300)
- Seger, E., Ovadya, A., Siddarth, D., Garfinkel, B., & Dafoe, A. (2023). Democratising AI: Multiple meanings, goals, and methods. In *AAAI/ACM Conference on AI, Ethics, and Society* (pp. 715–722). ACM. [https://doi.org/10.1145/3600211.3604693](https://doi.org/10.1145/3600211.3604693)
- Seger, E., Dreksler, N., Moulange, R., Dardaman, E., Schuett, J., Wei, K., Winter, C., Arnold, M., Ó hÉigeartaigh, S., Korinek, A., Anderljung, M., Bucknall, B., Chan, A., Stafford, E., Koessler, L., Ovadya, A., Garfinkel, B., Bluemke, E., Aird, M., … Gupta, A. (2023). *Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives*. arXiv. [https://arxiv.org/abs/2311.09227](https://arxiv.org/abs/2311.09227)
- Senft, S., & Gallegos, F. (2008). Information technology control and audit, third edition ( 3rd ed.). Auerbach. [https://doi.org/10.1201/9781420065541](https://doi.org/10.1201/9781420065541)
- Sevilla, J., Besiroglu, T., Cottier, B., You, J., Roldán, E., Villalobos, P., & Erdil, E. (2024, August 20). *Can AI scaling continue through 2030?* Epoch. [https://perma.cc/PC4B-YDME](https://perma.cc/PC4B-YDME)
- Shavit, Y., Agarwal, S., Brundage, M., Adler, S., O'Keefe, C., Campbell, R., Lee, T., Mishkin, P., Eloundou, T., Hickey, A., Slama, K., Ahmad, L., McMillan, P., Beutel, A., Passos, A., & Robinson, D. G. (2023). Practices for governing agentic AI systems. OpenAI. [https://perma.cc/62PZ-7K4F](https://perma.cc/62PZ-7K4F)
- Shevlane, T. (2022). Structured access: An emerging paradigm for safe AI deployment. In J. B. Bullock, Y.-C. Chen, J. Himmelreich, V. M. Hudson, A. Korinek, M. M. Young, & B. Zhang (Eds.), The Oxford handbook of AI governance. Oxford University Press. [https://doi.org/10.1093/oxfordhb/9780197579329.013.39](https://doi.org/10.1093/oxfordhb/9780197579329.013.39)
- Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., Kokotajlo, D., Marchal, N., Anderljung, M., Kolt, N., Ho, L., Siddarth, D., Avin, S., Hawkins, W., Kim, B., Gabriel, I., Bolina, V., Clark, J., Bengio, Y., … Dafoe, A. (2023). *Model evaluation for extreme risks*. arXiv. [https://arxiv.org/abs/2305.15324](https://arxiv.org/abs/2305.15324)
- Shrestha, Y. R., von Krogh, G., & Feuerriegel, S. (2023). Building open-source AI. *Nature Computational Science*, 3(11), 908–911. [https://doi.org/10.1038/s43588-023-00540-0](https://doi.org/10.1038/s43588-023-00540-0)
- Singer, P., & Tse, Y. F. (2023). AI ethics: The case for including animals. *AI and Ethics*, 3(2), 539–551. [https://doi.org/10.1007/s43681-022-00187-z](https://doi.org/10.1007/s43681-022-00187-z)
- Slattery, P., Saeri, A. K., Grundy, E. A. C., Graham, J., Noetel, M., Uuk, R., Dao, J., Pour, S., & Thompson, N. (2024). *The AI risk repository: A comprehensive meta-review, database, and taxonomy of risks from artificial intelligence*. arXiv. [http://arxiv.org/abs/2408.12622](http://arxiv.org/abs/2408.12622)
- Soh, D. S. B., & Martinov-Bennie, N. (2011). The internal audit function: Perceptions of internal audit roles, effectiveness and evaluation. *Managerial Auditing Journal*, 26(7), 605–622. [https://doi.org/10.1108/02686901111151332](https://doi.org/10.1108/02686901111151332)
- Soice, E. H., Rocha, R., Cordova, K., Specter, M., & Esvelt, K. M. (2023). *Can large language models democratize access to dual-use biotechnology?* arXiv. [https://arxiv.org/abs/2306.03809](https://arxiv.org/abs/2306.03809)
- Solaiman, I. (2023). *The gradient of generative AI release: Methods and considerations*. arXiv. [https://arxiv.org/abs/2302.04844](https://arxiv.org/abs/2302.04844)
- Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford, A., Krueger, G., Kim, J. W., Kreps, S., McCain, M., Newhouse, A., Blazakis, J., McGuffie, K., & Wang, J. (2019). *Release strategies and the social impacts of language models*. arXiv. [http://arxiv.org/abs/1908.09203](http://arxiv.org/abs/1908.09203)
- Soler Garrido, J., Fano Yela, D., Panigutti, C., Junklewitz, H., Hamon, R., Evas, T., André, A.-A., & Scalzo, S. (2023). Analysis of the preliminary AI standardisation work plan in support of the AI Act. European Commission, Joint Research Centre. [https://doi.org/10.2760/5847](https://doi.org/10.2760/5847)
- Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., Kocurek, A. W., Safaya, A., Tazarv, A., … Wu, Z. (2022). *Beyond the imitation game: Quantifying and extrapolating the capabilities of language models*. arXiv. [http://arxiv.org/abs/2206.04615](http://arxiv.org/abs/2206.04615)
- Stafford, T., Deitz, G., & Li, Y. (2018). The role of internal audit and user training in information security policy compliance. *Managerial Auditing Journal*, 33(4), 410–424. [https://doi.org/10.1108/MAJ-07-2017-1596](https://doi.org/10.1108/MAJ-07-2017-1596)
- Steinbart, P. J., Raschke, R. L., Gal, G., & Dilla, W. N. (2012). The relationship between internal audit and information security: An exploratory investigation. *International Journal of Accounting Information Systems*, 13(3), 228–243. [https://doi.org/10.1016/j.accinf.2012.06.007](https://doi.org/10.1016/j.accinf.2012.06.007)
- Stewart, J., & Subramaniam, N. (2010). Internal audit independence and objectivity: Emerging research opportunities. *Managerial Auditing Journal*, 25(4), 328–360. [https://doi.org/10.1108/02686901011034162](https://doi.org/10.1108/02686901011034162)
- Stoel, D., Havelka, D., & Merhout, J. W. (2012). An analysis of attributes that impact information technology audit quality: A study of IT and financial audit practitioners. *International Journal of Accounting Information Systems*, 13(1), 60–79. [https://doi.org/10.1016/j.accinf.2011.11.001](https://doi.org/10.1016/j.accinf.2011.11.001)
- Taleb, N. N. (2007). The black swan: The impact of the highly improbable. Random House.
- Tetlock, P. E. (2005). Expert political judgment: How good is it? How can we know? Princeton University Press.
- Tetlock, P. E., & Gardner, D. (2015). Superforecasting: The art and science of prediction. Penguin Random House.
- Thekdi, S. A., & Aven, T. (2022). Risk analysis under attack: How risk science can address the legal, social, and reputational liabilities faced by risk analysts. *Risk Analysis*, 43(6), 1212–1221. [https://doi.org/10.1111/risa.13984](https://doi.org/10.1111/risa.13984)
- Trager, R., Harack, B., Reuel, A., Carnegie, A., Heim, L., Ho, L., Kreps, S., Lall, R., Larter, O., Ó hÉigeartaigh, S., Staffell, S., & Villalobos, J. J. (2023). *International governance of civilian AI: A jurisdictional certification approach*. arXiv. [https://arxiv.org/abs/2308.15514](https://arxiv.org/abs/2308.15514)
- Turner, A. M., Smith, L., Shah, R., Critch, A., & Tadepalli, P. (2023). *Optimal policies tend to seek power*. arXiv. [https://arxiv.org/abs/1912.01683](https://arxiv.org/abs/1912.01683)
- Turner, A. M., & Tadepalli, P. (2022). *Parametrically retargetable decision-makers tend to seek power*. arXiv. [https://arxiv.org/abs/2206.13477](https://arxiv.org/abs/2206.13477)
- Urbina, F., Lentzos, F., Invernizzi, C., & Ekins, S. (2022). Dual use of artificial-intelligence-powered drug discovery. *Nature Machine Intelligence*, 4(3), 189–191. [https://doi.org/10.1038/s42256-022-00465-9](https://doi.org/10.1038/s42256-022-00465-9)
- Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., & Ho, A. (2024). *Will we run out of data? An analysis of the limits of scaling datasets in machine learning*. arXiv. [https://arxiv.org/abs/2211.04325](https://arxiv.org/abs/2211.04325)
- Vipra, J., & Korinek, A. (2023). Market concentration implications of foundation models. arXiv. [https://arxiv.org/abs/2311.01550](https://arxiv.org/abs/2311.01550)
- Wang, P. (2019). On defining artificial intelligence. *Journal of Artificial General Intelligence*, 10(2), 1–37. [https://doi.org/10.2478/jagi-2019-0002](https://doi.org/10.2478/jagi-2019-0002)
- Wassie, F. A., & Lakatos, L. P. (2024). Artificial intelligence and the future of the internal audit function. *Humanities and Social Sciences Communications*, 11(386), 1–13. [https://doi.org/10.1057/s41599-024-02905-w](https://doi.org/10.1057/s41599-024-02905-w)
- Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., & Fedus, W. (2022). *Emergent abilities of large language models*. arXiv. [https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)
- Weidenmier, M. L., & Ramamoorti, S. (2006). Research opportunities in information technology and internal auditing. *Journal of Information Systems*, 20(1), 205–219. [https://doi.org/10.2308/jis.2006.20.1.205](https://doi.org/10.2308/jis.2006.20.1.205)
- Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., Kenton, Z., Brown, S., Hawkins, W., Stepleton, T., Biles, C., Birhane, A., Haas, J., Rimell, L., Hendricks, L. A., … Gabriel, I. (2021). *Ethical and social risks of harm from language models*. arXiv. [https://arxiv.org/abs/2112.04359](https://arxiv.org/abs/2112.04359)
- Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, J., Bergman, S., Kay, J., Griffin, C., Bariach, B., Gabriel, I., Rieser, V., & Isaac, W. (2023). *Sociotechnical safety evaluation of generative AI systems*. arXiv. [https://arxiv.org/abs/2310.11986](https://arxiv.org/abs/2310.11986)
- Weidinger, L., Barnhart, J., Brennan, J., Butterfield, C., Young, S., Hawkins, W., Hendricks, L. A., Comanescu, R., Chang, O., Rodriguez, M., Beroshi, J., Bloxwich, D., Proleev, L., Chen, J., Farquhar, S., Ho, L., Gabriel, I., Dafoe, A., & Isaac, W. (2024). *Holistic safety and responsibility evaluations of advanced AI models*. arXiv. [http://arxiv.org/abs/2404.14068](http://arxiv.org/abs/2404.14068)
- van der Weij, T., Hofstätter, F., Jaffe, O., Brown, S. F., & Ward, F. R. (2024). *AI sandbagging: Language models can strategically underperform on evaluations*. arXiv. [https://arxiv.org/abs/2406.07358](https://arxiv.org/abs/2406.07358)
- The White House. (2023). *Safe, secure, and trustworthy development and use of artificial intelligence* (Executive Order 14110). The White House. [https://perma.cc/5HCL-LDMT](https://perma.cc/5HCL-LDMT)
- Whittaker, M. (2021). The steep cost of capture. *Interactions*, 28(6), 50–55. [https://doi.org/10.1145/3488666](https://doi.org/10.1145/3488666)
- van Wynsberghe, A. (2021). Sustainable AI: AI for sustainability and the sustainability of AI. *AI and Ethics*, 1(3), 213–218. [https://doi.org/10.1007/s43681-021-00043-6](https://doi.org/10.1007/s43681-021-00043-6)
- Yong, Z.-X., Menghini, C., & Bach, S. H. (2023). *Low-resource languages jailbreak GPT-4*. arXiv. [http://arxiv.org/abs/2310.02446](http://arxiv.org/abs/2310.02446)
- Zald, M. N. (1969). The power and functions of boards of directors: A theoretical synthesis. *American Journal of Sociology*, 75(1), 97–111. [https://doi.org/10.1086/224747](https://doi.org/10.1086/224747)
- Zhang, A. K., Perry, N., Dulepet, R., Ji, J., Lin, J. W., Jones, E., Menders, C., Hussein, G., Liu, S., Jasper, D., Peetathawatchai, P., Glenn, A., Sivashankar, V., Zamoshchin, D., Glikbarg, L., Askaryar, D., Yang, M., Zhang, T., Alluri, R., … Liang, P. (2024). *Cybench: A framework for evaluating cybersecurity capabilities and risk of language models*. arXiv. [https://arxiv.org/abs/2408.08926](https://arxiv.org/abs/2408.08926)
- Zhan, Q., Fang, R., Bindu, R., Gupta, A., Hashimoto, T., & Kang, D. (2023). *Removing RLHF protections in GPT-4 via fine-tuning*. arXiv. [https://arxiv.org/abs/2311.05553](https://arxiv.org/abs/2311.05553)
- Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). *A survey of large language models*. arXiv. [https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223)
- Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., & Irving, G. (2019). *Fine-tuning language models from human preferences*. arXiv. [https://arxiv.org/abs/1909.08593](https://arxiv.org/abs/1909.08593)